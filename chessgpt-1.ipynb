{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ostenloo/chess-gpt/blob/main/chessgpt-1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQEZqOBufGtm",
        "outputId": "06a4e88d-c40f-41e7-ab85-13c4b9182b95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting chess\n",
            "  Downloading chess-1.9.4-py3-none-any.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.1/149.1 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: chess\n",
            "Successfully installed chess-1.9.4\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting stockfish\n",
            "  Downloading stockfish-3.28.0-py3-none-any.whl (13 kB)\n",
            "Installing collected packages: stockfish\n",
            "Successfully installed stockfish-3.28.0\n"
          ]
        }
      ],
      "source": [
        "!pip install chess \n",
        "!pip install torch \n",
        "!pip install stockfish "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XeaBQhU7e1lh",
        "outputId": "dfd8f394-2d09-4c45-b69d-f90ac541514c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chess 1.9.4\n",
            "Torch 2.0.0+cu118\n"
          ]
        }
      ],
      "source": [
        "import torch \n",
        "import torch.nn as nn \n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "from torch.distributed import init_process_group, destroy_process_group\n",
        "\n",
        "import chess \n",
        "import chess.pgn \n",
        "import chess.engine\n",
        "from stockfish import Stockfish\n",
        "\n",
        "import math \n",
        "import matplotlib.pyplot as plt\n",
        "from dataclasses import dataclass \n",
        "import numpy as np \n",
        "import os \n",
        "import time \n",
        "import asyncio\n",
        "from contextlib import nullcontext\n",
        "import inspect \n",
        "\n",
        "print(\"Chess\", chess.__version__)\n",
        "print(\"Torch\", torch.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhC2bn_bf3jR",
        "outputId": "0fcc7ff6-603f-43da-fc29-ca50793ea4b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# for colab \n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9s_tQyYe1li"
      },
      "source": [
        "## Data Preprocessing\n",
        "\n",
        "Only need to run once. Create a data folder before running. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEhZ5nPge1li"
      },
      "source": [
        "### 1. Generate PGN and FEN files for Black and White Colors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ul4MUw_Re1lj"
      },
      "outputs": [],
      "source": [
        "# file_path = \"/content/drive/My Drive/chessdata/Tal.pgn\"\n",
        "\n",
        "# # save the games based on if tal played white or black\n",
        "# games = {\n",
        "#     \"white\": [], \n",
        "#     \"black\": []\n",
        "# }\n",
        "\n",
        "# with open(file_path) as f: \n",
        "#     lines = f.readlines() \n",
        "#     tal_color = None \n",
        "#     on_pgn_line = False  \n",
        "#     pgn = \"\"\n",
        "#     for line in lines:\n",
        "#         if on_pgn_line:\n",
        "#             if line.startswith(\"[Event\"): \n",
        "#                 on_pgn_line = False \n",
        "#                 games[tal_color].append(pgn)\n",
        "#                 pgn = \"\"\n",
        "#             else: \n",
        "#                 pgn += line \n",
        "#         if line.startswith(\"[White \"): \n",
        "#             player_white = line.split('\"')[1]\n",
        "#             if player_white == \"Tal, Mihail\":\n",
        "#                 tal_color = \"white\"\n",
        "#         elif line.startswith(\"[Black \"):\n",
        "#             player_black = line.split('\"')[1]\n",
        "#             if player_black == \"Tal, Mihail\":\n",
        "#                 tal_color = \"black\"\n",
        "#         elif line.startswith(\"1.\"): \n",
        "#             on_pgn_line = True \n",
        "#             pgn += line \n",
        "#         else: \n",
        "#             # print(tal_color)\n",
        "#             continue \n",
        "\n",
        "\n",
        "# f.close()  \n",
        "\n",
        "# with open(\"/content/drive/My Drive/chessdata/tal_white_games.pgn\", \"w\") as f: \n",
        "#     for game in games[\"white\"]:\n",
        "#         f.write(game)\n",
        "\n",
        "# f.close()\n",
        "\n",
        "# with open(\"/content/drive/My Drive/chessdata/tal_black_games.pgn\", \"w\") as f:\n",
        "#     for game in games[\"black\"]:\n",
        "#         f.write(game)\n",
        "\n",
        "# f.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8F2W9Yke1lj"
      },
      "source": [
        "### 2. Generate UCI Vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XXsIs422e1lj"
      },
      "outputs": [],
      "source": [
        "# from itertools import product\n",
        "\n",
        "# files = [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\"]\n",
        "# ranks = [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\"]\n",
        "# promotion = [\"Q\", \"R\", \"B\", \"N\"]\n",
        "\n",
        "# # gets an upper bound for the UCI vocab\n",
        "\n",
        "# with open(\"/content/drive/My Drive/chessdata/UCIvocab.txt\", \"w\") as f:\n",
        "#     for file1, rank1, file2, rank2 in product(files, ranks, files, ranks):\n",
        "#         if file1 == file2 and rank1 == rank2:\n",
        "#             continue \n",
        "#         if rank2 == \"8\" and rank1 == \"7\" or rank2 == \"1\" and rank1 == \"2\":\n",
        "#             idx1 = files.index(file1)\n",
        "#             idx2 = files.index(file2)\n",
        "#             if abs(idx1 - idx2) > 1:\n",
        "#                 continue\n",
        "#             for promotion_piece in promotion:\n",
        "#                 f.write(f'{file1}{rank1}{file2}{rank2}{promotion_piece}\\n') \n",
        "#         f.write(f'{file1}{rank1}{file2}{rank2}\\n')\n",
        "\n",
        "# f.close() "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OT0nq35ee1lj"
      },
      "source": [
        "### 3. Generate PGN Vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2vLtchxTe1lj"
      },
      "outputs": [],
      "source": [
        "# pieces = [\"K\", \"Q\", \"R\", \"B\", \"N\"] \n",
        "# print(pieces[1:])\n",
        "# ranks = [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\"]\n",
        "# for rank in ranks[1:7]:\n",
        "#     print(rank)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pKk9IaJPe1lj"
      },
      "outputs": [],
      "source": [
        "# pieces = [\"K\", \"Q\", \"R\", \"B\", \"N\"] \n",
        "# pawns = [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\"]\n",
        "# files = [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\"]\n",
        "# ranks = [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\"]\n",
        "\n",
        "# moves = set()\n",
        "\n",
        "# for piece in pieces: \n",
        "#     for file in files: \n",
        "#         for rank in ranks: \n",
        "#             moves.add(f\"{piece}{file}{rank}\")\n",
        "#             moves.add(f\"{piece}{file}{rank}+\")\n",
        "#             moves.add(f\"{piece}{file}{rank}#\")\n",
        "#             moves.add(f\"{piece}x{file}{rank}\")\n",
        "#             moves.add(f\"{piece}x{file}{rank}+\")\n",
        "#             moves.add(f\"{piece}x{file}{rank}#\")\n",
        "#             # 2 pieces can move to the same square \n",
        "#             if piece == \"R\" or piece == \"N\" or piece == \"B\" or piece == \"Q\": \n",
        "#                 for rank1 in ranks: \n",
        "#                     moves.add(f\"{piece}{rank1}{file}{rank}\")\n",
        "#                     moves.add(f\"{piece}{rank1}{file}{rank}+\")\n",
        "#                     moves.add(f\"{piece}{rank1}{file}{rank}#\")\n",
        "#                     moves.add(f\"{piece}{rank1}x{file}{rank}\")\n",
        "#                     moves.add(f\"{piece}{rank1}x{file}{rank}+\")\n",
        "#                     moves.add(f\"{piece}{rank1}x{file}{rank}#\")\n",
        "#                 for file1 in files: \n",
        "#                     moves.add(f\"{piece}{file1}{file}{rank}\")\n",
        "#                     moves.add(f\"{piece}{file1}{file}{rank}+\")\n",
        "#                     moves.add(f\"{piece}{file1}{file}{rank}#\")\n",
        "#                     moves.add(f\"{piece}{file1}x{file}{rank}\")\n",
        "#                     moves.add(f\"{piece}{file1}x{file}{rank}+\")\n",
        "#                     moves.add(f\"{piece}{file1}x{file}{rank}#\")\n",
        "#                 # if there are 3 pieces due to promotion, need to specify file and rank \n",
        "#                 for rank1 in ranks: \n",
        "#                     for file1 in files: \n",
        "#                         moves.add(f\"{piece}{file1}{rank1}{file}{rank}\")\n",
        "#                         moves.add(f\"{piece}{file1}{rank1}{file}{rank}+\")\n",
        "#                         moves.add(f\"{piece}{file1}{rank1}{file}{rank}#\")\n",
        "#                         moves.add(f\"{piece}{file1}{rank1}x{file}{rank}\")\n",
        "#                         moves.add(f\"{piece}{file1}{rank1}x{file}{rank}+\")\n",
        "#                         moves.add(f\"{piece}{file1}{rank1}x{file}{rank}#\")\n",
        "\n",
        "# for (idx, pawn) in enumerate(pawns): \n",
        "#     for rank in ranks[1:7]: #2,3,4,5,6,7\n",
        "#         if pawn == \"a\": \n",
        "#             if rank == \"7\": \n",
        "#                 for piece in pieces[1:]: \n",
        "#                     moves.add(f\"{pawn}xb{str(int(rank)+1)}={piece}\")\n",
        "#                     moves.add(f\"{pawn}xb{str(int(rank)+1)}={piece}+\")\n",
        "#                     moves.add(f\"{pawn}xb{str(int(rank)+1)}={piece}#\")\n",
        "#                     moves.add(f\"{pawn}{str(int(rank)+1)}={piece}\")\n",
        "#                     moves.add(f\"{pawn}{str(int(rank)+1)}={piece}+\")\n",
        "#                     moves.add(f\"{pawn}{str(int(rank)+1)}={piece}#\")\n",
        "#             elif rank == \"2\":\n",
        "#                 for piece in pieces[1:]:\n",
        "#                     moves.add(f\"{pawn}xb{str(int(rank)-1)}={piece}\")\n",
        "#                     moves.add(f\"{pawn}xb{str(int(rank)-1)}={piece}+\")\n",
        "#                     moves.add(f\"{pawn}xb{str(int(rank)-1)}={piece}#\")\n",
        "#                     moves.add(f\"{pawn}{str(int(rank)-1)}={piece}\")\n",
        "#                     moves.add(f\"{pawn}{str(int(rank)-1)}={piece}+\")\n",
        "#                     moves.add(f\"{pawn}{str(int(rank)-1)}={piece}#\")\n",
        "#             else: \n",
        "#                 moves.add(f\"{pawn}xb{str(int(rank)+1)}\")\n",
        "#                 moves.add(f\"{pawn}xb{str(int(rank)+1)}+\")\n",
        "#                 moves.add(f\"{pawn}xb{str(int(rank)+1)}#\")\n",
        "#                 moves.add(f\"{pawn}xb{str(int(rank)-1)}\")\n",
        "#                 moves.add(f\"{pawn}xb{str(int(rank)-1)}+\")\n",
        "#                 moves.add(f\"{pawn}xb{str(int(rank)-1)}#\")\n",
        "#                 moves.add(f\"{pawn}{str(int(rank)+1)}\")\n",
        "#                 moves.add(f\"{pawn}{str(int(rank)+1)}+\")\n",
        "#                 moves.add(f\"{pawn}{str(int(rank)+1)}#\")\n",
        "#                 moves.add(f\"{pawn}{str(int(rank)-1)}\")\n",
        "#                 moves.add(f\"{pawn}{str(int(rank)-1)}+\")\n",
        "#                 moves.add(f\"{pawn}{str(int(rank)-1)}#\")\n",
        "#         elif pawn == \"h\": \n",
        "#             if rank == \"7\":\n",
        "#                 for piece in pieces[1:]:\n",
        "#                     moves.add(f\"{pawn}xg{str(int(rank)+1)}={piece}\")\n",
        "#                     moves.add(f\"{pawn}xg{str(int(rank)+1)}={piece}+\")\n",
        "#                     moves.add(f\"{pawn}xg{str(int(rank)+1)}={piece}#\")\n",
        "#                     moves.add(f\"{pawn}{str(int(rank)+1)}={piece}\")\n",
        "#                     moves.add(f\"{pawn}{str(int(rank)+1)}={piece}+\")\n",
        "#                     moves.add(f\"{pawn}{str(int(rank)+1)}={piece}#\")\n",
        "#             elif rank == \"2\":\n",
        "#                 for piece in pieces[1:]:\n",
        "#                     moves.add(f\"{pawn}xg{str(int(rank)-1)}={piece}\")\n",
        "#                     moves.add(f\"{pawn}xg{str(int(rank)-1)}={piece}+\")\n",
        "#                     moves.add(f\"{pawn}xg{str(int(rank)-1)}={piece}#\")\n",
        "#                     moves.add(f\"{pawn}{str(int(rank)-1)}={piece}\")\n",
        "#                     moves.add(f\"{pawn}{str(int(rank)-1)}={piece}+\")\n",
        "#                     moves.add(f\"{pawn}{str(int(rank)-1)}={piece}#\")\n",
        "#             else: \n",
        "#                 moves.add(f\"{pawn}xg{str(int(rank)+1)}\")\n",
        "#                 moves.add(f\"{pawn}xg{str(int(rank)+1)}+\")\n",
        "#                 moves.add(f\"{pawn}xg{str(int(rank)+1)}#\")\n",
        "#                 moves.add(f\"{pawn}xg{str(int(rank)-1)}\")\n",
        "#                 moves.add(f\"{pawn}xg{str(int(rank)-1)}+\")\n",
        "#                 moves.add(f\"{pawn}xg{str(int(rank)-1)}#\")\n",
        "#                 moves.add(f\"{pawn}{str(int(rank)+1)}\")\n",
        "#                 moves.add(f\"{pawn}{str(int(rank)+1)}+\")\n",
        "#                 moves.add(f\"{pawn}{str(int(rank)+1)}#\")\n",
        "#                 moves.add(f\"{pawn}{str(int(rank)-1)}\")\n",
        "#                 moves.add(f\"{pawn}{str(int(rank)-1)}+\")\n",
        "#                 moves.add(f\"{pawn}{str(int(rank)-1)}#\")\n",
        "#         else: \n",
        "#             if rank == \"7\":\n",
        "#                 for piece in pieces[1:]: \n",
        "#                     moves.add(f\"{pawn}{str(int(rank)+1)}={piece}\")\n",
        "#                     moves.add(f\"{pawn}{str(int(rank)+1)}={piece}+\")\n",
        "#                     moves.add(f\"{pawn}{str(int(rank)+1)}={piece}#\")\n",
        "#                     moves.add(f\"{pawn}x{pawns[idx-1]}{str(int(rank)+1)}={piece}\")\n",
        "#                     moves.add(f\"{pawn}x{pawns[idx-1]}{str(int(rank)+1)}={piece}+\")\n",
        "#                     moves.add(f\"{pawn}x{pawns[idx-1]}{str(int(rank)+1)}={piece}#\")\n",
        "#                     moves.add(f\"{pawn}x{pawns[idx+1]}{str(int(rank)+1)}={piece}\")\n",
        "#                     moves.add(f\"{pawn}x{pawns[idx+1]}{str(int(rank)+1)}={piece}+\")\n",
        "#                     moves.add(f\"{pawn}x{pawns[idx+1]}{str(int(rank)+1)}={piece}#\")\n",
        "#             elif rank == \"2\":\n",
        "#                 for piece in pieces[1:]: \n",
        "#                     moves.add(f\"{pawn}{str(int(rank)-1)}={piece}\")\n",
        "#                     moves.add(f\"{pawn}{str(int(rank)-1)}={piece}+\")\n",
        "#                     moves.add(f\"{pawn}{str(int(rank)-1)}={piece}#\")\n",
        "#                     moves.add(f\"{pawn}x{pawns[idx-1]}{str(int(rank)-1)}={piece}\")\n",
        "#                     moves.add(f\"{pawn}x{pawns[idx-1]}{str(int(rank)-1)}={piece}+\")\n",
        "#                     moves.add(f\"{pawn}x{pawns[idx-1]}{str(int(rank)-1)}={piece}#\")\n",
        "#                     moves.add(f\"{pawn}x{pawns[idx+1]}{str(int(rank)-1)}={piece}\")\n",
        "#                     moves.add(f\"{pawn}x{pawns[idx+1]}{str(int(rank)-1)}={piece}+\")\n",
        "#                     moves.add(f\"{pawn}x{pawns[idx+1]}{str(int(rank)-1)}={piece}#\")\n",
        "#             else: \n",
        "#                 moves.add(f\"{pawn}x{pawns[idx-1]}{str(int(rank)+1)}\")\n",
        "#                 moves.add(f\"{pawn}x{pawns[idx-1]}{str(int(rank)+1)}+\")\n",
        "#                 moves.add(f\"{pawn}x{pawns[idx-1]}{str(int(rank)+1)}#\")\n",
        "#                 moves.add(f\"{pawn}x{pawns[idx+1]}{str(int(rank)+1)}\")\n",
        "#                 moves.add(f\"{pawn}x{pawns[idx+1]}{str(int(rank)+1)}+\")\n",
        "#                 moves.add(f\"{pawn}x{pawns[idx+1]}{str(int(rank)+1)}#\")\n",
        "#                 moves.add(f\"{pawn}x{pawns[idx-1]}{str(int(rank)-1)}\")\n",
        "#                 moves.add(f\"{pawn}x{pawns[idx-1]}{str(int(rank)-1)}+\")\n",
        "#                 moves.add(f\"{pawn}x{pawns[idx-1]}{str(int(rank)-1)}#\")\n",
        "#                 moves.add(f\"{pawn}x{pawns[idx+1]}{str(int(rank)-1)}\")\n",
        "#                 moves.add(f\"{pawn}x{pawns[idx+1]}{str(int(rank)-1)}+\")\n",
        "#                 moves.add(f\"{pawn}x{pawns[idx+1]}{str(int(rank)-1)}#\")\n",
        "#                 moves.add(f\"{pawn}{str(int(rank)+1)}\")\n",
        "#                 moves.add(f\"{pawn}{str(int(rank)+1)}+\")\n",
        "#                 moves.add(f\"{pawn}{str(int(rank)+1)}#\")\n",
        "#                 moves.add(f\"{pawn}{str(int(rank)-1)}\")\n",
        "#                 moves.add(f\"{pawn}{str(int(rank)-1)}+\")\n",
        "#                 moves.add(f\"{pawn}{str(int(rank)-1)}#\")\n",
        "\n",
        "# # add castling         \n",
        "# moves.add(\"O-O\")\n",
        "# moves.add(\"O-O+\")\n",
        "# moves.add(\"O-O#\")\n",
        "# moves.add(\"O-O-O\")\n",
        "# moves.add(\"O-O-O+\")\n",
        "# moves.add(\"O-O-O#\")\n",
        "\n",
        "# with open(\"/content/drive/My Drive/chessdata/PGNVocab.txt\", \"w\") as f: \n",
        "#     f.write(\"<eos>\\n1-0\\n0-1\\n1/2-1/2\\n\\n*\\n\")\n",
        "#     for move in moves: \n",
        "#         f.write(move + \"\\n\")\n",
        "\n",
        "# f.close() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZHziZ5pe1lk"
      },
      "outputs": [],
      "source": [
        "# file_names = [\"tal_black_games\", \"tal_white_games\"]\n",
        "\n",
        "# for file in file_names:\n",
        "#     pgn = open(f\"/content/drive/My Drive/chessdata/{file}.pgn\", 'r')\n",
        "#     iter = 0 \n",
        "#     while iter < 1: \n",
        "#         iter += 1 \n",
        "#         game = chess.pgn.read_game(pgn)\n",
        "#         if game is None: \n",
        "#             break\n",
        "#         board = game.board()\n",
        "#         # print(type(board)) \n",
        "#         for move in game.mainline_moves():\n",
        "#             # print(type(move))\n",
        "#             # print(move)\n",
        "#             # print(board.san(move)) \n",
        "#             board.push(move) \n",
        "#             # print(move) #uci \n",
        "#             # print(board.fen()) #fen\n",
        "#             # print(board.san(move))\n",
        "#             # how to get pgn? \n",
        "\n",
        "#     pgn.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9Bz7RXXe1lk"
      },
      "source": [
        "### 4. Generate PGNVocab Stripped of + and \\#  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wOXpKSBDe1lk"
      },
      "outputs": [],
      "source": [
        "# moves = []\n",
        "# move_set = set()\n",
        "\n",
        "# with open(\"/content/drive/My Drive/chessdata/PGNVocab.txt\", \"r\") as f: \n",
        "#     lines = f.readlines() \n",
        "#     for line in lines:\n",
        "#         line = line.strip(\"+#\\n\")\n",
        "#         if line not in move_set:\n",
        "#             moves.append(line)\n",
        "#             move_set.add(line)\n",
        "\n",
        "# f.close() \n",
        "\n",
        "# with open(\"/content/drive/My Drive/chessdata/PGNVocabStrip.txt\", \"w\") as f: \n",
        "#     for move in moves: \n",
        "#         f.write(move + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9cSJXTme1lk"
      },
      "source": [
        "## Chess Agent used to interface with Stockfish engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-bzVbKYHe1ll"
      },
      "outputs": [],
      "source": [
        "@dataclass \n",
        "class ChessAgentConfig: \n",
        "    depth: int = 20 # engine depth \n",
        "    mate_score: int = 10000 # engine mate score\n",
        "\n",
        "class ChessAgent: \n",
        "    def __init__(self, config):\n",
        "        self.engine = chess.engine.SimpleEngine.popen_uci(\"stockfish\")\n",
        "        self.config = config \n",
        "\n",
        "    def fen_and_pgn_from_uci(self, uci, fen): \n",
        "        board = chess.Board(fen)\n",
        "        move = chess.Move.from_uci(uci)\n",
        "        pgn = board.san(move)\n",
        "        board.push(move)\n",
        "        fen = board.fen()\n",
        "        \n",
        "        return fen, pgn \n",
        "\n",
        "    def legal_moves(self, fen): \n",
        "        board = chess.Board(fen)\n",
        "        legal_moves = list(board.legal_moves)\n",
        "\n",
        "        return legal_moves \n",
        "\n",
        "    async def async_position_info(self, fen): \n",
        "        board = chess.Board(fen)\n",
        "        return self.engine.analyse(board, chess.engine.Limit(depth=20))\n",
        "\n",
        "    async def async_position_eval(self, fen): \n",
        "        info = await self.async_position_info(fen)\n",
        "        return info[\"score\"].white().score(mate_score=self.config.mate_score)\n",
        "\n",
        "    def position_info(self, fen): \n",
        "        board = chess.Board(fen)\n",
        "        return self.engine.analyse(board, chess.engine.Limit(depth=20))\n",
        "\n",
        "    def position_eval(self, fen): \n",
        "        info = self.position_info(fen)\n",
        "        return info[\"score\"].white().score(mate_score=self.config.mate_score)\n",
        "\n",
        "    def quit(self): \n",
        "        self.engine.quit()\n",
        "\n",
        "asyncio.set_event_loop_policy(chess.engine.EventLoopPolicy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQ2lCe-8e1ll"
      },
      "outputs": [],
      "source": [
        "# chessConfig = ChessAgentConfig()\n",
        "\n",
        "# chessAgent = ChessAgent(chessConfig) \n",
        "\n",
        "# fen, pgn = chessAgent.fen_and_pgn_from_uci(\"e2e4\",\"rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq -\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YvQCmX_xe1ll"
      },
      "outputs": [],
      "source": [
        "# black_mate_in_1 = \"rnbqkbnr/pppp1ppp/4p3/8/5PP1/8/PPPPP2P/RNBQKBNR b KQkq - 0 2\"\n",
        "# white_mate_in_1 = \"rnbqkbnr/ppppp2p/8/5pp1/4P3/8/PPPPQPPP/RNB1KBNR w KQkq - 0 3\"\n",
        "\n",
        "# print(chess.Board(black_mate_in_1))\n",
        "# eval_black = await chessAgent.position_eval(black_mate_in_1)\n",
        "# print(\"Black Eval: \", eval_black)\n",
        "# print(\"\\n\")\n",
        "# print(chess.Board(white_mate_in_1))\n",
        "# eval_white = await chessAgent.position_eval(white_mate_in_1)\n",
        "# print(\"White Eval: \", eval_white)\n",
        "\n",
        "# chessAgent.quit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2I2SlEBe1ll"
      },
      "source": [
        "## PyTorch Custom Dataset Class to be used in Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "US4gbyqQe1ll"
      },
      "outputs": [],
      "source": [
        "class ChessDataset(Dataset): \n",
        "    def __init__(self, white, black): \n",
        "        super().__init__()\n",
        "\n",
        "        assert os.path.exists(f\"/content/drive/My Drive/chessdata/{white}.pgn\"), f\"/content/drive/My Drive/chessdata/{white}.pgn does not exist\" \n",
        "        assert os.path.exists(f\"/content/drive/My Drive/chessdata/{white}_fen.txt\"), f\"/content/drive/My Drive/chessdata/{white}_fen.txt does not exist\" \n",
        "\n",
        "        self.white_uci = self.load_uci_moves(white)\n",
        "        self.white_fen = self.load_fen_moves(white)\n",
        "        self.white_pgn = self.load_pgn_moves(white)\n",
        "\n",
        "        assert len(self.white_uci) == len(self.white_fen), f\"white_uci and white_fen are not the same length\"\n",
        "\n",
        "        assert os.path.exists(f\"/content/drive/My Drive/chessdata/{black}.pgn\"), f\"/content/drive/My Drive/chessdata/{black}.pgn does not exist\"\n",
        "        assert os.path.exists(f\"/content/drive/My Drive/chessdata/{black}_fen.txt\"), f\"/content/drive/My Drive/chessdata/{black}_fen.txt does not exist\"\n",
        "\n",
        "        self.black_uci = self.load_uci_moves(black)\n",
        "        self.black_fen = self.load_fen_moves(black)\n",
        "        self.black_pgn = self.load_pgn_moves(black)\n",
        "\n",
        "        assert len(self.black_uci) == len(self.black_fen), f\"black_uci and black_fen are not the same length\"\n",
        "\n",
        "        self.pgn_vocab = self.load_pgn_vocab()\n",
        "\n",
        "        self.pgn_vocab_stripped = self.load_pgn_vocab_stripped() \n",
        "\n",
        "        self.pad_game_length()\n",
        "\n",
        "        self.to_numpy()\n",
        "\n",
        "    def load_pgn_vocab(self): \n",
        "        vocab = open(\"/content/drive/My Drive/chessdata/PGNVocab.txt\", 'r')\n",
        "        vocab_dict = dict()\n",
        "        for (idx,line) in enumerate(vocab.readlines()): \n",
        "            vocab_dict[line.strip(\"\\n\")] = idx\n",
        "            \n",
        "        vocab.close() \n",
        "\n",
        "        return vocab_dict\n",
        "\n",
        "    def load_pgn_vocab_stripped(self): \n",
        "        vocab = open(\"/content/drive/My Drive/chessdata/PGNVocabStrip.txt\", 'r')\n",
        "        vocab_dict = dict()\n",
        "        for (idx,line) in enumerate(vocab.readlines()): \n",
        "            vocab_dict[line.strip(\"\\n\")] = idx\n",
        "            \n",
        "        vocab.close() \n",
        "\n",
        "        return vocab_dict\n",
        "        \n",
        "    def load_fen_moves(self, file_name): \n",
        "        with open(f\"/content/drive/My Drive/chessdata/{file_name}_fen.txt\", 'r') as f: \n",
        "            lines = f.readlines() \n",
        "            games = []\n",
        "            for line in lines: \n",
        "                games.append(line.strip(\"\\n\").split(\";\")[:-1])\n",
        "        f.close() \n",
        "\n",
        "        return games\n",
        "\n",
        "    def load_uci_moves(self, file_name): \n",
        "        pgn = open(f\"/content/drive/My Drive/chessdata/{file_name}.pgn\", 'r')\n",
        "        games = [] \n",
        "\n",
        "        while True: \n",
        "            game = chess.pgn.read_game(pgn)\n",
        "            if game is None: \n",
        "                break\n",
        "            board = game.board() \n",
        "            moves = []\n",
        "            for move in game.mainline_moves(): \n",
        "                moves.append(str(move))\n",
        "            games.append(moves)\n",
        "        \n",
        "        return games \n",
        "\n",
        "    def load_pgn_moves(self, file_name): \n",
        "        pgn = open(f\"/content/drive/My Drive/chessdata/{file_name}.pgn\", \"r\")\n",
        "        games = [] \n",
        "\n",
        "        pgn_content = pgn.read() \n",
        "\n",
        "        games = pgn_content.split(\"\\n\\n\")[:-1]\n",
        "\n",
        "        game_move_list = []\n",
        "\n",
        "        for game in games: \n",
        "            moves = [move[max(0,move.find(\".\")+1):].strip(\"+#\\n\") for move in game.split(\" \")]\n",
        "            game_move_list.append(moves)\n",
        "\n",
        "        return game_move_list\n",
        "\n",
        "    def __len__(self): \n",
        "        return len(self.white_uci) + len(self.black_uci)\n",
        "\n",
        "    # for pgn moves \n",
        "    def to_numpy(self): \n",
        "        stoi = { move:i for i, move in enumerate(self.pgn_vocab_stripped) }\n",
        "        itos = { i:move for i, move in enumerate(self.pgn_vocab_stripped) }\n",
        "\n",
        "        def encode(pgn): \n",
        "            return [stoi[move] for move in pgn]\n",
        "        def decode(l): \n",
        "            return [itos[i] for i in l]\n",
        "\n",
        "        # print(self.white_pgn[0:10])\n",
        "        \n",
        "        self.white_pgn_np = np.array([encode(pgn) for pgn in self.white_pgn])\n",
        "        self.black_pgn_np = np.array([encode(pgn) for pgn in self.black_pgn])\n",
        "\n",
        "    # required for batch loading \n",
        "    def pad_game_length(self): \n",
        "        max_len = 512 \n",
        "        lists = [self.white_uci, self.black_uci, self.white_fen, self.black_fen, self.white_pgn, self.black_pgn]\n",
        "        self.game_lengths = []\n",
        "        for game_list in lists:\n",
        "            for game in game_list:\n",
        "                game_len = len(game)\n",
        "                if game_list == self.white_pgn or game_list == self.black_pgn: \n",
        "                  self.game_lengths.append(game_len)\n",
        "                while game_len < max_len:\n",
        "                    game.append(\"<eos>\")\n",
        "                    game_len += 1\n",
        "\n",
        "    def game_item(self, idx): \n",
        "        if idx < len(self.white_uci):\n",
        "            game = {\"UCI\": self.white_uci[idx], \"PGN\": self.white_pgn[idx], \"FEN\": self.white_fen[idx], \"np\": self.white_pgn_np[idx], \"color\": \"white\", \"game_length\": self.game_lengths[idx]}\n",
        "        else:\n",
        "            game = {\"UCI\": self.black_uci[idx - len(self.white_uci)], \"PGN\": self.black_pgn[idx - len(self.white_uci)], \"FEN\": self.black_fen[idx - len(self.white_uci)], \"np\": self.black_pgn_np[idx - len(self.white_uci)],  \"color\": \"black\", \"game_length\": self.game_lengths[idx]}\n",
        "\n",
        "        return game\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.game_item(idx)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "eQgnlPGee1ll"
      },
      "outputs": [],
      "source": [
        "# need to do this for the dataloader to work \n",
        "def collate_fn(batch):\n",
        "    # Combine individual samples into a list of dictionaries (batch)\n",
        "    batch_list = []\n",
        "    for sample in batch:\n",
        "        batch_list.append(sample)\n",
        "\n",
        "    return batch_list"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = ChessDataset(\"tal_white_games\", \"tal_black_games\")\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = (len(dataset) - train_size) // 2\n",
        "test_size = val_size \n",
        "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n",
        "vocab = dataset.pgn_vocab_stripped"
      ],
      "metadata": {
        "id": "ceprfZKCjeBP"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  stoi = { move:i for i, move in enumerate(vocab) }\n",
        "  itos = { i:move for i, move in enumerate(vocab) }\n",
        "\n",
        "  def encode(pgn): \n",
        "      return [stoi[move] for move in pgn]\n",
        "  def decode(l): \n",
        "      return [itos[i] for i in l]"
      ],
      "metadata": {
        "id": "Oex2os7VjYbb"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "val = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "test = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "hSoB3GzYz_qN"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34252oD1e1ll"
      },
      "source": [
        "## Building Blocks for Transformer \n",
        "\n",
        "Sourced Heavily from https://github.com/karpathy/nanoGPT/blob/master/model.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "JgdLPVNGe1ll"
      },
      "outputs": [],
      "source": [
        "def new_gelu(x):\n",
        "    \"\"\"\n",
        "    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).\n",
        "    Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415\n",
        "    \"\"\"\n",
        "    return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, ndim, bias):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(ndim))\n",
        "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
        "\n",
        "    def forward(self, input):\n",
        "        input = input.to(self.weight.dtype)\n",
        "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n",
        "\n",
        "# masked attention \n",
        "class CausalSelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias) # in features, out features. ok this makes sense because it is a n_embd size matrix. \n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
        "\n",
        "        self.attn_dropout = nn.Dropout(config.dropout)\n",
        "        self.resid_dropout = nn.Dropout(config.dropout)\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "        self.dropout = config.dropout\n",
        "\n",
        "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
        "        if not self.flash:\n",
        "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
        "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "                                        .view(1, 1, config.block_size, config.block_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
        "\n",
        "        # hs = hidden state \n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
        "        # print(\"size of v: \", v.size())\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs) # .view() returns a new tensor of the same data but with a different shape specified by the args \n",
        "        # reshape() is a more robust version of view(). has to do with the tensor being contiguous or not. view() is more efficient if the tensor is contiguous. \n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "\n",
        "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
        "        if self.flash:\n",
        "            # efficient attention using Flash Attention CUDA kernels\n",
        "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout, is_causal=True)\n",
        "        else:\n",
        "            # manual implementation of attention\n",
        "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
        "            att = F.softmax(att, dim=-1)\n",
        "            att = self.attn_dropout(att)\n",
        "            y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "\n",
        "        # output projection\n",
        "        y = self.resid_dropout(self.c_proj(y))\n",
        "        return y\n",
        "\n",
        "class CausalCrossAttention(CausalSelfAttention): \n",
        "\n",
        "    def __init__(self, config): \n",
        "        super().__init__(config)\n",
        "        self.c_attn = nn.Linear(config.n_embd, config.n_embd * 2, bias=config.bias)\n",
        "        self.c_attn2 = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
        "    \n",
        "    #x1 is from encoder, x2 is from decoder \n",
        "    def forward(self, x1, x2): \n",
        "        # x1 provides the Q and K \n",
        "        B, T, C = x2.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
        "\n",
        "        q, k = self.c_attn(x1).split(self.n_embd, dim=2)\n",
        "        v = self.c_attn2(x2)\n",
        "\n",
        "        # print(\"cross size of v: \", v.size())\n",
        "\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "\n",
        "        if self.flash: \n",
        "            y = torch.nn.functional.scaled_dot_product_attention(q, k , v, attn_mask=None, dropout_p=self.dropout, is_causal=True)\n",
        "        else: \n",
        "            att = (q @ k.transpose(-2,-1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
        "            att = F.softmax(att, dim=-1)\n",
        "            att = self.attn_dropout(att)\n",
        "            y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side \n",
        "\n",
        "        # output projection \n",
        "        y = self.resid_dropout(self.c_proj(y))\n",
        "        return y \n",
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias) # classifier fully connected\n",
        "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias) # classifier projection \n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = new_gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "class EncoderBlock(nn.Module):\n",
        "    # uses pre layer norm \n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x\n",
        "\n",
        "class DecoderBlock(nn.Module): \n",
        "    def __init__(self, config): \n",
        "        super().__init__()\n",
        "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
        "        self.attn_1 = CausalSelfAttention(config)\n",
        "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
        "        self.attn_2 = CausalCrossAttention(config)\n",
        "        self.ln_3 = LayerNorm(config.n_embd, bias=config.bias)\n",
        "        self.mlp = MLP(config)\n",
        "    \n",
        "    #x1 is from encoder, x2 is from decoder \n",
        "    def forward(self, x1, x2): \n",
        "        x2 = x2 + self.attn_1(self.ln_1(x2))\n",
        "        x2 = x2 + self.attn_2(x1, self.ln_2(x2))\n",
        "        x2 = x2 + self.mlp(self.ln_3(x2))\n",
        "        return x2\n",
        "\n",
        "@dataclass \n",
        "class ChessGPTConfig: \n",
        "    block_size: int = 512 # longest chess game was 269 moves or 538 half moves. However, that's the only one. We can discard that as an outlier. 512 covers basically every chess game. \n",
        "    vocab_size: int = 65536 # can be smaller using the PGNVocabStrip.txt # using the PGNVocab.txt, there are 125736 words in the vocabulary. Not every vocabulary will be found in real games. Most will not because most of the moves are very rare. Some are also impossible. \n",
        "    n_layer: int = 12 # number of encoder and decoder blocks in the stack \n",
        "    n_head: int = 12 # multihead attention blocks \n",
        "    n_embd: int = 768 # size of the embedding... originally chosen by GPT-2 ... may have to modify later \n",
        "    dropout: float = 0.0 # none for now \n",
        "    bias: bool = True "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrunB6ppe1ll"
      },
      "source": [
        "## Chess Transformer Module \n",
        "\n",
        "Also sourced heavily from https://github.com/karpathy/nanoGPT/blob/master/model.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "OKt0sme2e1ll"
      },
      "outputs": [],
      "source": [
        "class ChessGPT(nn.Module): \n",
        "\n",
        "    def __init__(self, config): \n",
        "        super().__init__() \n",
        "        assert config.vocab_size is not None\n",
        "        assert config.block_size is not None\n",
        "        self.config = config \n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd), \n",
        "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "            wtoe = nn.Embedding(config.vocab_size, config.n_embd), # target output embedding\n",
        "            wpoe = nn.Embedding(config.block_size, config.n_embd), # target output positional embedding\n",
        "            drop = nn.Dropout(config.dropout), \n",
        "            enc = nn.ModuleList([EncoderBlock(config) for _ in range(config.n_layer)]),\n",
        "            dec = nn.ModuleList([DecoderBlock(config) for _ in range(config.n_layer)]), \n",
        "            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n",
        "        ))\n",
        "\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "        self.transformer.wte.weight = self.lm_head.weight\n",
        "        # experiment with weight tying \n",
        "        # currently wtoe and wte are the same thing. may experiment with making them different. \n",
        "        # self.transformer.wtoe.weight = self.lm_head.weight \n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "        for pn, p in self.named_parameters(): \n",
        "            # print(pn)\n",
        "            if pn.endswith('c_proj.weight'): \n",
        "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
        "\n",
        "        self.chess_agent_config = ChessAgentConfig()\n",
        "        self.chess_engine = ChessAgent(ChessAgentConfig)\n",
        "        self.sample_tries = [] \n",
        "\n",
        "        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
        "\n",
        "    def get_num_params(self, non_embedding=True): \n",
        "        \"\"\"\n",
        "        Return the number of parameters in the model.\n",
        "        For non-embedding count (default), the position embeddings get subtracted.\n",
        "        The token embeddings would too, except due to the parameter sharing these\n",
        "        params are actually used as weights in the final layer, so we include them.\n",
        "        \"\"\"\n",
        "        n_params = sum(p.numel() for p in self.parameters())\n",
        "        if non_embedding: \n",
        "            n_params -= self.transformer.wpe.weight.numel()\n",
        "        return n_params \n",
        "\n",
        "    def _init_weights(self, module): \n",
        "        if isinstance(module, nn.Linear): \n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None: \n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            # initializes module.weight with normal distribution from N(mean, std^2)\n",
        "\n",
        "    def forward(self, pgn, targets=None): \n",
        "        device = pgn.device\n",
        "        b, t_pgn = pgn.size() # pgn is a tensor of shape (b, t_pgn) (its a batch of pgns)\n",
        "        # b, t_fen, _ = fen.size() # fen is a tensor of shape (b, t_fen, 128)\n",
        "        assert t_pgn <= self.config.block_size, f\"Cannot forward sequence of length {t_pgn}, block size is only {self.config.block_size}\"\n",
        "        # assert t_fen <= self.config.block_size, f\"Cannot forward sequence of length {t_fen}, block size is only {self.config.block_size}\"\n",
        "        # assert t_pgn == t_fen, f\"PGN and FEN must have the same length. PGN has length {t_pgn}, FEN has length {t_fen}\"\n",
        "\n",
        "        # fen = torch.round(fen.float().mean(dim=2)).long()\n",
        "\n",
        "        pos = torch.arange(0, t_pgn, dtype=torch.long, device=device).unsqueeze(0) # shape (1, t)\n",
        "\n",
        "        input_tok_emb = self.transformer.wte(pgn) # token embeddings of shape (b, t_pgn, n_embd = 768)\n",
        "        input_pos_emb = self.transformer.wpe(pos) # token embeddings of shape (b, t_fen, n_embd = 768) \n",
        "        input_emb = input_tok_emb + input_pos_emb # shape (b, t_fen x 128, 1, n_embd = 768)\n",
        "\n",
        "        # print(\"shape\", input_emb.shape)\n",
        "\n",
        "        x1 = self.transformer.drop(input_emb)\n",
        "        for block in self.transformer.enc:\n",
        "            x1 = block(x1)\n",
        "\n",
        "        output_tok_emb = self.transformer.wtoe(pgn)\n",
        "        output_pos_emb = self.transformer.wpoe(pos) \n",
        "        output_emb = output_tok_emb + output_pos_emb\n",
        "\n",
        "        x2 = self.transformer.drop(output_emb)\n",
        "\n",
        "        for block in self.transformer.dec: \n",
        "            x2 = block(x1, x2)\n",
        "            \n",
        "        x2 = self.transformer.ln_f(x2)\n",
        "\n",
        "        if targets is not None: \n",
        "            logits = self.lm_head(x2)\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
        "        else: \n",
        "            logits = self.lm_head(x2[:, [-1], :])\n",
        "            loss = None \n",
        "\n",
        "        return logits, loss \n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, pgn, fen, max_new_tokens=1, temperature=1.0, top_k=None): \n",
        "        # here pgn and fen are called game by game \n",
        "        for _ in range(max_new_tokens): \n",
        "            logits, _ = self(pgn)\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "            legal_moves = set(self.chess_engine.legal_moves(fen))\n",
        "\n",
        "            if top_k is not None: \n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "            tries = 0 \n",
        "            while True: \n",
        "              if tries > 10: \n",
        "                move = random.choice(list(legal_moves))\n",
        "                idx_next = encode(move)\n",
        "                break \n",
        "              tries += 1 \n",
        "              idx_next = torch.multinomial(probs, num_samples=1)\n",
        "              move = decode(idx_next)\n",
        "              if move in legal_moves:\n",
        "                break \n",
        "\n",
        "            self.sample_tires.append(tries)\n",
        "\n",
        "            #unpad idx \n",
        "\n",
        "            # nonzero_rows = torch.any(idx != 0, dim=1)\n",
        "            # idx = idx[nonzero_rows]\n",
        "\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "            #repad idx \n",
        "\n",
        "            # rows_to_pad = self.config.block_size - idx.size(0)\n",
        "\n",
        "            # idx = F.pad(idx, pad=(0,0,0, rows_to_pad))\n",
        "\n",
        "        return idx\n",
        "\n",
        "    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n",
        "        \"\"\"\n",
        "        This long function is unfortunately doing something very simple and is being very defensive:\n",
        "        We are separating out all parameters of the model into two buckets: those that will experience\n",
        "        weight decay for regularization and those that won't (biases, and layernorm/embedding weights).\n",
        "        We are then returning the PyTorch optimizer object.\n",
        "        \"\"\"\n",
        "\n",
        "        # separate out all parameters to those that will and won't experience regularizing weight decay\n",
        "        decay = set()\n",
        "        no_decay = set()\n",
        "        whitelist_weight_modules = (torch.nn.Linear, )\n",
        "        blacklist_weight_modules = (torch.nn.LayerNorm, LayerNorm, torch.nn.Embedding)\n",
        "        for mn, m in self.named_modules():\n",
        "            for pn, p in m.named_parameters():\n",
        "                fpn = '%s.%s' % (mn, pn) if mn else pn # full param name\n",
        "                # random note: because named_modules and named_parameters are recursive\n",
        "                # we will see the same tensors p many many times. but doing it this way\n",
        "                # allows us to know which parent module any tensor p belongs to...\n",
        "                # print(fpn)\n",
        "                if pn.endswith('bias'):\n",
        "                    # all biases will not be decayed\n",
        "                    no_decay.add(fpn)\n",
        "                elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):\n",
        "                    # weights of whitelist modules will be weight decayed\n",
        "                    decay.add(fpn)\n",
        "                elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n",
        "                    # weights of blacklist modules will NOT be weight decayed\n",
        "                    no_decay.add(fpn)\n",
        "\n",
        "        # subtle: 'transformer.wte.weight' and 'lm_head.weight' are tied, so they\n",
        "        # will appear in the no_decay and decay sets respectively after the above.\n",
        "        # In addition, because named_parameters() doesn't return duplicates, it\n",
        "        # will only return the first occurence, key'd by 'transformer.wte.weight', below.\n",
        "        # so let's manually remove 'lm_head.weight' from decay set. This will include\n",
        "        # this tensor into optimization via transformer.wte.weight only, and not decayed.\n",
        "        decay.remove('lm_head.weight')\n",
        "        \n",
        "        # validate that we considered every parameter\n",
        "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
        "        inter_params = decay & no_decay\n",
        "        union_params = decay | no_decay\n",
        "        assert len(inter_params) == 0, \"parameters %s made it into both decay/no_decay sets!\" % (str(inter_params), )\n",
        "        assert len(param_dict.keys() - union_params) == 0, \"parameters %s were not separated into either decay/no_decay set!\" \\\n",
        "                                                    % (str(param_dict.keys() - union_params), )\n",
        "\n",
        "        # create the pytorch optimizer object\n",
        "        optim_groups = [\n",
        "            {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": weight_decay},\n",
        "            {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},\n",
        "        ]\n",
        "        # new PyTorch nightly has a new 'fused' option for AdamW that is much faster\n",
        "        use_fused = (device_type == 'cuda') and ('fused' in inspect.signature(torch.optim.AdamW).parameters)\n",
        "        print(f\"using fused AdamW: {use_fused}\")\n",
        "        extra_args = dict(fused=True) if use_fused else dict()\n",
        "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n",
        "\n",
        "        return optimizer\n",
        "\n",
        "    def estimate_mfu(self, fwdbwd_per_iter, dt):\n",
        "        \"\"\" estimate model flops utilization (MFU) in units of A100 bfloat16 peak FLOPS \"\"\"\n",
        "        # first estimate the number of flops we do per iteration.\n",
        "        # see PaLM paper Appendix B as ref: https://arxiv.org/abs/2204.02311\n",
        "        N = self.get_num_params()\n",
        "        cfg = self.config\n",
        "        L, H, Q, T = cfg.n_layer, cfg.n_head, cfg.n_embd//cfg.n_head, cfg.block_size\n",
        "        flops_per_token = 6*N + 12*L*H*Q*T\n",
        "        flops_per_fwdbwd = flops_per_token * T\n",
        "        flops_per_iter = flops_per_fwdbwd * fwdbwd_per_iter\n",
        "        # express our flops throughput as ratio of A100 bfloat16 peak flops\n",
        "        flops_achieved = flops_per_iter * (1.0/dt) # per second\n",
        "        flops_promised = 312e12 # A100 GPU bfloat16 peak flops is 312 TFLOPS\n",
        "        mfu = flops_achieved / flops_promised\n",
        "        return mfu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ydusj95e1lm"
      },
      "source": [
        "## Training the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPWDm0BUe1lm"
      },
      "source": [
        "### 1. Load the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "bij_p-WHe1lm"
      },
      "outputs": [],
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# default config values designed to train a gpt2 (124M) on OpenWebText\n",
        "# I/O\n",
        "out_dir = 'out'\n",
        "eval_interval = 1\n",
        "log_interval = 1\n",
        "eval_iters = 5\n",
        "eval_only = False # if True, script exits right after the first eval\n",
        "always_save_checkpoint = True # if True, always save a checkpoint after each eval\n",
        "init_from = 'scratch' # 'scratch' or 'resume' or 'gpt2*'\n",
        "# wandb logging\n",
        "wandb_log = False # disabled by default\n",
        "wandb_project = 'owt'\n",
        "wandb_run_name = 'gpt2' # 'run' + str(time.time())\n",
        "# data\n",
        "# dataset = 'openwebtext'\n",
        "gradient_accumulation_steps = 5 # used to simulate larger batch sizes\n",
        "batch_size = 8 # if gradient_accumulation_steps > 1, this is the micro-batch size\n",
        "block_size = 512 \n",
        "# model\n",
        "n_layer = 12\n",
        "n_head = 12\n",
        "n_embd = 768\n",
        "dropout = 0.0 # for pretraining 0 is good, for finetuning try 0.1+\n",
        "bias = False # do we use bias inside LayerNorm and Linear layers?\n",
        "# adamw optimizer\n",
        "learning_rate = 6e-4 # max learning rate\n",
        "max_iters = 50 # total number of training iterations\n",
        "weight_decay = 1e-1\n",
        "beta1 = 0.9\n",
        "beta2 = 0.95\n",
        "grad_clip = 1.0 # clip gradients at this value, or disable if == 0.0\n",
        "# learning rate decay settings\n",
        "decay_lr = False # whether to decay the learning rate\n",
        "warmup_iters = 2000 # how many steps to warm up for\n",
        "lr_decay_iters = 600000 # should be ~= max_iters per Chinchilla\n",
        "min_lr = 6e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla\n",
        "# DDP settings\n",
        "backend = 'nccl' # 'nccl', 'gloo', etc.\n",
        "# system\n",
        "device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks\n",
        "dtype = 'bfloat16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
        "compile = False # use PyTorch 2.0 to compile the model to be faster\n",
        "device_type = 'cuda'\n",
        "master_process = True "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "c76IgkFUe1lm"
      },
      "outputs": [],
      "source": [
        "# dataset = ChessDataset(\"tal_white_games\", \"tal_black_games\")\n",
        "# train_size = int(0.8 * len(dataset))\n",
        "# val_size = (len(dataset) - train_size) // 2\n",
        "# test_size = val_size \n",
        "# train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-BvZaALge1lm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cb7f02a-edcb-4f04-a8f0-87dcd421f130"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1944\n",
            "243\n",
            "243\n"
          ]
        }
      ],
      "source": [
        "# print(len(train_dataset))\n",
        "# print(len(val_dataset))\n",
        "# print(len(test_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SF10G02Be1lm"
      },
      "outputs": [],
      "source": [
        "# train = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "# val = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "# test = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oPGgQZsPe1lm"
      },
      "outputs": [],
      "source": [
        "# chessAgentConfig = ChessAgentConfig()\n",
        "# chessAgent = ChessAgent(chessAgentConfig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BkIXyfnae1lm"
      },
      "outputs": [],
      "source": [
        "# fen_evals = {}  # Dictionary to cache position evaluations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0w6j4yfe1lm"
      },
      "outputs": [],
      "source": [
        "# def fen_to_number(fen):\n",
        "#     if fen == '<eos>': \n",
        "#         return 0 \n",
        "\n",
        "#     mapping = {\n",
        "#         '1': 0, '2': 1, '3': 2, '4': 3, '5': 4, '6': 5, '7': 6, '8': 7,\n",
        "#         'p': -1, 'n': -3, 'b': -3, 'r': -5, 'q': -9, 'k': 0,\n",
        "#         'P': 1, 'N': 3, 'B': 3, 'R': 5, 'Q': 9, 'K': 0\n",
        "#     }\n",
        "\n",
        "#     try: \n",
        "#         board, turn, castling, en_passant, half_move, full_move = fen.split(' ')[:6]\n",
        "#     except: \n",
        "#         print(\"error with fen: \", fen)\n",
        "        \n",
        "#     number = 0\n",
        "\n",
        "#     for char in board:\n",
        "#         if char == '/':\n",
        "#             continue\n",
        "#         elif char.isdigit():\n",
        "#             number += int(char)\n",
        "#         else:\n",
        "#             number += mapping[char]\n",
        "\n",
        "#     if turn in mapping:\n",
        "#         number += mapping[turn] * 100\n",
        "#     else:\n",
        "#         number += 0  # Assigning a default value if turn is not recognized\n",
        "\n",
        "#     number += castling_score(castling) * 1000\n",
        "\n",
        "#     if en_passant != '-':\n",
        "#         if en_passant in mapping:\n",
        "#             number += mapping[en_passant] * 10000\n",
        "#         else:\n",
        "#             number += 0  # Assigning a default value if en_passant is not recognized\n",
        "\n",
        "#     number += int(half_move) * 100000\n",
        "#     number += int(full_move) * 1000000\n",
        "\n",
        "#     # eval = pos_eval(fen)  # Evaluate the position using the pos_eval function\n",
        "#     # number += eval * 10000000  # Incorporate the position evaluation\n",
        "\n",
        "#     return number"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h5M2ODOze1lm"
      },
      "outputs": [],
      "source": [
        "# def fen_to_tensor(fen): \n",
        "#     if fen == '<eos>': \n",
        "#         return torch.zeros(128, dtype=torch.long)\n",
        "\n",
        "#     fen_vocab = ' wPNBRQKpnbrqk12345678/KQkq-abcdefgh0123456789'\n",
        "\n",
        "#     stoi = { ch:i for i, ch in enumerate(fen_vocab) }\n",
        "#     itos = { i:ch for i, ch in enumerate(fen_vocab) }\n",
        "\n",
        "#     def encode(fen): \n",
        "#         return [stoi[ch] for ch in fen]\n",
        "#     def decode(l): \n",
        "#         return [itos[i] for i in l]\n",
        "\n",
        "#     encode_fen = torch.tensor(encode(fen), dtype=torch.long)\n",
        "#     padded_encode_fen = F.pad(encode_fen, (0, 128 - len(encode_fen)), value=0)\n",
        "\n",
        "#     return padded_encode_fen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DoprGpPNe1lm"
      },
      "outputs": [],
      "source": [
        "# def pos_eval(fen):\n",
        "#     if fen in fen_evals: \n",
        "#         return fen_evals[fen]\n",
        "#     eval = chessAgent.position_eval(fen) \n",
        "#     fen_evals[fen] = eval\n",
        "\n",
        "#     return eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Ah0mrpmVe1lm"
      },
      "outputs": [],
      "source": [
        "def get_batch(split):\n",
        "    dataloader = train if split == 'train' else val if split == 'val' else test\n",
        "    batch = next(iter(dataloader))\n",
        "    # fens = [[fen_to_tensor(fen) for fen in game['FEN']] for game in batch]\n",
        "    fens = [[fen for fen in game['FEN']] for game in batch]\n",
        "    x = torch.stack([torch.from_numpy((game['np']).astype(np.int64)) for game in batch]) # shape batch_size x block_size \n",
        "    y = torch.stack([torch.cat((torch.from_numpy((game['np'][1:]).astype(np.int64)), torch.tensor([0]))) for game in batch]) # shape batch_size x block_size\n",
        "    # p = torch.stack([torch.stack([fen_tensor for fen_tensor in game]) for game in fens]) # shape batch_size x block_size x 128 \n",
        "    # print(x.shape)\n",
        "    # print(y.shape)\n",
        "    # print(p.shape)\n",
        "    if device == 'cuda': \n",
        "      # x, y, p = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True), p.pin_memory().to(device, non_blocking=True)\n",
        "      x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
        "    else: \n",
        "      x, y = x.to(device), y.to(device)\n",
        "    return x, y, fens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YFXsR1m_e1lm"
      },
      "outputs": [],
      "source": [
        "# x, y, p  = get_batch('train')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hL0_j9YMe1lm"
      },
      "source": [
        "### 2. Run Training Epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "HEvQVNgFe1lm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "outputId": "c39d3244-b106-4c32-ec9b-a83f5780bde1"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-17dd95d2e969>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mchessGPTConfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mChessGPTConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mChessGPT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchessGPTConfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-6e2a5e651e8e>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchess_agent_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mChessAgentConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchess_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mChessAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mChessAgentConfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_tries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-4ff2982fda03>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mChessAgent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSimpleEngine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopen_uci\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"stockfish\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/chess/engine.py\u001b[0m in \u001b[0;36mpopen_uci\u001b[0;34m(cls, command, timeout, debug, setpgrp, **popen_args)\u001b[0m\n\u001b[1;32m   2859\u001b[0m         \u001b[0mReturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mchess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSimpleEngine\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2860\u001b[0m         \"\"\"\n\u001b[0;32m-> 2861\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mUciProtocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msetpgrp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msetpgrp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpopen_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2862\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2863\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/chess/engine.py\u001b[0m in \u001b[0;36mpopen\u001b[0;34m(cls, Protocol, command, timeout, debug, setpgrp, **popen_args)\u001b[0m\n\u001b[1;32m   2851\u001b[0m             \u001b[0;32mawait\u001b[0m \u001b[0msimple_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2853\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mrun_in_background\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackground\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"{cls.__name__} (command={command!r})\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2854\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2855\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/chess/engine.py\u001b[0m in \u001b[0;36mrun_in_background\u001b[0;34m(coroutine, name, debug, _policy_lock)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0mthreading\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mThread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbackground\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    456\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m                 \u001b[0;31m# Break a reference cycle with the exception in self._exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/chess/engine.py\u001b[0m in \u001b[0;36mbackground\u001b[0;34m()\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbackground\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m             \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoroutine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfuture\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m             \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcancel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/asyncio/runners.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdebug\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_debug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_until_complete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/asyncio/base_events.py\u001b[0m in \u001b[0;36mrun_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    647\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Event loop stopped before Future completed.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 649\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/chess/engine.py\u001b[0m in \u001b[0;36mbackground\u001b[0;34m(future)\u001b[0m\n\u001b[1;32m   2839\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mProtocol\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mType\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mProtocol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommand\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msetpgrp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpopen_args\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mSimpleEngine\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2840\u001b[0m         \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbackground\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfuture\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mconcurrent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfutures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFuture\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mSimpleEngine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2841\u001b[0;31m             \u001b[0mtransport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mProtocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msetpgrp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msetpgrp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpopen_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2842\u001b[0m             \u001b[0mthreading\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{cls.__name__} (pid={transport.get_pid()})\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2843\u001b[0m             \u001b[0msimple_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/chess/engine.py\u001b[0m in \u001b[0;36mpopen\u001b[0;34m(cls, command, setpgrp, **popen_args)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                 \u001b[0mpopen_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"start_new_session\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_running_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubprocess_exec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpopen_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/asyncio/base_events.py\u001b[0m in \u001b[0;36msubprocess_exec\u001b[0;34m(self, protocol_factory, program, stdin, stdout, stderr, universal_newlines, shell, bufsize, encoding, errors, text, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1679\u001b[0m             \u001b[0mdebug_log\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'execute program {program!r}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1680\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_subprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdebug_log\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1681\u001b[0;31m         transport = await self._make_subprocess_transport(\n\u001b[0m\u001b[1;32m   1682\u001b[0m             \u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpopen_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1683\u001b[0m             bufsize, **kwargs)\n",
            "\u001b[0;32m/usr/lib/python3.10/asyncio/unix_events.py\u001b[0m in \u001b[0;36m_make_subprocess_transport\u001b[0;34m(self, protocol, args, shell, stdin, stdout, stderr, bufsize, extra, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m                                    \"subprocess support is not installed.\")\n\u001b[1;32m    206\u001b[0m             \u001b[0mwaiter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_future\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m             transp = _UnixSubprocessTransport(self, protocol, args, shell,\n\u001b[0m\u001b[1;32m    208\u001b[0m                                               \u001b[0mstdin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbufsize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m                                               \u001b[0mwaiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwaiter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextra\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/asyncio/base_subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, loop, protocol, args, shell, stdin, stdout, stderr, bufsize, waiter, extra, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# Create the child process: set the _proc attribute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             self._start(args=args, shell=shell, stdin=stdin, stdout=stdout,\n\u001b[0m\u001b[1;32m     37\u001b[0m                         stderr=stderr, bufsize=bufsize, **kwargs)\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/asyncio/unix_events.py\u001b[0m in \u001b[0;36m_start\u001b[0;34m(self, args, shell, stdin, stdout, stderr, bufsize, **kwargs)\u001b[0m\n\u001b[1;32m    797\u001b[0m             \u001b[0mstdin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdin_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocketpair\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    798\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 799\u001b[0;31m             self._proc = subprocess.Popen(\n\u001b[0m\u001b[1;32m    800\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshell\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstdin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    801\u001b[0m                 universal_newlines=False, bufsize=bufsize, **kwargs)\n",
            "\u001b[0;32m/usr/lib/python3.10/subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize)\u001b[0m\n\u001b[1;32m    969\u001b[0m                             encoding=encoding, errors=errors)\n\u001b[1;32m    970\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 971\u001b[0;31m             self._execute_child(args, executable, preexec_fn, close_fds,\n\u001b[0m\u001b[1;32m    972\u001b[0m                                 \u001b[0mpass_fds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    973\u001b[0m                                 \u001b[0mstartupinfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreationflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshell\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session)\u001b[0m\n\u001b[1;32m   1861\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0merrno_num\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1862\u001b[0m                         \u001b[0merr_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrerror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1863\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1864\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'stockfish'"
          ]
        }
      ],
      "source": [
        "chessGPTConfig = ChessGPTConfig()\n",
        "model = ChessGPT(chessGPTConfig)\n",
        "\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5fVCw03We1ln"
      },
      "outputs": [],
      "source": [
        "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89nyyX6Qe1ln",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad7da724-3aba-436b-ce07-47355c0c91f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using fused AdamW: True\n"
          ]
        }
      ],
      "source": [
        "optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nCh-OmvAe1ln"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y, fens = get_batch(split)\n",
        "            with ctx:\n",
        "                logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9GXdT7se1ln"
      },
      "outputs": [],
      "source": [
        "# learning rate decay scheduler (cosine with warmup)\n",
        "def get_lr(it):\n",
        "    # 1) linear warmup for warmup_iters steps\n",
        "    if it < warmup_iters:\n",
        "        return learning_rate * it / warmup_iters\n",
        "    # 2) if it > lr_decay_iters, return min learning rate\n",
        "    if it > lr_decay_iters:\n",
        "        return min_lr\n",
        "    # 3) in between, use cosine decay down to min learning rate\n",
        "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
        "    return min_lr + coeff * (learning_rate - min_lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BNIvaw9re1ln"
      },
      "outputs": [],
      "source": [
        "X, Y, fens = get_batch('train')\n",
        "t0 = time.time() \n",
        "local_iter_num = 0 \n",
        "raw_model = model  \n",
        "running_mfu = -1.0\n",
        "ctx = nullcontext()\n",
        "iter_num = 0\n",
        "best_val_loss = 1e9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t5eWbidne1ln",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1292e350-cb61-4d07-d2a7-a1b71ba63ce7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 12.4432, val loss 12.4685\n",
            "iter 0: loss 12.4307, time 4796.69ms, mfu -100.00%\n",
            "step 1: train loss 12.4285, val loss 12.4457\n",
            "iter 1: loss 12.3819, time 4833.55ms, mfu -100.00%\n",
            "step 2: train loss 12.1354, val loss 12.1664\n",
            "iter 2: loss 12.1003, time 4751.23ms, mfu -100.00%\n",
            "step 3: train loss 11.6008, val loss 11.5721\n",
            "iter 3: loss 11.6053, time 4770.88ms, mfu -100.00%\n",
            "step 4: train loss 10.7850, val loss 10.7267\n",
            "iter 4: loss 10.7359, time 4783.40ms, mfu -100.00%\n",
            "step 5: train loss 9.7109, val loss 9.6883\n",
            "iter 5: loss 9.6980, time 4772.66ms, mfu 3.38%\n",
            "step 6: train loss 8.5475, val loss 8.4733\n",
            "iter 6: loss 8.4882, time 4763.70ms, mfu 3.38%\n",
            "step 7: train loss 7.3189, val loss 7.1700\n",
            "iter 7: loss 7.2690, time 4901.07ms, mfu 3.37%\n",
            "step 8: train loss 6.2450, val loss 6.2772\n",
            "iter 8: loss 6.3497, time 4757.95ms, mfu 3.37%\n",
            "step 9: train loss 5.1681, val loss 5.1839\n",
            "iter 9: loss 5.1882, time 4744.31ms, mfu 3.38%\n",
            "step 10: train loss 4.4876, val loss 4.4810\n",
            "iter 10: loss 4.6231, time 4764.38ms, mfu 3.38%\n",
            "step 11: train loss 3.8060, val loss 3.5651\n",
            "iter 11: loss 3.6969, time 4752.87ms, mfu 3.38%\n",
            "step 12: train loss 3.1388, val loss 2.8242\n",
            "iter 12: loss 2.7019, time 4905.95ms, mfu 3.37%\n",
            "step 13: train loss 2.5583, val loss 2.6409\n",
            "iter 13: loss 2.6451, time 4743.16ms, mfu 3.37%\n",
            "step 14: train loss 2.1451, val loss 2.0501\n",
            "iter 14: loss 1.8749, time 4734.40ms, mfu 3.38%\n",
            "step 15: train loss 1.9131, val loss 2.0273\n",
            "iter 15: loss 2.2215, time 4739.28ms, mfu 3.38%\n",
            "step 16: train loss 1.8791, val loss 2.0173\n",
            "iter 16: loss 1.7791, time 4760.01ms, mfu 3.38%\n",
            "step 17: train loss 1.8439, val loss 1.7832\n",
            "iter 17: loss 1.8521, time 4910.86ms, mfu 3.37%\n",
            "step 18: train loss 1.9620, val loss 1.6179\n",
            "iter 18: loss 2.3147, time 4750.07ms, mfu 3.37%\n",
            "step 19: train loss 1.6779, val loss 1.8897\n",
            "iter 19: loss 1.7457, time 4758.27ms, mfu 3.38%\n",
            "step 20: train loss 1.6486, val loss 1.7450\n",
            "iter 20: loss 1.5593, time 4740.67ms, mfu 3.38%\n",
            "step 21: train loss 1.5653, val loss 1.7450\n",
            "iter 21: loss 1.6609, time 4743.93ms, mfu 3.38%\n",
            "step 22: train loss 1.4738, val loss 1.6122\n",
            "iter 22: loss 1.5519, time 4737.45ms, mfu 3.38%\n",
            "step 23: train loss 1.6559, val loss 1.6880\n",
            "iter 23: loss 1.5315, time 4749.01ms, mfu 3.38%\n",
            "step 24: train loss 1.5910, val loss 1.5685\n",
            "iter 24: loss 2.4293, time 4931.68ms, mfu 3.37%\n",
            "step 25: train loss 1.7245, val loss 1.4836\n",
            "iter 25: loss 1.7321, time 4752.45ms, mfu 3.38%\n",
            "step 26: train loss 1.6459, val loss 1.4558\n",
            "iter 26: loss 1.8372, time 4761.38ms, mfu 3.38%\n",
            "step 27: train loss 1.5968, val loss 1.5792\n",
            "iter 27: loss 1.5601, time 4759.95ms, mfu 3.38%\n",
            "step 28: train loss 1.6398, val loss 1.5079\n",
            "iter 28: loss 1.3188, time 4774.52ms, mfu 3.38%\n",
            "step 29: train loss 1.6177, val loss 1.3704\n",
            "iter 29: loss 1.6370, time 4747.54ms, mfu 3.38%\n",
            "step 30: train loss 1.6172, val loss 1.4284\n",
            "iter 30: loss 1.4207, time 4938.83ms, mfu 3.37%\n",
            "step 31: train loss 1.7047, val loss 1.4422\n",
            "iter 31: loss 1.4153, time 4780.83ms, mfu 3.37%\n",
            "step 32: train loss 1.6734, val loss 1.6216\n",
            "iter 32: loss 1.5488, time 4775.82ms, mfu 3.37%\n",
            "step 33: train loss 1.5220, val loss 1.3732\n",
            "iter 33: loss 1.6530, time 4748.10ms, mfu 3.37%\n",
            "step 34: train loss 1.6491, val loss 1.3906\n",
            "iter 34: loss 1.4340, time 4800.87ms, mfu 3.37%\n",
            "step 35: train loss 1.5943, val loss 1.5026\n",
            "iter 35: loss 1.0270, time 4759.46ms, mfu 3.37%\n",
            "step 36: train loss 1.4998, val loss 1.1806\n",
            "iter 36: loss 1.5210, time 4736.19ms, mfu 3.38%\n",
            "step 37: train loss 1.4922, val loss 1.4029\n",
            "iter 37: loss 1.6005, time 4751.43ms, mfu 3.38%\n",
            "step 38: train loss 1.2443, val loss 1.4277\n",
            "iter 38: loss 1.6667, time 4736.59ms, mfu 3.38%\n",
            "step 39: train loss 1.6060, val loss 1.4072\n",
            "iter 39: loss 1.8462, time 4753.85ms, mfu 3.38%\n",
            "step 40: train loss 1.6577, val loss 1.0929\n",
            "iter 40: loss 1.2515, time 4746.13ms, mfu 3.38%\n",
            "step 41: train loss 1.4704, val loss 1.2603\n",
            "iter 41: loss 1.4357, time 4902.53ms, mfu 3.37%\n",
            "step 42: train loss 1.4981, val loss 1.3846\n",
            "iter 42: loss 1.2830, time 4761.10ms, mfu 3.38%\n",
            "step 43: train loss 1.2697, val loss 1.1698\n",
            "iter 43: loss 1.2724, time 4738.46ms, mfu 3.38%\n",
            "step 44: train loss 1.2987, val loss 1.3981\n",
            "iter 44: loss 1.8935, time 4754.25ms, mfu 3.38%\n",
            "step 45: train loss 1.3544, val loss 1.4606\n",
            "iter 45: loss 1.5458, time 4765.30ms, mfu 3.38%\n",
            "step 46: train loss 1.4643, val loss 1.3603\n",
            "iter 46: loss 1.6787, time 4754.43ms, mfu 3.38%\n",
            "step 47: train loss 1.3802, val loss 1.4618\n",
            "iter 47: loss 1.1438, time 4937.45ms, mfu 3.37%\n",
            "step 48: train loss 1.3922, val loss 1.1710\n",
            "iter 48: loss 1.4799, time 4754.86ms, mfu 3.37%\n",
            "step 49: train loss 1.3617, val loss 1.3635\n",
            "iter 49: loss 1.3651, time 4761.06ms, mfu 3.37%\n",
            "step 50: train loss 1.3881, val loss 1.4293\n",
            "iter 50: loss 1.6135, time 4768.51ms, mfu 3.38%\n",
            "step 51: train loss 1.3833, val loss 1.3533\n",
            "iter 51: loss 1.7311, time 4754.54ms, mfu 3.38%\n",
            "step 52: train loss 1.3202, val loss 1.3484\n",
            "iter 52: loss 1.1708, time 4895.46ms, mfu 3.37%\n",
            "step 53: train loss 1.3613, val loss 1.2095\n",
            "iter 53: loss 1.3147, time 4743.22ms, mfu 3.37%\n",
            "step 54: train loss 1.3103, val loss 1.3189\n",
            "iter 54: loss 1.5348, time 4744.21ms, mfu 3.38%\n",
            "step 55: train loss 1.2328, val loss 1.5122\n",
            "iter 55: loss 1.4134, time 4761.79ms, mfu 3.38%\n",
            "step 56: train loss 1.3971, val loss 0.9549\n",
            "iter 56: loss 1.3087, time 4727.19ms, mfu 3.38%\n",
            "step 57: train loss 1.3178, val loss 1.3197\n",
            "iter 57: loss 1.0329, time 4906.21ms, mfu 3.37%\n",
            "step 58: train loss 1.3677, val loss 1.1729\n",
            "iter 58: loss 1.3171, time 4743.65ms, mfu 3.37%\n",
            "step 59: train loss 1.3435, val loss 1.2492\n",
            "iter 59: loss 1.2580, time 4743.38ms, mfu 3.38%\n",
            "step 60: train loss 1.2991, val loss 1.2797\n",
            "iter 60: loss 1.3661, time 4744.69ms, mfu 3.38%\n",
            "step 61: train loss 1.4273, val loss 1.4090\n",
            "iter 61: loss 0.9321, time 4765.21ms, mfu 3.38%\n",
            "step 62: train loss 1.2523, val loss 1.3472\n",
            "iter 62: loss 1.3089, time 4907.68ms, mfu 3.37%\n",
            "step 63: train loss 1.4862, val loss 1.3993\n",
            "iter 63: loss 1.0609, time 4764.52ms, mfu 3.37%\n",
            "step 64: train loss 1.3258, val loss 1.3666\n",
            "iter 64: loss 1.1398, time 4754.37ms, mfu 3.37%\n",
            "step 65: train loss 1.3340, val loss 1.2257\n",
            "iter 65: loss 1.3044, time 4749.98ms, mfu 3.38%\n",
            "step 66: train loss 1.3472, val loss 1.1816\n",
            "iter 66: loss 1.3864, time 4768.77ms, mfu 3.38%\n",
            "step 67: train loss 1.2343, val loss 1.3245\n",
            "iter 67: loss 1.1176, time 4923.14ms, mfu 3.37%\n",
            "step 68: train loss 1.3919, val loss 1.2612\n",
            "iter 68: loss 1.3401, time 4752.44ms, mfu 3.37%\n",
            "step 69: train loss 1.4981, val loss 1.2336\n",
            "iter 69: loss 1.1893, time 4760.15ms, mfu 3.37%\n",
            "step 70: train loss 1.3610, val loss 1.2211\n",
            "iter 70: loss 1.0167, time 4747.11ms, mfu 3.37%\n",
            "step 71: train loss 1.3362, val loss 1.3540\n",
            "iter 71: loss 1.3798, time 4765.28ms, mfu 3.38%\n",
            "step 72: train loss 1.4027, val loss 1.3044\n",
            "iter 72: loss 1.3441, time 4934.34ms, mfu 3.36%\n",
            "step 73: train loss 1.4551, val loss 1.3218\n",
            "iter 73: loss 1.7899, time 4757.75ms, mfu 3.37%\n",
            "step 74: train loss 1.3656, val loss 1.5273\n",
            "iter 74: loss 1.0682, time 4777.98ms, mfu 3.37%\n",
            "step 75: train loss 1.3539, val loss 1.3852\n",
            "iter 75: loss 1.4558, time 4754.64ms, mfu 3.37%\n",
            "step 76: train loss 1.3321, val loss 1.1980\n",
            "iter 76: loss 1.1287, time 4766.44ms, mfu 3.37%\n",
            "step 77: train loss 1.3019, val loss 1.0575\n",
            "iter 77: loss 0.9921, time 4905.17ms, mfu 3.36%\n",
            "step 78: train loss 1.3487, val loss 1.2095\n",
            "iter 78: loss 1.2901, time 4752.10ms, mfu 3.37%\n",
            "step 79: train loss 1.4036, val loss 1.2337\n",
            "iter 79: loss 1.3270, time 4785.64ms, mfu 3.37%\n",
            "step 80: train loss 1.1701, val loss 1.2249\n",
            "iter 80: loss 1.1516, time 4744.57ms, mfu 3.37%\n",
            "step 81: train loss 1.0814, val loss 1.0454\n",
            "iter 81: loss 1.4061, time 4739.62ms, mfu 3.37%\n",
            "step 82: train loss 1.3231, val loss 1.1167\n",
            "iter 82: loss 1.5867, time 4755.62ms, mfu 3.38%\n",
            "step 83: train loss 1.2498, val loss 1.2868\n",
            "iter 83: loss 0.9059, time 4760.59ms, mfu 3.38%\n",
            "step 84: train loss 1.3068, val loss 1.1672\n",
            "iter 84: loss 1.4810, time 4922.35ms, mfu 3.37%\n",
            "step 85: train loss 1.3345, val loss 1.2235\n",
            "iter 85: loss 1.3672, time 4758.01ms, mfu 3.37%\n",
            "step 86: train loss 1.3751, val loss 1.0254\n",
            "iter 86: loss 1.3115, time 4754.73ms, mfu 3.37%\n",
            "step 87: train loss 1.2126, val loss 1.1969\n",
            "iter 87: loss 1.1809, time 4755.92ms, mfu 3.37%\n",
            "step 88: train loss 1.2313, val loss 1.2599\n",
            "iter 88: loss 1.1605, time 4760.51ms, mfu 3.38%\n",
            "step 89: train loss 1.1985, val loss 1.2421\n",
            "iter 89: loss 0.9781, time 4761.08ms, mfu 3.38%\n",
            "step 90: train loss 1.2881, val loss 1.1251\n",
            "iter 90: loss 1.2590, time 4934.79ms, mfu 3.37%\n",
            "step 91: train loss 1.1516, val loss 1.1657\n",
            "iter 91: loss 1.0588, time 4775.01ms, mfu 3.37%\n",
            "step 92: train loss 1.0530, val loss 1.2089\n",
            "iter 92: loss 0.9783, time 4752.87ms, mfu 3.37%\n",
            "step 93: train loss 1.1752, val loss 1.3253\n",
            "iter 93: loss 1.0742, time 4755.40ms, mfu 3.37%\n",
            "step 94: train loss 1.1971, val loss 1.0858\n",
            "iter 94: loss 1.1129, time 4738.09ms, mfu 3.38%\n",
            "step 95: train loss 1.1425, val loss 0.9950\n",
            "iter 95: loss 1.1099, time 4729.47ms, mfu 3.38%\n",
            "step 96: train loss 1.3344, val loss 1.0709\n",
            "iter 96: loss 0.8911, time 4753.88ms, mfu 3.38%\n",
            "step 97: train loss 1.0385, val loss 0.9492\n",
            "iter 97: loss 1.4454, time 4898.71ms, mfu 3.37%\n",
            "step 98: train loss 1.2098, val loss 1.0165\n",
            "iter 98: loss 1.1439, time 4748.86ms, mfu 3.37%\n",
            "step 99: train loss 1.0831, val loss 0.9963\n",
            "iter 99: loss 1.3708, time 4735.67ms, mfu 3.38%\n",
            "step 100: train loss 1.1559, val loss 1.1172\n",
            "iter 100: loss 1.2482, time 4754.77ms, mfu 3.38%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjIAAAGwCAYAAACzXI8XAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABjBUlEQVR4nO3dd3hUddrG8e+k9wppkBBK6KGDUhQUFFARUNfGKtgLFvS1rmJHLKuLfS0r2BAr2EVEeu+9twSSEEJI7zPn/eOE0UgLIeTMhPtzXXOZmTlz5pljYG5+1WYYhoGIiIiIG/KwugARERGRmlKQEREREbelICMiIiJuS0FGRERE3JaCjIiIiLgtBRkRERFxWwoyIiIi4ra8rC7gdHM4HKSlpREcHIzNZrO6HBEREakGwzDIz88nLi4OD49jt7vU+yCTlpZGfHy81WWIiIhIDaSmptK4ceNjPl/vg0xwcDBgXoiQkBCLqxEREZHqyMvLIz4+3vk9fiz1Psgc7k4KCQlRkBEREXEzJxoWosG+IiIi4rYUZERERMRtKciIiIiI26r3Y2REROTU2O12ysvLrS5D6hlvb288PT1P+TwKMiIiclSGYZCRkUFOTo7VpUg9FRYWRkxMzCmt86YgIyIiR3U4xERFRREQEKBFRaXWGIZBUVERmZmZAMTGxtb4XAoyIiJyBLvd7gwxkZGRVpcj9ZC/vz8AmZmZREVF1bibSYN9RUTkCIfHxAQEBFhcidRnh3+/TmUMloKMiIgck7qT5HSqjd8vBRkRERFxWwoyIiIi4rYUZERERE4gMTGRCRMmWF2GHIWCTE3lpcGhPeCwW12JiIhUstlsx7099dRTNTrvsmXLuPXWW0+ptn79+jFmzJhTOoccSdOvayh9+qvEbngfu4cP5aGJ2CKa4x3dEo/kyyG2o9XliYickdLT050/f/HFFzzxxBNs2bLF+VhQUJDzZ8MwsNvteHmd+KuwYcOGtVuo1Bq1yNTQnv1ZlBpeeDrK8Du0Fd8dv+Cx8DXK3+2P4+Auq8sTEal1hmFQVFZhyc0wjGrVGBMT47yFhoZis9mc9zdv3kxwcDC//PILXbt2xdfXl/nz57Njxw6GDh1KdHQ0QUFBdO/end9//73Kef/etWSz2fjggw8YPnw4AQEBJCUl8f3335/S9f3mm29o164dvr6+JCYm8sorr1R5/u233yYpKQk/Pz+io6O54oornM99/fXXJCcn4+/vT2RkJAMGDKCwsPCU6nEXapGpoa1dn+LdTbdhy91LYMFuIktSudxzDskeu9n77b9ofMvnVpcoIlKrisvttH1iuiXvvfGZgQT41M5X1iOPPMK///1vmjVrRnh4OKmpqVx00UWMGzcOX19fPv74Y4YMGcKWLVtISEg45nmefvppXnrpJV5++WXeeOMNRowYwZ49e4iIiDjpmlasWMGVV17JU089xVVXXcXChQu58847iYyMZNSoUSxfvpx77rmHTz75hF69epGdnc28efMAsxXqmmuu4aWXXmL48OHk5+czb968aoc/d6cgU0PX90zk+p6Jzvt2h8GU73+k3arraLzvZypSluGV0N26AkVE5KieeeYZLrjgAuf9iIgIOnb8c0jAs88+y9SpU/n++++56667jnmeUaNGcc011wDw/PPP8/rrr7N06VIGDRp00jW9+uqr9O/fn7FjxwLQsmVLNm7cyMsvv8yoUaNISUkhMDCQSy65hODgYJo0aULnzp0BM8hUVFRw2WWX0aRJEwCSk5NPugZ3pSBTSzw9bAwbPJgf1/TjUmMWB6c+RPQ9f4AWkxKResLf25ONzwy07L1rS7du3arcLygo4KmnnuKnn35yhoLi4mJSUlKOe54OHTo4fw4MDCQkJMS5d9DJ2rRpE0OHDq3yWO/evZkwYQJ2u50LLriAJk2a0KxZMwYNGsSgQYOc3VodO3akf//+JCcnM3DgQC688EKuuOIKwsPDa1SLu9EYmVoU6OtFSZ9HKDZ8iD60ktL1P1hdkohIrbHZbAT4eFlyq80VhgMDA6vcf+CBB5g6dSrPP/888+bNY/Xq1SQnJ1NWVnbc83h7ex9xfRwOR63V+VfBwcGsXLmSzz//nNjYWJ544gk6duxITk4Onp6ezJgxg19++YW2bdvyxhtv0KpVK3btOjPGayrI1LKhfbvzhdelABT//BjYa75/hIiInH4LFixg1KhRDB8+nOTkZGJiYti9e3ed1tCmTRsWLFhwRF0tW7Z0bqbo5eXFgAEDeOmll1i7di27d+/mjz/+AMwQ1bt3b55++mlWrVqFj48PU6dOrdPPYBV1LdUyXy9Pwi54kKxffqNBcQrFi/+Hf+/brS5LRESOISkpiW+//ZYhQ4Zgs9kYO3bsaWtZOXDgAKtXr67yWGxsLP/3f/9H9+7defbZZ7nqqqtYtGgRb775Jm+//TYAP/74Izt37uTcc88lPDycn3/+GYfDQatWrViyZAkzZ87kwgsvJCoqiiVLlnDgwAHatGlzWj6Dq1GLzGkwpEcrPvO/FgBj1ngoybW4IhEROZZXX32V8PBwevXqxZAhQxg4cCBdunQ5Le81efJkOnfuXOX2/vvv06VLF7788kumTJlC+/bteeKJJ3jmmWcYNWoUAGFhYXz77becf/75tGnThv/+9798/vnntGvXjpCQEObOnctFF11Ey5Ytefzxx3nllVcYPHjwafkMrsZm1PP5WXl5eYSGhpKbm0tISEidve/0tak0//pCWnikUXDOWIL6P1Bn7y0icqpKSkrYtWsXTZs2xc/Pz+pypJ463u9Zdb+/1SJzmlyY3JjfQ4YBkL32F2uLERERqacsDTJz585lyJAhxMXFYbPZmDZtmvO58vJyHn74YZKTkwkMDCQuLo7rr7+etLQ06wo+CTabjcTuFwEQk7sGyostrkhERKT+sTTIFBYW0rFjR956660jnisqKmLlypWMHTuWlStX8u2337JlyxYuvfRSCyqtma6du5NhhONDOYe2zLe6HBERkXrH0llLgwcPPuZgpNDQUGbMmFHlsTfffJMePXqQkpJy3GWjXUXDED/+8OtMTOkfpK+eTnj7C078IhEREak2t5p+nZubi81mIyws7JjHlJaWUlpa6ryfl5dXB5UdW1l8H9j+B/571SIjIiJS29xmsG9JSQkPP/ww11xzzXFHL48fP57Q0FDnLT4+vg6rPFLjLuaeGwnFmykvPGRpLSIiIvWNWwSZ8vJyrrzySgzD4J133jnusY8++ii5ubnOW2pqah1VeXRtWrdlD7F42gx2LJ9x4heIiIhItbl8kDkcYvbs2cOMGTNOuBaMr68vISEhVW5W8vSwsS/M3AU7b9NMS2sRERGpb1w6yBwOMdu2beP3338nMjLS6pJqxDupHwANMhdbW4iIiFRLv379GDNmjPN+YmIiEyZMOO5r/r6MSE3V1nnOFJYGmYKCAlavXu3cd2LXrl2sXr2alJQUysvLueKKK1i+fDmfffYZdrudjIwMMjIyTrgjqatJ6mHOzGrm2E162vG3hRcRkZobMmQIgwYNOupz8+bNw2azsXbt2pM+77Jly7j11ltPtbwqnnrqKTp16nTE4+np6ad9e4FJkyYdd+KMO7E0yCxfvty51wTA/fffT+fOnXniiSfYt28f33//PXv37qVTp07ExsY6bwsXLrSy7JMW1jCOXV7NANixVKv8ioicLjfddBMzZsxg7969Rzw3ceJEunXrRocOHU76vA0bNiQgIKA2SjyhmJgYfH196+S96gNLg0y/fv0wDOOI26RJk0hMTDzqc4Zh0K9fPyvLrpGc6J4A2HfMsbgSEZH665JLLqFhw4ZMmjSpyuMFBQV89dVX3HTTTRw8eJBrrrmGRo0aERAQQHJyMp9//vlxz/v3rqVt27Zx7rnn4ufnR9u2bY9Y9wzg4YcfpmXLlgQEBNCsWTPGjh1LeXk5YLaIPP3006xZswabzYbNZnPW/PeupXXr1nH++efj7+9PZGQkt956KwUFBc7nR40axbBhw/j3v/9NbGwskZGRjB492vleNZGSksLQoUMJCgoiJCSEK6+8kv379zufX7NmDeeddx7BwcGEhITQtWtXli9fDsCePXsYMmQI4eHhBAYG0q5dO37++eca13IibrWOjDsLbz8A9n1GYt5ySivs+Hp5Wl2SiMjJMQwoL7Lmvb0DwGY74WFeXl5cf/31TJo0icceewxb5Wu++uor7HY711xzDQUFBXTt2pWHH36YkJAQfvrpJ6677jqaN29Ojx49TvgeDoeDyy67jOjoaJYsWUJubm6V8TSHBQcHM2nSJOLi4li3bh233HILwcHBPPTQQ1x11VWsX7+eX3/9ld9//x0wF4L9u8LCQgYOHEjPnj1ZtmwZmZmZ3Hzzzdx1111VwtqsWbOIjY1l1qxZbN++nauuuopOnTpxyy23nPDzHO3zHQ4xc+bMoaKigtGjR3PVVVcxe/ZsAEaMGEHnzp1555138PT0ZPXq1Xh7ewMwevRoysrKmDt3LoGBgWzcuJGgoKCTrqO6FGTqSJPOA6iY7kET236WrltHj86drC5JROTklBfB83HWvPe/0sAnsFqH3njjjbz88svMmTPH2YI/ceJELr/8cucaYw888IDz+Lvvvpvp06fz5ZdfVivI/P7772zevJnp06cTF2dej+eff/6IcS2PP/648+fExEQeeOABpkyZwkMPPYS/vz9BQUF4eXkRExNzzPeaPHkyJSUlfPzxxwQGmp//zTffZMiQIbz44otER0cDEB4ezptvvomnpyetW7fm4osvZubMmTUKMjNnzmTdunXs2rXLuRbbxx9/TLt27Vi2bBndu3cnJSWFBx98kNatWwOQlJTkfH1KSgqXX345ycnJADRr1uykazgZLj1rqT6x+YWw178NABmrp1tcjYhI/dW6dWt69erFhx9+CMD27duZN28eN910EwB2u51nn32W5ORkIiIiCAoKYvr06aSkVG8yxqZNm4iPj3eGGICePXsecdwXX3xB7969iYmJISgoiMcff7za7/HX9+rYsaMzxAD07t0bh8PBli1bnI+1a9cOT88/W/pjY2PJzMw8qff663vGx8dXWVC2bdu2hIWFsWnTJsAc03rzzTczYMAAXnjhBXbs2OE89p577uG5556jd+/ePPnkkzUaXH0y1CJThyqanAubN+C/b4HVpYiInDzvALNlxKr3Pgk33XQTd999N2+99RYTJ06kefPm9O3bF4CXX36Z1157jQkTJpCcnExgYCBjxoyp1RmxixYtYsSIETz99NMMHDiQ0NBQpkyZwiuvvFJr7/FXh7t1DrPZbDgcjtPyXmDOuLr22mv56aef+OWXX3jyySeZMmUKw4cP5+abb2bgwIH89NNP/Pbbb4wfP55XXnmFu++++7TUohaZOhTb+UIAOpav4UBeicXViIicJJvN7N6x4laN8TF/deWVV+Lh4cHkyZP5+OOPufHGG53jZRYsWMDQoUP55z//SceOHWnWrBlbt26t9rnbtGlDamoq6enpzscWL666TtjChQtp0qQJjz32GN26dSMpKYk9e/ZUOcbHxwe73X7C91qzZg2FhYXOxxYsWICHhwetWrWqds0n4/Dn++vK+Bs3biQnJ4e2bds6H2vZsiX33Xcfv/32G5dddhkTJ050PhcfH8/tt9/Ot99+y//93//x/vvvn5ZaQUGmTgU260UFnkTZctixY8uJXyAiIjUSFBTEVVddxaOPPkp6ejqjRo1yPpeUlMSMGTNYuHAhmzZt4rbbbqsyI+dEBgwYQMuWLRk5ciRr1qxh3rx5PPbYY1WOSUpKIiUlhSlTprBjxw5ef/11pk6dWuWYxMRE5/ppWVlZVTY8PmzEiBH4+fkxcuRI1q9fz6xZs7j77ru57rrrnONjasputzvXcjt827RpEwMGDCA5OZkRI0awcuVKli5dyvXXX0/fvn3p1q0bxcXF3HXXXcyePZs9e/awYMECli1bRps25vCJMWPGMH36dHbt2sXKlSuZNWuW87nTQUGmLnn7kemTAMDBnassLkZEpH676aabOHToEAMHDqwynuXxxx+nS5cuDBw4kH79+hETE8OwYcOqfV4PDw+mTp1KcXExPXr04Oabb2bcuHFVjrn00ku57777uOuuu+jUqRMLFy5k7NixVY65/PLLGTRoEOeddx4NGzY86hTwgIAApk+fTnZ2Nt27d+eKK66gf//+vPnmmyd3MY6ioKDAuZbb4duQIUOw2Wx89913hIeHc+655zJgwACaNWvGF198AYCnpycHDx7k+uuvp2XLllx55ZUMHjyYp59+GjAD0ujRo2nTpg2DBg2iZcuWvP3226dc77HYDMMwTtvZXUBeXh6hoaHk5uZavu8SwNa3r6Jl5q98H3kTl979qtXliIgcVUlJCbt27aJp06b4+flZXY7UU8f7Pavu97daZOqYV5y5omRw7maLKxEREXF/CjJ1LKJ5FwASynZSWFphcTUiIiLuTUGmjoUlmkEm0ZbBlr01m+MvIiIiJgWZuhYcTa5HGJ42g4ytK6yuRkRExK0pyFjgUHBLAIr3nt7VDkVETlU9nw8iFquN3y8FGQs4otoB4Htwo8WViIgc3eGVYouKLNokUs4Ih3+//r4y8cnQFgUWCG7SGbZNJLp4OxV2B16eypMi4lo8PT0JCwtz7tcTEBDgXBlX5FQZhkFRURGZmZmEhYVV2SfqZCnIWCCyeRf4HVqRwo7MAlrFWr++jYjI3x3elbmmmw+KnEhYWNhxd/+uDgUZC3g0bEUFXoTYili0cxOtYs+yuiQRkSPYbDZiY2OJioqivLzc6nKknvH29j6llpjDFGSs4OXDAf9EYou3k7NrNfRWkBER1+Xp6VkrXzgip4MGZ1ikNMLcQMtj/3qLKxEREXFfCjIW8W3cEYCw/C2a3igiIlJDCjIWiazcqqCFYzdpuSUWVyMiIuKeFGQs4tPIbJFpYstky550i6sRERFxTwoyVglsQK5XJB42g8wdq6yuRkRExC0pyFgoL7Q1ABVp2qpARESkJhRkLOQZY25VEHBos8WViIiIuCcFGQuFNTMH/CaU7yS3SItNiYiInCwFGQsFxHcCoLUthQ1ph6wtRkRExA0pyFgpMolymzdBthJSd2yyuhoRERG3oyBjJU8vsgOaA1C2b53FxYiIiLgfBRmLlUSYM5f8DqlFRkRE5GQpyFjMK8bccymkcI/FlYiIiLgfBRmLhTY2g0xcRSrFZXaLqxEREXEvCjIWC4ozg0xTWwa7swosrkZERMS9KMhYLTwROx4E2UpI37vb6mpERETcioKM1bx8yPaOASAvTSv8ioiInAwFGRdQEJQIQEXmNmsLERERcTMKMi7AEW6uJeOTu9PiSkRERNyLgowL8I1uCUBoUYrFlYiIiLgXBRkXEBbfFoDG9r0UllZYXI2IiIj7UJBxAUFxrQBIsGWy+0CuxdWIiIi4DwUZVxDSiFJ88LbZyUzRgF8REZHqUpBxBR4eHPRtDEChpmCLiIhUm4KMiygMbgqAPUstMiIiItWlIOMijIgWAPjm7rK4EhEREfehIOMi/GPMKdjhJZqCLSIiUl0KMi4ioknlFGxHGvkl5RZXIyIi4h4UZFxEYGxrABrZDpKSkWVxNSIiIu7B0iAzd+5chgwZQlxcHDabjWnTplV53jAMnnjiCWJjY/H392fAgAFs21ZPB8MGRJBvCwYgK2WTxcWIiIi4B0uDTGFhIR07duStt9466vMvvfQSr7/+Ov/9739ZsmQJgYGBDBw4kJKSkjqutG4c9EsAoChti8WViIiIuAcvK9988ODBDB48+KjPGYbBhAkTePzxxxk6dCgAH3/8MdHR0UybNo2rr766LkutEyXBiVC8AePgdqtLERERcQsuO0Zm165dZGRkMGDAAOdjoaGhnHXWWSxatOiYrystLSUvL6/KzW00MKdg++drCraIiEh1uGyQycjIACA6OrrK49HR0c7njmb8+PGEhoY6b/Hx8ae1ztoUEGvuuRRZkmpxJSIiIu7BZYNMTT366KPk5uY6b6mp7hMKGiS2ByDeSCO3WFOwRURETsRlg0xMTAwA+/fvr/L4/v37nc8dja+vLyEhIVVu7iIgOgmAcFsBe/fttbgaERER1+eyQaZp06bExMQwc+ZM52N5eXksWbKEnj17WljZaeQTQJZHQwAO7tlgcTEiIiKuz9JZSwUFBWzf/ucMnV27drF69WoiIiJISEhgzJgxPPfccyQlJdG0aVPGjh1LXFwcw4YNs67o0+yQfwINCg9QkrHV6lJERERcnqVBZvny5Zx33nnO+/fffz8AI0eOZNKkSTz00EMUFhZy6623kpOTQ58+ffj111/x8/OzquTTriS0KRSuwJatKdgiIiInYmmQ6devH4ZhHPN5m83GM888wzPPPFOHVVnLo0ESpEFg/m6rSxEREXF5LjtG5kwVFGdOwW5Q5j6zrURERKyiIONiGjZpB0CCkU5uYanF1YiIiLg2BRkXExDVjAo88LOVk75vt9XliIiIuDQFGVfj6cXByinYh/bV052+RUREaomCjAvK84sFoChzp8WViIiIuDYFGRdUGmTuD+U4tMfiSkRERFybgowrCmsCgE++Zi6JiIgcj4KMC/Jv2BSA4OI0iysRERFxbQoyLii0UQsAGtj3U2F3WFyNiIiI61KQcUERcWaQiSOL9EMFFlcjIiLiuhRkXJBHSBzleOFlc5C5VzOXREREjkVBxhV5eHDQKwqA3PQdFhcjIiLiuhRkXFSBfyMASg6oRUZERORYFGRcVHlwgvlDToq1hYiIiLgwBRkX5RlhriXjX7jX4kpERERcl4KMiwqMbgZAaGm6xZWIiIi4LgUZFxVeuZZMjLGfvJJyi6sRERFxTQoyLiogqjkAMRxi74Eca4sRERFxUQoyriqwIaX44mEzyNqnKdgiIiJHoyDjqmw2DvnEAJCfoSnYIiIiR6Mg48KKAhoDUH5wl8WViIiIuCYFGRfmCI0HwCNXa8mIiIgcjYKMC/OKbApAYFGaxZWIiIi4JgUZFxYca64lE1GWjt1hWFyNiIiI61GQcWFhsUkANLIdICOvxOJqREREXI+CjAs7vE1BlC2HvfuzLa5GRETE9SjIuDL/cIptAQBkp223uBgRERHXoyDjymw2cnxjASjar7VkRERE/k5BxsWVBplrydiz91hciYiIiOtRkHF1YeY4Ge98rSUjIiLydwoyLs63YSIAwcX7rC1ERETEBSnIuLjQ2BYANHRkUlBaYXE1IiIirkVBxsUFRDUHIN6WSWp2kcXViIiIuBYFGVcXlgBAhK2AffszLS5GRETEtSjIuDq/EAo9QgDITd9hcTEiIiKuRUHGDeT7m2vJlBzYZXElIiIirkVBxg2UBZndS7YcTcEWERH5KwUZN2ALN4OMb8FeiysRERFxLQoybsC/ci2ZoNIMDMOwthgREREXoiDjBkJjmwEQYxzgUFG5xdWIiIi4DgUZN+Bd2bUUZ8vSWjIiIiJ/oSDjDkLNjSMb2vJIyzpkcTEiIiKuQ0HGHfiHU+LhD0BuhqZgi4iIHKYg4w5sNgp8YwAoztpjcTEiIiKuQ0HGTZQFxQFgHNJaMiIiIocpyLgJW6g54NerIM3iSkRERFyHgoyb8GvQBICgknStJSMiIlJJQcZNBEUnAuZaMgcLy6wtRkRExEW4dJCx2+2MHTuWpk2b4u/vT/PmzXn22WfPyBYJ7wizRSbOdpC9h4otrkZERMQ1eFldwPG8+OKLvPPOO3z00Ue0a9eO5cuXc8MNNxAaGso999xjdXl1q3ItmVjbQdYdLKBTfJi19YiIiLgAlw4yCxcuZOjQoVx88cUAJCYm8vnnn7N06dJjvqa0tJTS0lLn/by8vNNeZ50IjsOBB762CrL37wUaW12RiIiI5Vy6a6lXr17MnDmTrVu3ArBmzRrmz5/P4MGDj/ma8ePHExoa6rzFx8fXVbmnl6cXBT4NASg6sNvaWkRERFyES7fIPPLII+Tl5dG6dWs8PT2x2+2MGzeOESNGHPM1jz76KPfff7/zfl5eXr0JM6WBcVC2H3tOqtWliIiIuASXDjJffvkln332GZMnT6Zdu3asXr2aMWPGEBcXx8iRI4/6Gl9fX3x9feu40joSGg+HVuGVv9fqSkRERFyCSweZBx98kEceeYSrr74agOTkZPbs2cP48eOPGWTqM5/IBNgNAcUZGIaBzWazuiQRERFLufQYmaKiIjw8qpbo6emJw+GwqCJrBUYlAhBrHOBAQenxDxYRETkDuHSLzJAhQxg3bhwJCQm0a9eOVatW8eqrr3LjjTdaXZolvMIPryWTxd5DxUQF+1lckYiIiLVcOsi88cYbjB07ljvvvJPMzEzi4uK47bbbeOKJJ6wuzRqVa8nE2Q4y71AxXRLCLS5IRETEWi4dZIKDg5kwYQITJkywuhTXUBlkwm0FZBzIAuKsrUdERMRiLj1GRv7GL4QSz2AACjN3W1uLiIiIC1CQcTMlAbEAlGenWFyJiIiI9RRk3IwjxOxe0loyIiIiCjJuxzvSnLnkX5yOw3Hm7QIuIiLyVwoybiagoRlkYowDZGktGREROcMpyLgZz/AEwJyCnXqoyOJqRERErKUg425CzQ0wG1UuiiciInImU5BxN5VrycSQzb7sfIuLERERsZaCjLsJisFu88LL5iAvM9XqakRERCylIONuPDwo9o8BoCxbQUZERM5sCjJuyBHcCABbrhbFExGRM5uCjBvyijBnLvkVaS0ZERE5synIuCG/BuZaMrHGAdLzSiyuRkRExDoKMm7II8ycgh1nO8jurEKLqxEREbGOgow7Cv1LkDmoICMiImcuBRl39JdF8XYfKLC4GBEREesoyLijykXxgm3FHMjMsLgYERER6yjIuCOfAEoCzTDjkbXZ4mJERESsoyDjpoyGbQAIzd+GXVOwRUTkDFWjIJOamsrevXud95cuXcqYMWN47733aq0wOT7fRskANDdSSMvR5pEiInJmqlGQufbaa5k1axYAGRkZXHDBBSxdupTHHnuMZ555plYLlKPziG4LQCuPVM1cEhGRM1aNgsz69evp0aMHAF9++SXt27dn4cKFfPbZZ0yaNKk265NjiTK7llrZ9motGREROWPVKMiUl5fj6+sLwO+//86ll14KQOvWrUlPT6+96uTYGrTEbvMkxFbEwfTdVlcjIiJiiRoFmXbt2vHf//6XefPmMWPGDAYNGgRAWloakZGRtVqgHIOXD/mBiQAYGRusrUVERMQiNQoyL774Iu+++y79+vXjmmuuoWPHjgB8//33zi4nOf0qIlsBEJC7xeJKRERErOFVkxf169ePrKws8vLyCA8Pdz5+6623EhAQUGvFyfF5x7WHPT8TVbyTCrsDL0/NphcRkTNLjb75iouLKS0tdYaYPXv2MGHCBLZs2UJUVFStFijHFhzfAYAkUknL0S7YIiJy5qlRkBk6dCgff/wxADk5OZx11lm88sorDBs2jHfeeadWC5RjOzwFO8m2j10H8iyuRkREpO7VKMisXLmSc845B4Cvv/6a6Oho9uzZw8cff8zrr79eqwXKcYQ3pdTmi6+tnEN7tVWBiIiceWoUZIqKiggODgbgt99+47LLLsPDw4Ozzz6bPXv21GqBchweHhwMaAZAWdp6i4sRERGpezUKMi1atGDatGmkpqYyffp0LrzwQgAyMzMJCQmp1QLl+IrDzJlLPgc1c0lERM48NQoyTzzxBA888ACJiYn06NGDnj17AmbrTOfOnWu1QDm+w+Nkwgu2WVyJiIhI3avR9OsrrriCPn36kJ6e7lxDBqB///4MHz681oqTEwtp0gFWQuOKPZqCLSIiZ5waBRmAmJgYYmJinLtgN27cWIvhWSA8sRMAiaSz98AhmsRoZWURETlz1Oif7w6Hg2eeeYbQ0FCaNGlCkyZNCAsL49lnn8XhcNR2jXIcHiEx5NmC8bQZHNi11upyRERE6lSNWmQee+wx/ve///HCCy/Qu3dvAObPn89TTz1FSUkJ48aNq9Ui5ThsNjJ8mxJSspaiveuB86yuSEREpM7UKMh89NFHfPDBB85drwE6dOhAo0aNuPPOOxVk6lh+aEsoWYtH1iarSxEREalTNepays7OpnXr1kc83rp1a7Kzs0+5KDk5jgZtAAjO3WpxJSIiInWrRkGmY8eOvPnmm0c8/uabb9KhQ4dTLkpOjn/j9gDElO6yuBIREZG6VaOupZdeeomLL76Y33//3bmGzKJFi0hNTeXnn3+u1QLlxBo2M9fuiTayKC88hHdg+AleISIiUj/UqEWmb9++bN26leHDh5OTk0NOTg6XXXYZGzZs4JNPPqntGuUEoqKiSDfMadeZO1ZbW4yIiEgdshmGYdTWydasWUOXLl2w2+21dcpTlpeXR2hoKLm5ufV6+4Rlz51H94qVbOn2DK0uudfqckRERE5Jdb+/tQxsPZET1AKAiv2auSQiImcOBZl6oiK8OQDeuRrwKyIiZw4FmXrCL8pskQkuSrW4EhERkbpzUrOWLrvssuM+n5OTcyq1yCkIa9walkDDigywV4BnjbfREhERcRsn9W0XGhp6wuevv/76UypIaqZRQjNKDG/8bOWUHdyDT1Rzq0sSERE57U4qyEycOPF01XFM+/bt4+GHH+aXX36hqKiIFi1aMHHiRLp161bntbiyhiH+bCeaJPaSlbqJOAUZERE5A7j0GJlDhw7Ru3dvvL29+eWXX9i4cSOvvPIK4eFa8O3vbDYbB3waA5C/b4vF1YiIiNQNlx5I8eKLLxIfH1+lJahp06YWVuTaigLjIWcxZQd2WF2KiIhInXDpFpnvv/+ebt268Y9//IOoqCg6d+7M+++/f9zXlJaWkpeXV+V2prCHNwPAO3e3tYWIiIjUEZcOMjt37uSdd94hKSmJ6dOnc8cdd3DPPffw0UcfHfM148ePJzQ01HmLj4+vw4qt5RudBEBI0R6LKxEREakbtbpFQW3z8fGhW7duLFy40PnYPffcw7Jly1i0aNFRX1NaWkppaanzfl5eHvHx8fV+iwKA1evW0umbcyjHC+8nMsHD0+qSREREaqRebFEQGxtL27ZtqzzWpk0bUlJSjvkaX19fQkJCqtzOFHEJzSk1vPCmgtLsY18jERGR+sKlg0zv3r3ZsqXqDJytW7fSpEkTiypybQ1DAthLFABZe7TnkoiI1H8uHWTuu+8+Fi9ezPPPP8/27duZPHky7733HqNHj7a6NJdks9nIqpyCnbdvq8XViIiInH4uHWS6d+/O1KlT+fzzz2nfvj3PPvssEyZMYMSIEVaX5rIKAhMAqMjaZnElIiIip59LryMDcMkll3DJJZdYXYbbcIQ3hRzwytltdSkiIiKnnUu3yMjJ89Uu2CIicgZRkKlnwhq3BqBhRRo4HBZXIyIicnopyNQzsQlJlBue+FJOySG1yoiISP2mIFPPNNAUbBEROYMoyNQz5hTsRgDkaRdsERGp5xRk6qHDU7DLtQu2iIjUcwoy9ZAjrCkAXrm7LK5ERETk9FKQqYd8o8xdsIMLNdhXRETqNwWZeigsvhUAURX7wHU3NxcRETllCjL1UGxCSyoMD/woo+TQPqvLEREROW0UZOqhiJBA0mgIwIE9Gy2uRkRE5PRRkKmHbDYbByqnYOdqCraIiNRjCjL1VKGmYIuIyBlAQaaeqghrBoBXjqZgi4hI/aUgU085d8HWFGwREanHFGTqqdDGmoItIiL1n4JMPRWX2Jpyw5MASijO2m11OSIiIqeFgkw9FR4cyHaPRAD2rptnbTEiIiKniYJMPWWz2dgf0h6Awp2LLa5GRETk9FCQqc8adQcg6MAqiwsRERE5PRRk6rGGbfsAkFC6FaO8xOJqREREap+CTD2W1KoD2UYwPlSQtnmp1eWIiIjUOgWZeszH25Odfm0AyNw03+JqREREap+CTD1X2LAzAB77lllciYiISO1TkKnnApv3BCA6b73FlYiIiNQ+BZl6rmnHc3EYNmKMTPIytV2BiIjULwoy9VxkRCS7PM2dsPesnWNxNSIiIrVLQeYMkBnSAYBiLYwnIiL1jILMGcAWby6MF5y12tpCREREapmCzBkguu05ACSWbqGirNTiakRERGqPgswZoEnLjuQRiL+tjN2bNA1bRETqDwWZM4CHpye7KxfGy9q0wOJqREREao+CzBmiOMpcGM8zTS0yIiJSfyjInCGCWpgL48Xka2E8ERGpPxRkzhBNOvQFIN5IZ3/GPourERERqR0KMmeIoLAGpHg0BiBVC+OJiEg9oSBzBskKO7ww3hKLKxEREakdCjJnEK/G5oBf3+xNFlciIiJSOxRkziAxzc0gE1O6i5Jyu8XViIiInDoFmTNIw+adAEiwZbJ+V7q1xYiIiNQCBZkziC2oIfmeYQDs3rzK2mJERERqgYLMGSY/uAUAeSlrLa5ERETk1CnInGG8YtsC4HlwC4ZhWFyNiIjIqVGQOcOEJ5pTsOMr9rD7YJHF1YiIiJwaBZkzjHdMOwBaeuxlxZ5DFlcjIiJyahRkzjQNWwPQ2JbFup3aqkBERNybgsyZJiCCUr+GABzarQG/IiLi3twqyLzwwgvYbDbGjBljdSluzRZltsr45Wwlt7jc4mpERERqzm2CzLJly3j33Xfp0KGD1aW4PZ9Yc5xMkm0vq1NzrC1GRETkFLhFkCkoKGDEiBG8//77hIeHW12O+4tqA0BLmwb8ioiIe3OLIDN69GguvvhiBgwYcMJjS0tLycvLq3KTv2lYGWQ89rJSQUZERNyYyweZKVOmsHLlSsaPH1+t48ePH09oaKjzFh8ff5ordEMNWwEQa8tme8pe7A4tjCciIu7JpYNMamoq9957L5999hl+fn7Ves2jjz5Kbm6u85aamnqaq3RD/mEYwXEAxJWnsCUj3+KCREREasbL6gKOZ8WKFWRmZtKlSxfnY3a7nblz5/Lmm29SWlqKp6dnldf4+vri6+tb16W6HVtUa8hPMxfGSzlE27gQq0sSERE5aS7dItO/f3/WrVvH6tWrnbdu3boxYsQIVq9efUSIkZMQZe651NKmcTIiIuK+XLpFJjg4mPbt21d5LDAwkMjIyCMel5NUucJvkm0vkxRkRETETbl0i4ycRpVTsFt57CUlu4jMvBKLCxIRETl5Lt0iczSzZ8+2uoT6oXLmUpQth1AKWLHnEIOTYy0uSkRE5OSoReZM5RsMoebU9Ja2vSzbre4lERFxPwoyZ7KoPxfGW7En2+JiRERETp6CzJnsLwN+N6TlUVxmt7ggERGRk6MgcyarbJFJ9k6jwmFoA0kREXE7CjJnssoWmZYeewFYvlvdSyIi4l4UZM5kDVsBNoLtOTQgl+VaT0ZERNyMgsyZzCcQGrQEINljJytTDmkDSRERcSsKMme6RuY+Vt28d5FfUsHW/dpAUkRE3IeCzJmuUVcAevvvAVD3koiIuBUFmTNdnNki07JiG2CwQgN+RUTEjbjdFgVSy2Lag4c3ARU5NLYdYNnuAKsrEhERqTa1yJzpvHzNMAN08tjJvpxiMnK1gaSIiLgHBRlxdi+dH5wKwHJtVyAiIm5CQUacA367eO0CYLk2kBQRETehICPOKdiNi7figUMtMiIi4jYUZMRcFM8nCC97ES1s+9iUnk9haYXVVYmIiJyQgoyAhyfEdgKgb2Aqdm0gKSIibkJBRkyV3Ut9g1IAWKb1ZERExA0oyIipMsi0cWwHYNGOg1ZWIyIiUi0KMmKqnLkUUbAVX8pYmXKIojKNkxEREdemICOm0HgIaIDNUUHf0P2U2w2W7lL3koiIuDYFGTHZbM7upUsi0gCYvy3LyopEREROSEFG/lTZvdS5cmG8+dsVZERExLUpyMifKrcqiC3cBMDmjHwO5JdaWZGIiMhxKcjInyq7lryyt9EtxhOAhTvUKiMiIq5LQUb+FNgAwhIAuCzmAAAL1L0kIiIuTEFGqqocJ9PL21xPZv62LAzDsLIiERGRY1KQkaoSzwGgyeb36Oi5h7TcEnZlFVpclIiIyNEpyEhVXUZC8/OxlRfxoe8rRHFI3UsiIuKyFGSkKk8vuGIiNGhFpCOLD3z+zZKte62uSkRE5KgUZORI/mFw7RdU+IbTwWMXl+56BrvdbnVVIiIiR1CQkaOLaIrtmsmU4cWFLOHAd2OtrkhEROQICjJyTJ6Jvfgs6gEAYta+BTmpFlckIiJSlYKMHJdXl2tZ4Ugy7+ycbWktIiIif6cgI8fVp0UDFjjaAVCxfba1xYiIiPyNgowcV2JkADuDugFQvmM2aHE8ERFxIQoyclw2m42krudTbPjgX5oFBzZbXZKIiIiTgoyc0LDuzVhutALg0PoZFlcjIiLyJwUZOaFGYf7sDesOQLaCjIiIuBAFGamWmE4DAYjOXo5hL7e4GhEREZOCjFTLWb3OI9cIJIgiNq6Ya3U5IiIigIKMVFOAny+poV0BSFn+i8XViIiImBRkpNqC2/QHIHz/IorKKiyuRkREREFGTkJCt8EAdGYLM9butrYYERERFGTkJNgatKTApyG+tnI2LP7N6nJEREQUZOQk2GzYmvUFICxjEXsPFVlckIiInOkUZOSkBLY2x8n08ljPF8u0G7aIiFhLQUZOTlOzRSbZtovJc9axbX++xQWJiMiZzKWDzPjx4+nevTvBwcFERUUxbNgwtmzZYnVZZ7bQRhiRLfC0GfQxVvDgl6uosDusrkpERM5QNsNw3e2MBw0axNVXX0337t2pqKjgX//6F+vXr2fjxo0EBgZW6xx5eXmEhoaSm5tLSEjIaa74DPHj/bD8fwAUGb4UBSXQoElbCI0H/zDwCwP/cAiKgiZ9wMOl87KIiLig6n5/u3SQ+bsDBw4QFRXFnDlzOPfcc496TGlpKaWlpc77eXl5xMfHK8jUpoz18O0tOA5sxcM4wXoyyf+Ayz+om7pERKTeqG6Q8arDmk5Zbm4uABEREcc8Zvz48Tz99NN1VdKZKaY93LkIW0UZj0z8if27NtAn/BA3JPvhUZoLJTlQkgu7F8C6r6DNEGg79IjT7M4q5NPFe/hHt3haxQTX/ecQERG35zYtMg6Hg0svvZScnBzmz59/zOPUIlO3MnJLuOA/c8gvqeD+C1oy+rwWeHrYzCdnPgvz/g2BDeHOJRAY6Xzdr+vTefCrteSXVtCsQSDT7zsXb091QYmIiKnedS3dcccd/PLLL8yfP5/GjRtX+3UaI3P6fbU8lQe/XgtAsK8XHePD6JwQRpdGAZw76wo8szY7u5jK7Q5e+GUz/5u/q8o5nhvWnn+e3QRSFkP2LuhwlcbWiIicwepVkLnrrrv47rvvmDt3Lk2bNj2p1yrInH6GYfDcT5v4fGkKRWX2Ks91sO1gqu+TeOJgbtfXeW1vEiv2HALgtr7NaBjky3M/baJBkC9zb21OwLtngb3UDDJD3wJPbys+koiIWKxeBBnDMLj77ruZOnUqs2fPJikp6aTPoSBTdyrsDrbuL2BlyiFWpeSwKuUQO7MKecTrc273+oH9RhgXlL6E4RfGq1d24oK20ZRVOLjgP3PYc7CI6Y0n0iprxp8nbDkY/jEJvP0s+0wiImKNehFk7rzzTiZPnsx3331Hq1atnI+Hhobi7+9frXMoyFgru7CM1Tsz6PjTJUSW7GFh0IU0vuEjEiIDnMf8uDaNDz//gm99n8LAhm3AUzB7PFSUQOI5cM3n4KvBwCIiZ5J6EWRsNttRH584cSKjRo2q1jkUZFxEyhL4cCBgwMDx0PNO51OGw86258+mZcVWlkcMods9n8Lu+TD5aijLh7jO0Ose+OvvQ0QziO1Y959DRETqRL2Yfu3CGUtOVsJZ0H8szHwGpj8KfiHQ+Z8A2NZ9TcuKrRQYfozOuIjPDxTQLLEPjPoBPrkM0lbB1zccec5+j0Lfh6sGnGoyDIOJC3azPi2XG3s3pX2j0FP9hMdVYXdwqKicBkE+xwzoIiJy8ly6RaY2qEXGhRgG/PY4LHoTbB7m+JcWA+CNbpCfxtdhN/FARn/Obx3Fk0PakhARgC1rK/zxHBRl/3keeynsXWb+3OFquPR18PL98/nSfFg+EQoz4dwHwa9qSCmrcPDot+v4ZuVe52MD20UzZkBL2sTW7u+IYRj8tC6dF37ZzN5DxYQHeJPcOIzkRiEkNwqjb8uG+Pt41up7iojUB/Wia6k2KMi4GMOA7++GVZ+AhzckXQBbfoawBLZe8QeD3lqKo/I3MtjXizZxIbSLC+Gyzo1JbvyXQLJikrlVgmE3t0G46hMzHC19Dxa/DcXmzKji4Cb4jfgMW0wyALnF5dz+yQoW7TyIp4eN3i0aMG/bAQ7/Kbg4OZaRvRLp2iT8z/Vw/qKswsGGtFyCfL2IjwjAz/vYIWR1ag7P/biR5ZWztI4mKSqIz245i6jgIwc0l5Tb+WxJCjEhfvRvE3Xc9zoVWQWl/Lo+g9lbMunZvAE39Tm5mYEiIqeDgkwlBRkX5LDD1zfCxml/PvaPSdBuON+s2MvEhbvYmlFA2d82o7ykQywPXNiKxAaV+2zt+AO+HAmleRCWAMW5UGqu/rzDEYufrYxGtoOU4MPy5Cdo0Hskd01exfbMAgJ9PHlzRBfOaxXFtv35TJi5jZ/Wpjvfq0GQDxe0jebCdjEkRASwYHsWc7ceYNGOgxRWTjG32SAu1J/EBgHEhPjjYfuzlyu7sIzfN2UC4O/tyW19m3FDr6bsyS5k7d5cdu7eTact/+GX0g5siTyfz285m+iQP8NMZl4Jt3yygjWpOQAE+ngysH0Mwzo1olfzSLxOcfHAknI736zcy09r01m886AzPAK8emVHLutS/bWaREROBwWZSgoyLqqiDKZcA9t/h4SecMMvVca6lNsdbM8sYGNaHnO2HuCHtWkYBnh52Lj2rATuPj+JhsG+sH8jTL4SclMBKApN4tGDg/ih4iz6N/FmZMY4+tjMxfo+rejPMxXXExESzIejutM2rurvw6b0PN6ft5PfN+4nr8TcQ6qTbTttPPbwo70n+ZgzrcIDvCm3GxSUnmCfKeDyLo15cGArYkL/0uJSXgwfXQp7l1KCDwNKX8IrIpHJt5xNXJg/a/fmcMvHy9mfV0qovzdBvl7syyl2vjw6xJeHB7VmeOdGNR5vc9sny5m+Yb/zfofGocSG+jF9w358PD2YctvZdEkIr9G5RURqg4JMJQUZF1ZeDOu/NbuXgqKOe+jGtDxe/HUzc7YeAMwWinv6J3FD76b4FGfCorfYH9Kegb+GklNi5+IOsbxxdWcKSkrZ+fVYOu18D4B9HrEEXPQs4V2vOOYg4XK7g8U7ssib/ToD097CCwdFtgC2NL4C/3PuomWLlthskFVQxp6DhezKKiSroAwD84+SYYCHzcY5SQ2OHETscJgDl//SGrXQoyvXFt1PfEQAN/Zuygu/bKa0wkGLqCA+uL4bTSIDWLHnENNW7+OntekcKioH4KymETw3rD1J0Sc3NX3HgQL6vzIHmw0eHNiKS5LjSIgMwOEwuP3TFfy2cT8Ngnz5/q7exIVVb5kDEZHapiBTSUGmflm4I4sXftnM2r1mF1KzhoE8NaQdyY1CGf72AnYfLKJTfBhTbj27ypgSY+tv2KfeiVexGYRo3AMufM6cTfV3ZUXww72w7kvztYFR2ArNbiI8vM1Vh/s+BOFNTv4D/P4UzP+PeZ6LXoKfHwJHOU/6PshHuZ2dh53fOooJV3cixK/qysZlxQV8MXsF4xYWUVLuwMvDxs3nNOPKbo0J8vUiwNeLAG9PPI4yvuewJ79bz0eL9jCgTRQfjOxe5bnC0gouf2chmzPyaRcXwle39yTA58/JjTlFZQT5ep1y15bLy0uDdV9D15FHDBY/ExSUVhDkW8uTWu0V8O0tcHAbXPYBRLWu3fNLvaMgU0lBpv5xOAy+XrmXl37dTFZBGQANgnzJKiilUZg/00b3Nrud/q40Hxa+Yd7Ki8zHmveHhLMhup15A/jin5CxDmyeMPB56HErbPsNFrwGKQvNYwIi4dqvoHHX6he+YpIZkACG/Rc6XQN/jIO5L2EPjGYY/2HdQXPrhocGtj5ysPHuBeYXQX46mUM/519rGvD7pv1HvA1AbKgfH9/Y44jWmvyScs5+fiaFZXY+veks+iQ1OOK1qdlFDH1rAdmFZfRr1ZAWDYPYnJHP5ow8sgrK6NYknCm3nn1yYcbhqLJ3Vn5JOb5envh4eRzzGMuU5MEH/SFrK5zzf9D/CasrYmNaHm/N2s51PZtwdrPIE7+ghvJLynnmh418tWIvQzvF8dIVHfD1qqVB5oc3kQXwD4cRX0PjbrVzbqmXFGQqKcjUX7nF5bz2+zY+WrQbu8MgyNeLb+7oRauYE3S15KXD7Odh1adgOI5+TEADuPIjSOxT9fHUpfDzA5C+BrwD4IqJ0GrQka8vzoG8fVB4AAoOwKHd5mrFhh36PgLnPWoeV14C7/SE7J1UdLuF1LOfpunhwcyH2Stgzovml8DheuPPghunM2NTJq/8toV9h4opLKuoMmi3W5Nwvrytp9k6U1YEnj5MXJzK0z9spEVUEDPuO/eYY2yW7spmxAeLKbcf/a+H/7ugJXf3P8GWIeXFsGGqORU+Yx30GYPR5z4+XZ7BuJ82EuznzZgBSVzV0gOvXx+CXXPh6s+gWb/jn7fKtSk3Z6t51NKXrcMBX14Hm38078ckw+3za+fcx2F3GEedJQdQXGZn8Gtz2X2wCD9vDz66oQdnnYYws2jHQR74ak2V8VhnNY3gveu7Eepf/T3PDMNgZUoOP61NZ0XKIfq0iOSuhBT8v7gSMCC8KRzaBd6B5mzDFv1r/bNI/aAgU0lBpv7bkpHP5CV7uLRTI7o2OYkBqge2mC0t+zfA/vXmfXuZuZLwVZ9C6DFm7pQWwFcjzYHKNg+4ZILZBVFeAlt+gtWTzRlVRwtJHa6C4e9WHZ+zYxZ8MgywwS0zodFfWnlyUuCbmyF1iXm//RWw6QdzLZ2RP0LTc5yHGoZBSbmDtIwMPvngVZo4UhkUk0dsWQrk7cPw8CLNaMCOiobEJLamZfJZ0PUG8Dx6F8LP69L5cnkqTRsE0iYmhNaxwWxOz+ehb9bi7Wlj2ujetIs7SrdL1nZY9gGsmQwluVWe2uudyOiCG1ljtAAMrvKczVjvzwiisoWsYRu4YwF4eFJuN7vOjhq20labU+3XfW0urtjrbuh2E/gGHfWzVNucl2DWOPD0MX8XAB7YBkFR5BaX4+lhq9UuF7vD4O3v51K28nOadx/MsEsuPWLs1nM/buTj+VtpaUsl1YiiwieUT24+q9YGY5eU23l5+hbnjvTxEf7c0Kspr87YSkFpBS2jg5h0Q48TjpfamJbH1FV7+XldRtXB6WTzq9+/CCcPo+uN2C581gyLO/4wu1gvexfaX35KnyGvpJylO7PpnhhBaIA2mq0vFGQqKchItdnLoWA/hDQ68WrB9nKzm2j1Z+b9pIGQurjqF3dAJAQ2NG9BURDdHnqOrrp432Hf3GKOyYlsYQaZ3H2Qt9f8r6McfEPgkv9A8hXm+jnL/2e2XFz/XdXzlJfA++dB5sbqfeZBL8LZt1fv2EO7MX4byzeZsTywry+tY4L5/q4+VbuH9q2ADwebQQvMafFdRrKpMJDoJc8TQR52w8amhGsIL9xBo2wzoK1ytKCFZwbBRgHP+9zL52W9yS+pINjPi/ZxobRvFEL7uCA65M4iZO1EIrNXHVFeri2Yb32HMyd0KOd1bMHwLo2OGGMEZtfkoaIyIgL/tsryll/h86sBAy590wxKGWth+LusixzMiA8WU+EwGNUrkVvPbUZYgM8R5127L5ei0gqaRwURFex73FlleSXlTJj0ObenjyXKlmN+hpCWhPa5FTpcCV7+bFv8A2umT+JCj+WE2Mywt8sRzUaPJLr07E9s10uhQYtq/M87uuzCMv75wRI2pucBcE2PBB67uA1Bvl5sSMvlhonLyMwvJSbEjw9Gdjti8HqF3cGMjfuZuHA3S3f9uWhloI8nF7SNpkvjYDr+cR0dHRtZ70jkqagJ3DmgLY1DvEiYcz9+W6YBNhj6pnOl7+oqrbAza/MBvlu9j5mbMymrcNCxcSjf3NGrTsdwpWYXsWB7FkVldq7r2QTv+j5+rA4pyFRSkJHTxjBg1vMw96U/HwtpbI596XgNRDav/rkKDsCb3aAk58jnGveAy9+H8ETz/qE98Hpns5vq5j+qjtOZ/hgsehMjIJKpjnNYnN+QuOYdGXP1xYz5bDH7dm7in60cDA3dAWs+h+BYuGf1iXcY3zkbvhrlXGjw/zwe5Juiztx1XgseGFi5oWt5MbzbF7K2QPxZlPW6n9/K2jF1VQYzN2cSTh4vB09hQPnsPy+hlx+zG93G6J1ncZ3jBx71/py9RgPOL32FMv4aQgze9H6dSzzN4FNmePKz4yw+qbiAph4ZjPacRlMPc7xQkeFLNsGU4YOffyBhIcGUN+nLzxHXM39nDgt2ZJFTVE6zhoFc1D6Wi5JjaeOzH9v755trEnW/GS5+xTkwOy9pOH13XOucLQbmYo039mnKqF6JbEzP49f1GUzfkEFmfqnzmCBfL5o3DKR5VBA9m0XSt1VD58KHew4W8skHr/JA0Wv42co56BlFYMUh/GyV7+EdgOHpje2vwdg3xKzvLxwePuw7fwLpjQZTUFpOfkkF2YVlHCwo42BhKVkFZTSJCODeAUkEHw519grw9CKnqIxr3zdDTIMgH16+oiPnta46e3BfTjEjP1zK9swCwFx6oEVUEMN8lnPR/ndZXx7LtJKu/O7oQoFHMBe2jWZopzj6tapcwPH3p2H+q5R5BjK0/Hk2lTV0ntsDB095fcT1XjOowJPXG/2bssa9SIwMoFnDINrGhRzR+lVYWsG8bQeYsTGT3zZmkF/y5xIINpv5R/LhQa25o99J/Nk7CRV2B7sPFrIhLY/FOw8yf3sWqdl/tj5d0iGW167ufMxuwtPGXmF2r57Ecgy5xeWs35dLr+aRLrttioJMJQUZOe3Wfgl7FkLbodC0b80HrO5ZaHYbBUWb3VohcWbrUFjCkX9BTb3dDCKtLoZrJpuP7ZwDH19q/nzNF6wNPJthby3AYcCzw9ozdtp6bDaY/UA/moR6wetdzFafi/4NPW45ek2GYa6U/NvjZleZfzgUH6LMJ5Rz854j0xbJt3f2plN8mDNElfo1ZFyTiXy7uajKWjujeiXyyODW+O2aCdP/ZX7Gi1+ByObszyth6Za9XPDHYPyKM8nq8wy2s29nf14p69NyiVjxOgPS36McL6YFXsWOxH/QqHFTkqKDCfX3prSsjKBt3xG39i0C8nYc9aPMsHfl7vK7KKFqi1hz2z4m+v2HBCONisZn4zXqB/DygV3z4KNLyCaEriVv06FxOLee25w3Z21nU3oe/T1W8C+vyUyouJwfHL0AM+BEBvmQkl1UZbzSYe0bhdCzaTgNlv+H2/gagLyEAQRdM5GHv15F4Oav+afXH7SwmdtnZBph/OHZi4uvvoPgFn2gJIeiPcv59ofvaVWwhO4eW3EYNp6oGMWn9guO+auVEBHA61d3olPKRzDzGSriuvJa7jm8l5VMcFAwU249mxZRR++Wyy0q594vVjF7ywHA4HbPH3jEe0qVYxx4Up7QG9/GHc0xaHlp5u9WTop5wD8mkZkwmP/M2MbinQc5VFRGbnE5hmHwuvebXOq5iGwjiKFlz5JqRAPmr3zTyEDaNQqlWYNAVqfmsGjHwSoLZcaE+DG0UxyXdopjY1oeD369Fh8vD36+pw8too4/Vs4wDO7/cg1ztx7gqu7x3NinKQ2Cqv5uFJVV8NuG/czfnsXmjDy27i+grKJql7Gnh40OjUNZvy+XcrvBP7o25sXLOxx35mCtMQyzVfjXR6FZX7jyk2qFmb/OTnzhsmSu7pFw+mutAQWZSgoyUi8d2Apv9QAMuGOhGXje6W1+eXQdBUNeA+Cp7zcwaeFu58vObx3Fh6Mqp1wvfd8cuBzSCO5ZdWSXV3mx2X229gvzfsdrYfCL8NEQSF/N1oDODMr+PxpFBDI4eDeP7L8fDwxuLHuAPxxdAGgU5s+wznEM79zohF8sgDkw+Mcx5mDre1eDbzBsnQ6TrzI/65DXzfFIx+KwQ9Y2jLICNqceYNaGVPbv2ci/PD7F11ZOWnAHMod8RGLjxszZkknB/He5POsd/GzlpBkRXGt7gWF9OnNjn6YUFxcT/FoSAZRwZ9B/GHfHPwkP9MHhMPhlXTptvxtIU0cKDmxMbfQAEefeSq/mkfh6eVJaYSflYBE7DhSyIS2XNZu34pGxhmTbLnp7buBsj00AFHYbTeBFz4KH+Zrr/7eUJbsOcm5gKkXFJax0tODd63twQdvoKh8zt6icuycvY1Dqf7jW9hsAk/2v5Yew64kI9qVBoA+RQb4E+3nxwbxd7Msp4jHvz7nF88cq5zlECEana4noeT00aHXM8VIARcXFlHx3HxGbPwdgQfhQwhrE0SZ3Dh7H68rsPQYuePqIh+0Og7zicrJzc2j49WWEZK9jv18zxjb4D+sP2EnLLTnq6ZpEBjCgTTQXtI2mR2KEMzAYhsENk5Yxe8sBOieE8fXtvY7bMvLd6n3cO2W1876vlwcPtM1lVOZLHGzYnde8buT7DYecK3kf5u/tScuYYLo1Cad3i0h6NI0kyNeLX9alM3ryShyGGdqfHNK2aktHeYk5ASAo+qhjubbtz+e5nzaxMT2Pl67owHmtjr+2FmWF8NP/mf+gOezSN6DL9cd9mWEY3PnZSn5ZnwGYYXD2g/1O2xYop0JBppKCjNRbX14PG78zBwB7eJqBI7ypOcum8i/K/JJyBrw6h/15ZpfHxzf24NyWlc375SXwWkcoyDAHLHf7yw7j5SXw2RWwe96f09DPus38117Wdnj3XCgv5E2PEbxV1J9ffB4l0WM/X1Wcy9th/8fZzSIZ3rkR3ZqEn9y/TO3l8NZZkL0D+v3LHAT6/vnm1hPdbjTHCZ2kwtIKPPcuxu+ra80xTA1awfD/wuwXYNt0APY37MW9xbewOMsMcyF+XoQGePNE/rNc4LmSgj7/ImjAw3+edNdcM9D91YXjoNddf94vyobF75j/Ys7bV/Vj2rxwXDwB727XVXk8t6icf7y7kK37za6coZ3ieO3qzhyTYZgz2maPN+93uxEGvwSef3bL5RYWs/HdUfTM+xWA17ma8vIKRnjPIoaDf57L0xcatjLHckW1MVsEAyIhsAH4BJpjs3bOMge4DxxfdWzVwR1ma2J+hvm60EZmN2tYAoTEHrv+w/LS4L3zzN/FloPh6s84WFTBhrQ81qflsj2zgKSoYC5oG0XzhkF/BgTDgIJMc6p89k5ySh08OX0vB8p9+Eevtgzv38dsRfyb7MIyBrw6h+zCMoZ3bsTOAwUU7dvAVz5PE2YrBGCTI4Hby8dARDMu6RBLcqMw2sQGEx8egEd5gTnr6m+tr9+u3Mv9X64BYPR5zXlwYGtKMzZTvOgDAjd9iXeZ2VVo+IVhC20MoY0panUZL+1rzyeL92CvbMbz9LDx/PD2XNX9GC0lmZvM7Vmytpj/P5r2Nf/f+IbCXUshOOaYl/qtWdt5efoWvD1thPp7k1VQxuMXt+Hmc5qd+P9THVOQqaQgI/VW+hozUBxm84Abp0N8jyqH/bo+g9s/XUGr6GB+ufecqsFi8Tvw6yMQmgD3rDS/AO0V5qySLT+DT3DllOi+Vd971afw3WgMmxfbg7uTlLeIssA4ym5dQFBoxKl9rvXfmqsf+wSZY3gOboP4s2FkZZdPTWVugk8vrxoqPH3N1oIet+HAxk/r0nlt5jbnmJDRgbN40P6+uTHpDT/9+bopI8wp2t1uNFuNFpgtYPR7FLrfAovfgiXvQVl+5Qts0KAlxHWC2I7Q4gJo2PKoZe7LKWbE+4ux2Wx8c0cvIgKr8ZmXfQA/PQAY5pdZs77mitmJ55jdgpt/xGHz5F8VtzCl/FxC/LyYfFM32hcugeUfmt2a5YUnfh/vQLjiw6MvOXCq9q6AiZUDxfvcBwOeOvaxyz6A1Z9D1jbn/mpH4/AKwOOfXx2xjML9X6zm21X7aBUdzA9398G7YB9l7w3AtyiD9Y5EYj0OEUkuFd7BeF72X2xtLjHXF9r4HayZAnvmQ+tLzK6cv4WZTxbvYey0dVzosZybfWbQg/XO50oNb3xt5fzdFxX9eLJiJOe2TcDfx5PvVqcBcG//JMYMSDKDm70c9i4zZ1oueddcCysoBq74n/nn438DIG0VJS0uZlXPNygpt3NWs4gqC1r+sXk/N320HMOAFy5LxmaDh79ZR0SgD3MfOu+IMUlztx7ghzVp3HleiyOXhagDCjKVFGSkXvvsH+ZfbADnPgjnP37Uw1alHKJRmD9RIX8b1FtWZLbKFGaaM3U6jYDvRptTpz194Z/fVJni7WQYZtjYMPXPx66bCs3PP/XP5HDA+/3MoAYQHAe3zobg6OO9qnpy95ph5sBmiGpnDqI+vBBiJbvD4Kd16SzYlsXojjYSPusDHl7w8G4ztOSkwmsdzDFDdy6Ghq1h7r9h1nPmCf46dTu6PZz7gBlcTmJquN1hYHcYVWeEnciGaWZXQ1HWkc95+sI/JrI1/FymLE3lH90a0yb2L38fOhyQs9vcu2z/BjiwCQqzzFvRQfMWnmiGmLhO1a/pZK39Cr692fz53IfgvH9VHfNxtAH2Ng8Ia+IcXG+U5rMvPQO/8kM0sOWZrR83/w4NzHWP5mw9wMgPl2Kzwbd39KJzAwM+HGi26jRoxYErpxHs5cBv6k3mTEQwA+HeZVDxt+6ufv+Cfg/zd8s+eoTuu94BwG7YmEMXZgZczCqfLuw7cJAo4yBxtoOc7bGR2zx/xMNmUBjWisB/foYR2YJXZ2zljT+2kWDL5N6mexkevAWP3XOrDvZudh5c9j4HjBBe+nUz+XtW80b+fXjb7NxeNoZfHT0I8PE0B2B3bkSjMH8uf3sh+aUVXHd2E54d1p4Ku4ML/jOXXVmF3H9BS+75y9pQC7dnMWriMsrsDkL8vHhrRBfOSWr494+K3WEwa3Mm57WOqvVBzgoylRRkpF7bu8L8Szi2I9z4a5UuhWpb+Ib5r/bwRGh1kTm41+ZpLlbW+uJjv644B/7bx9yws4bdPse0fSZ8epkZCm749eRWUD6R0nxIWWx+OZ1othaYQe/Qbrj6c2h9kXMmDk3PNVuJDlv0NkyvXOgwpgP0fdi8nnW5WrHDDmmrYNsM2D4D9q00w9fVk48eSKt9XocZKOpidsvcf8Mfz5o/dx0FF79qdp0ahvn4vFfM5859CNoNh4hmR/x/3JdTzKX/+Z33jKfp6rGNgoDG+N8xm1LfcC78z1z2Hirmht6JPDkw0dzAdd9yc6zYTb/9uX6UvRxmPGm2rh3WoKW5FpSXH/z2GGCDEV+ZrV+H/WUF76z2N+HV+y5CY5o6u8OKy+ysT8tlTWoOmzPyGRiwmQEbH8NWeMBshTznfsjeScHmPwgqTqt6bQIizX8stBwE7Yazam8ed3y6kow8M2D9n9eX3O01jYO2cEb4vM7m3CPHvfRIjODTm89yhuTv16Rxz+erCPb1Yt7D5xEW4MP6fblc/d5iCkorCPb1Ir+0Ag8bPHZxW27snYjNZqO0ws60Vft4d/YOArLXc9eIKxjUvhrdiCdBQaaSgozUe3lp4B9RvS/loykrhAnJ5r+6Dxv2DnS69sSvzdputgh1uwG8a3mDyY3fmwMjj7YfVl06vG5P95vNcTD/aWteq6s+hTZ/Gyeza5657k+z8+rmS/9EiirXdgk4xe6+urbsf2brEoZ5jS/7wFyN+3AX3sDnzTWZjmPxzoO8+M18JuQ/QBOPTDZ6tua9xAlM25BNfKgPMwbn4bfoP7B/nTmO5oZfj77/05ZfzBW92wwxF8s8/P/1x/vMbjm/MLhtjvkPgc0/wxcjzNa6cx6A/mOr93nzM+DrG2HPgioPOzy8WWlvzpyK9iywdeb88y7g1r5J+Hh5MHlJCk99v4Eyu4PmDQN5dHAbkiK8SPhqILaD2zA6/ZNVXZ7j+9Vp/Lg2jayCMmJD/fj+rj5VtnBxOAwuen0emzPyub1vc67tkcBl7ywkq6CUs5uZKzs/88NGvl5hzqT7R9fGtIoJ5oN5u8jJy2Wc9/8Y7rmA+d3f4dyLr6ne560mBZlKCjIi1TD/P+a6KVCtL4kzyuafYMq15kDqvg/BtDsgNN5cf+c4s3zkFG38zlzV2l5mtpYcHts0+GU469ZqnaKswsF3v8/iwsXXEUohP9rPYpGjHY+F/05AQeXUcJ9guO7bI8aWnVBFqTmmZ98Ks0X0gmfM2XUVJebifpe+eXJh1l5h/jncNcc8X7PzoElPUgtsPD5tPXO2mhvetooOpk1sMNMqx9EMahfDv6/s+Of4lj2LYGLlGKYet8F5j1LhE8qq1BwSIwPNEFNRZk4OSF8Nfe5jZpo3N320HD9vD6JD/NhzsIg2sSF8cdvZhPh5YxgG/5u/i+d/3uRcViDBtp8P/F6jpbEbw+aJbfCLx17GoYYUZCopyIhUQ1mh2fIQ1wnOvsPqalxLSR681BQcFeag6NwU6P+k2QUgp9eueWaIPDw25OJXzJaxk5S3aRYBX16Bl/Hnukb4R5gbwva4xZyZVRO5e80B939tzWw5CK76rFZDrmEYfL8mjad/2Eh2oTn+ysMGDwxsxR19mx+5oN3h7k8wu6P6PwGdrzN/h1d9agam3NTK5xtgXPEhl/3qxaqUHAAah/vz7R29iAr2NQcVeweAzcbcrQcY88VqBnqv4emKCfhU5Jsrl18x8dS6Lo9BQaaSgoyInLKJF/3Z7O/pC/dvgsDTtwu1/EX6WnP/q/aXm1s31NSaKRjT7oTQRth63g2dR5jTyk/VztnwyXCzO6lxD3PbEJ+AUz/vUWQXljH+502sSs3hiUva/rmUwtHsmAW/PGxO0QaIruw+zq8cdxMUbXapHdgMNg92d36YfgvbExHoyzc3d6Zpxq/mrMb9681VpcOaQHgTHN4BeKz70jxH4x7m5rohcafl8yrIVFKQEZFT9tcBqJ1GwLC3ra1HaqYo2/xSru0uwfXfmmsL9X/CtcYj2cvNhS9nj/+zVSs4DvqMMRfOMwz46X7nonqHEi/GK7olwes+Ofrst7/qcas5ZuxUlkQ4AQWZSgoyInLK0lbBe/3Mn2+dc3qnIIvUtoID5mzEsARzEP9fV/E2DHNdnl8fMbueDgtpbHa7dbjKDEGHdpv7vOWmQpNe0GrwaS9bQaaSgoyInDLDMKeo+wSaa5uI1DcpS8w1pAIbmKt4tx5i+WB2BZlKCjIiIiLup7rf33W4UpOIiIhI7VKQEREREbelICMiIiJuS0FGRERE3JaCjIiIiLgtBRkRERFxWwoyIiIi4rYUZERERMRtKciIiIiI21KQEREREbelICMiIiJuS0FGRERE3JaCjIiIiLgtBRkRERFxW15WF3C6GYYBmNuBi4iIiHs4/L19+Hv8WOp9kMnPzwcgPj7e4kpERETkZOXn5xMaGnrM523GiaKOm3M4HKSlpREcHIzNZqu18+bl5REfH09qaiohISG1dl45kq513dB1rhu6znVD17lunM7rbBgG+fn5xMXF4eFx7JEw9b5FxsPDg8aNG5+284eEhOgPSR3Rta4bus51Q9e5bug6143TdZ2P1xJzmAb7ioiIiNtSkBERERG3pSBTQ76+vjz55JP4+vpaXUq9p2tdN3Sd64auc93Qda4brnCd6/1gXxEREam/1CIjIiIibktBRkRERNyWgoyIiIi4LQUZERERcVsKMjX01ltvkZiYiJ+fH2eddRZLly61uiS3Nn78eLp3705wcDBRUVEMGzaMLVu2VDmmpKSE0aNHExkZSVBQEJdffjn79++3qOL64YUXXsBmszFmzBjnY7rOtWPfvn3885//JDIyEn9/f5KTk1m+fLnzecMweOKJJ4iNjcXf358BAwawbds2Cyt2P3a7nbFjx9K0aVP8/f1p3rw5zz77bJW9eXSda2bu3LkMGTKEuLg4bDYb06ZNq/J8da5rdnY2I0aMICQkhLCwMG666SYKCgpqv1hDTtqUKVMMHx8f48MPPzQ2bNhg3HLLLUZYWJixf/9+q0tzWwMHDjQmTpxorF+/3li9erVx0UUXGQkJCUZBQYHzmNtvv92Ij483Zs6caSxfvtw4++yzjV69ellYtXtbunSpkZiYaHTo0MG49957nY/rOp+67Oxso0mTJsaoUaOMJUuWGDt37jSmT59ubN++3XnMCy+8YISGhhrTpk0z1qxZY1x66aVG06ZNjeLiYgsrdy/jxo0zIiMjjR9//NHYtWuX8dVXXxlBQUHGa6+95jxG17lmfv75Z+Oxxx4zvv32WwMwpk6dWuX56lzXQYMGGR07djQWL15szJs3z2jRooVxzTXX1HqtCjI10KNHD2P06NHO+3a73YiLizPGjx9vYVX1S2ZmpgEYc+bMMQzDMHJycgxvb2/jq6++ch6zadMmAzAWLVpkVZluKz8/30hKSjJmzJhh9O3b1xlkdJ1rx8MPP2z06dPnmM87HA4jJibGePnll52P5eTkGL6+vsbnn39eFyXWCxdffLFx4403VnnssssuM0aMGGEYhq5zbfl7kKnOdd24caMBGMuWLXMe88svvxg2m83Yt29frdanrqWTVFZWxooVKxgwYIDzMQ8PDwYMGMCiRYssrKx+yc3NBSAiIgKAFStWUF5eXuW6t27dmoSEBF33Ghg9ejQXX3xxlesJus615fvvv6dbt2784x//ICoqis6dO/P+++87n9+1axcZGRlVrnNoaChnnXWWrvNJ6NWrFzNnzmTr1q0ArFmzhvnz5zN48GBA1/l0qc51XbRoEWFhYXTr1s15zIABA/Dw8GDJkiW1Wk+93zSytmVlZWG324mOjq7yeHR0NJs3b7aoqvrF4XAwZswYevfuTfv27QHIyMjAx8eHsLCwKsdGR0eTkZFhQZXua8qUKaxcuZJly5Yd8Zyuc+3YuXMn77zzDvfffz//+te/WLZsGffccw8+Pj6MHDnSeS2P9veIrnP1PfLII+Tl5dG6dWs8PT2x2+2MGzeOESNGAOg6nybVua4ZGRlERUVVed7Ly4uIiIhav/YKMuJyRo8ezfr165k/f77VpdQ7qamp3HvvvcyYMQM/Pz+ry6m3HA4H3bp14/nnnwegc+fOrF+/nv/+97+MHDnS4urqjy+//JLPPvuMyZMn065dO1avXs2YMWOIi4vTdT6DqGvpJDVo0ABPT88jZnHs37+fmJgYi6qqP+666y5+/PFHZs2aRePGjZ2Px8TEUFZWRk5OTpXjdd1PzooVK8jMzKRLly54eXnh5eXFnDlzeP311/Hy8iI6OlrXuRbExsbStm3bKo+1adOGlJQUAOe11N8jp+bBBx/kkUce4eqrryY5OZnrrruO++67j/HjxwO6zqdLda5rTEwMmZmZVZ6vqKggOzu71q+9gsxJ8vHxoWvXrsycOdP5mMPhYObMmfTs2dPCytybYRjcddddTJ06lT/++IOmTZtWeb5r1654e3tXue5btmwhJSVF1/0k9O/fn3Xr1rF69WrnrVu3bowYMcL5s67zqevdu/cRywds3bqVJk2aANC0aVNiYmKqXOe8vDyWLFmi63wSioqK8PCo+jXm6emJw+EAdJ1Pl+pc1549e5KTk8OKFSucx/zxxx84HA7OOuus2i2oVocOnyGmTJli+Pr6GpMmTTI2btxo3HrrrUZYWJiRkZFhdWlu64477jBCQ0ON2bNnG+np6c5bUVGR85jbb7/dSEhIMP744w9j+fLlRs+ePY2ePXtaWHX98NdZS4ah61wbli5danh5eRnjxo0ztm3bZnz22WdGQECA8emnnzqPeeGFF4ywsDDju+++M9auXWsMHTpU04JP0siRI41GjRo5p19/++23RoMGDYyHHnrIeYyuc83k5+cbq1atMlatWmUAxquvvmqsWrXK2LNnj2EY1buugwYNMjp37mwsWbLEmD9/vpGUlKTp167kjTfeMBISEgwfHx+jR48exuLFi60uya0BR71NnDjReUxxcbFx5513GuHh4UZAQIAxfPhwIz093bqi64m/Bxld59rxww8/GO3btzd8fX2N1q1bG++9916V5x0OhzF27FgjOjra8PX1Nfr3729s2bLFomrdU15ennHvvfcaCQkJhp+fn9GsWTPjscceM0pLS53H6DrXzKxZs476d/LIkSMNw6jedT148KBxzTXXGEFBQUZISIhxww03GPn5+bVeq80w/rIEooiIiIgb0RgZERERcVsKMiIiIuK2FGRERETEbSnIiIiIiNtSkBERERG3pSAjIiIibktBRkRERNyWgoyIiIi4LQUZEan3EhMTmTBhgtVliMhpoCAjIrVq1KhRDBs2DIB+/foxZsyYOnvvSZMmERYWdsTjy5Yt49Zbb62zOkSk7nhZXYCIyImUlZXh4+NT49c3bNiwFqsREVeiFhkROS1GjRrFnDlzeO2117DZbNhsNnbv3g3A+vXrGTx4MEFBQURHR3PdddeRlZXlfG2/fv246667GDNmDA0aNGDgwIEAvPrqqyQnJxMYGEh8fDx33nknBQUFAMyePZsbbriB3Nxc5/s99dRTwJFdSykpKQwdOpSgoCBCQkK48sor2b9/v/P5p556ik6dOvHJJ5+QmJhIaGgoV199Nfn5+af3oonISVOQEZHT4rXXXqNnz57ccsstpKenk56eTnx8PDk5OZx//vl07tyZ5cuX8+uvv7J//36uvPLKKq//6KOP8PHxYcGCBfz3v/8FwMPDg9dff50NGzbw0Ucf8ccff/DQQw8B0KtXLyZMmEBISIjz/R544IEj6nI4HAwdOpTs7GzmzJnDjBkz2LlzJ1dddVWV43bs2MG0adP48ccf+fHHH5kzZw4vvPDCabpaIlJT6loSkdMiNDQUHx8fAgICiImJcT7+5ptv0rlzZ55//nnnYx9++CHx8fFs3bqVli1bApCUlMRLL71U5Zx/HW+TmJjIc889x+23387bb7+Nj48PoaGh2Gy2Ku/3dzNnzmTdunXs2rWL+Ph4AD7++GPatWvHsmXL6N69O2AGnkmTJhEcHAzAddddx8yZMxk3btypXRgRqVVqkRGROrVmzRpmzZpFUFCQ89a6dWvAbAU5rGvXrke89vfff6d///40atSI4OBgrrvuOg4ePEhRUVG133/Tpk3Ex8c7QwxA27ZtCQsLY9OmTc7HEhMTnSEGIDY2lszMzJP6rCJy+qlFRkTqVEFBAUOGDOHFF1884rnY2Fjnz4GBgVWe2717N5dccgl33HEH48aNIyIigvnz53PTTTdRVlZGQEBArdbp7e1d5b7NZsPhcNTqe4jIqVOQEZHTxsfHB7vdXuWxLl268M0335CYmIiXV/X/ClqxYgUOh4NXXnkFDw+zMfnLL7884fv9XZs2bUhNTSU1NdXZKrNx40ZycnJo27ZttesREdegriUROW0SExNZsmQJu3fvJisrC4fDwejRo8nOzuaaa65h2bJl7Nixg+nTp3PDDTccN4S0aNGC8vJy3njjDXbu3Mknn3ziHAT81/crKChg5syZZGVlHbXLacCAASQnJzNixAhWrlzJ0qVLuf766+nbty/dunWr9WsgIqeXgoyInDYPPPAAnp6etG3bloYNG5KSkkJcXBwLFizAbrdz4YUXkpyczJgxYwgLC3O2tBxNx44defXVV3nxxRdp3749n332GePHj69yTK9evbj99tu56qqraNiw4RGDhcHsIvruu+8IDw/n3HPPZcCAATRr1owvvvii1j+/iJx+NsMwDKuLEBEREakJtciIiIiI21KQEREREbelICMiIiJuS0FGRERE3JaCjIiIiLgtBRkRERFxWwoyIiIi4rYUZERERMRtKciIiIiI21KQEREREbelICMiIiJu6/8B/XiF7ypdfu8AAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "while True:\n",
        "    lr = get_lr(iter_num)\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "    if iter_num % eval_interval == 0 and master_process:\n",
        "        losses = estimate_loss()\n",
        "        train_loss = losses['train']\n",
        "        val_loss = losses['val']\n",
        "        print(f\"step {iter_num}: train loss {train_loss:.4f}, val loss {val_loss:.4f}\")\n",
        "\n",
        "        # Collect the losses\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "    if iter_num == 0 and eval_only:\n",
        "        break\n",
        "\n",
        "    # forward backward update, with optional gradient accumulation to simulate larger batch size\n",
        "    # and using the GradScaler if data type is float16\n",
        "    for micro_step in range(gradient_accumulation_steps):\n",
        "        with ctx:\n",
        "            logits, loss = model(X, Y)\n",
        "        # immediately async prefetch next batch while model is doing the forward pass on the GPU\n",
        "        X, Y, fens = get_batch('train')\n",
        "        # backward pass, with gradient scaling if training in fp16\n",
        "        scaler.scale(loss).backward()\n",
        "    # clip the gradient\n",
        "    if grad_clip != 0.0:\n",
        "        scaler.unscale_(optimizer)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "    # step the optimizer and scaler if training in fp16\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "    # flush the gradients as soon as we can, no need for this memory anymore\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    # timing and logging\n",
        "    t1 = time.time()\n",
        "    dt = t1 - t0\n",
        "    t0 = t1\n",
        "    if iter_num % log_interval == 0 and master_process:\n",
        "        lossf = loss.item()  # loss as float. note: this is a CPU-GPU sync point\n",
        "        # if local_iter_num >= 5:  # let the training loop settle a bit\n",
        "        #     mfu = raw_model.estimate_mfu(batch_size * gradient_accumulation_steps, dt)\n",
        "        #     running_mfu = mfu if running_mfu == -1.0 else 0.9 * running_mfu + 0.1 * mfu\n",
        "        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt * 1000:.2f}ms\")\n",
        "\n",
        "    iter_num += 1\n",
        "    local_iter_num += 1\n",
        "\n",
        "    # termination conditions\n",
        "    if iter_num > 100:\n",
        "        break\n",
        "\n",
        "# Plot the losses\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_test_data(): \n",
        "    dataloader = test\n",
        "    batch = next(iter(dataloader))\n",
        "    # fens = [[fen_to_tensor(fen) for fen in game['FEN']] for game in batch]\n",
        "    fens = [[fen for game['FEN']] for game in batch]\n",
        "    pgns = [[pgn for game['PGN']] for game in batch] # shape batch_size x block_size \n",
        "    # y = torch.stack([torch.cat((torch.from_numpy((game['np'][1:]).astype(np.int64)), torch.tensor([0]))) for game in batch]) # shape batch_size x block_size\n",
        "    # p = torch.stack([torch.stack([fen_tensor for fen_tensor in game]) for game in fens]) # shape batch_size x block_size x 128 \n",
        "    game_lengths = [game['game_length'] for game in batch]\n",
        "    game_colors = [game['color'] for game in batch]\n",
        "    # print(x.shape)\n",
        "    # print(y.shape)\n",
        "    # print(p.shape)\n",
        "    # if device == 'cuda': \n",
        "    #   x = x.pin_memory().to(device, non_blocking=True)\n",
        "    # else: \n",
        "    #   x = x.to(device) \n",
        "\n",
        "    return pgns, fens, game_lengths, game_colors\n"
      ],
      "metadata": {
        "id": "GEGMVJAadBLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(len(x))"
      ],
      "metadata": {
        "id": "kin3a7Tim1BN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  # vocab = dataset.pgn_vocab_stripped\n",
        "  \n",
        "  # stoi = { move:i for i, move in enumerate(vocab) }\n",
        "  # itos = { i:move for i, move in enumerate(vocab) }\n",
        "\n",
        "  # def encode(pgn): \n",
        "  #     return [stoi[move] for move in pgn]\n",
        "  # def decode(l): \n",
        "  #     return [itos[i] for i in l]\n"
      ],
      "metadata": {
        "id": "UJ87M6SjpDmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# x = encode([\"e4\", \"e5\", \"Nf3\", \"Nf6\"])\n",
        "# print(x)"
      ],
      "metadata": {
        "id": "uLxl3zsxpgtj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W5qhu4F7keTA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# x, y, fens = get_batch('test')\n",
        "\n",
        "# # def "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHELzh63kOBS",
        "outputId": "5aedb216-4bbe-4570-a66e-cc6196601f86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z2s0WPM5mPJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_iters = 10\n",
        "# max_new_tokens = 1\n",
        "# temperature = 0.8\n",
        "# top_k = 20\n",
        "\n",
        "correct_list = []\n",
        "trials_list = []]\n",
        "accuracies = []\n",
        "\n",
        "with torch.no_grad(): \n",
        "  with ctx: \n",
        "    for k in range(test_iters): \n",
        "      pgns, fens, game_lengths, game_colors = get_test_data() #size of 8 x _ \n",
        "      for i in range(batch_size):\n",
        "        pgn = pgns[i]\n",
        "        fen = fens[i]\n",
        "        game_length = game_lengths[i]\n",
        "        color = game_colors[i] \n",
        "        pad = 0 if color == \"white\" else 1 \n",
        "        correct = 0.0 \n",
        "        trials = 0.0 \n",
        "        if game_length < 20: \n",
        "          continue \n",
        "        for j in range(10*2 + pad, game_length - 1, 2): \n",
        "          pgn_up_to_move = pgn[:j]\n",
        "          fen_position = fen[:j]\n",
        "          # don't need to pad \n",
        "          x = torch.tensor(encode(pgn_up_to_move))\n",
        "          y = model.generate(x, fen_position)\n",
        "          trials += 1 \n",
        "          new_pgn = decode(y.tolist())\n",
        "          if new_pgn == pgn[:j+1]\n",
        "            correct += 1 \n",
        "        correct_list.append(correct)\n",
        "        trials_list.append(trials) \n",
        "        accuracies.append(correct / trials)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pIv1z86WLBtt",
        "outputId": "95c882ea-36ec-4833-92d8-a080bf28500b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['d4', 'Nf6', 'c4', 'c5', 'd5', 'g6', 'Nc3', 'Bg7', 'e4', 'd6', 'Be2', 'O-O', 'Nf3', 'e5', 'O-O', 'Ne8', 'a3', 'b4', 'Qe7', 'Rb1', 'h6', 'g3', 'Nc7', 'Be3', 'b6', 'Nh4', 'Kh7', 'Qc2', 'Bf6', 'Ng2', 'Bd3', 'Nf6', 'f3', 'Bh3', 'bxc5', 'bxc5', 'Rb3', 'Rfb8', 'Rfb1', 'Nd7', 'Rxb8', 'Rxb8', 'Rxb8', 'Nb5', 'Na8', '', '1/2-1/2', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>']\n",
            "['d4', 'Nf6', 'c4', 'c5', 'd5', 'g6', 'Nc3', 'Bg7', 'e4', 'd6', 'Be2', 'O-O', 'Nf3', 'e5', 'O-O', 'Ne8', 'a3', 'b4', 'Qe7', 'Rb1', 'h6', 'g3', 'Nc7', 'Be3', 'b6', 'Nh4', 'Kh7', 'Qc2', 'Bf6', 'Ng2', 'Bd3', 'Nf6', 'f3', 'Bh3', 'bxc5', 'bxc5', 'Rb3', 'Rfb8', 'Rfb1', 'Nd7', 'Rxb8', 'Rxb8', 'Rxb8', 'Nb5', 'Na8', '', '1/2-1/2', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', 'Rf8xh1', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>']\n",
            "---------------\n",
            "['e4', 'e5', 'Nf3', 'Nc6', 'Bb5', 'd6', 'd4', 'Bd7', 'Bxc6', 'Bxc6', 'Nc3', 'Nf6', 'Qd3', 'exd4', 'Nxd4', 'Nf5', 'g6', 'Nxe7', 'Qxe7', 'O-O', 'O-O-O', 'Re1', 'Nd7', 'Qd4', 'a6', 'Bf4', 'Rhe8', 'Re3', 'Bg3', 'Qg5', 'Rae1', 'f5', 'f4', 'Qh5', 'Nd5', 'fxe4', 'Nb4', 'Nb8', 'c4', 'Qc5', 'Qxc5', 'Nxc6', 'Nxc6', 'Rxe4', 'Rxe4', 'Rxe4', 'Rd4', 'Rxd4', 'cxd4', 'Be1', 'Kd7', 'Kf2', 'Ke2', 'Ke6', 'Kf3', 'Ne7', 'Ke4', 'Nf5', 'c5', 'Ne3', 'g3', 'h5', 'h3', 'Nf5', 'Bf2', 'g4', 'hxg4', 'hxg4', 'Nh6', 'Kf3', 'd3', 'Be3', 'Ng8', 'Bd2', 'Nf6', 'Bc3', 'Nd5', '', '1/2-1/2', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>']\n",
            "['e4', 'e5', 'Nf3', 'Nc6', 'Bb5', 'd6', 'd4', 'Bd7', 'Bxc6', 'Bxc6', 'Nc3', 'Nf6', 'Qd3', 'exd4', 'Nxd4', 'Nf5', 'g6', 'Nxe7', 'Qxe7', 'O-O', 'O-O-O', 'Re1', 'Nd7', 'Qd4', 'a6', 'Bf4', 'Rhe8', 'Re3', 'Bg3', 'Qg5', 'Rae1', 'f5', 'f4', 'Qh5', 'Nd5', 'fxe4', 'Nb4', 'Nb8', 'c4', 'Qc5', 'Qxc5', 'Nxc6', 'Nxc6', 'Rxe4', 'Rxe4', 'Rxe4', 'Rd4', 'Rxd4', 'cxd4', 'Be1', 'Kd7', 'Kf2', 'Ke2', 'Ke6', 'Kf3', 'Ne7', 'Ke4', 'Nf5', 'c5', 'Ne3', 'g3', 'h5', 'h3', 'Nf5', 'Bf2', 'g4', 'hxg4', 'hxg4', 'Nh6', 'Kf3', 'd3', 'Be3', 'Ng8', 'Bd2', 'Nf6', 'Bc3', 'Nd5', '', '1/2-1/2', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>']\n",
            "---------------\n",
            "['e4', 'e6', 'd4', 'd5', 'Nc3', 'Bb4', 'Bd3', 'dxe4', 'Bxe4', 'Nf6', 'Bg5', 'Nbd7', 'Bf3', 'c5', 'dxc5', 'Qd3', 'Nxc5', 'Qc4', 'Na4', 'Bd2', 'Nd5', 'Nge2', 'O-O', 'a3', 'Naxc3', 'Nxc3', 'Bxc3', 'bxc3', 'Qd4', 'Rac8', 'O-O', 'Qa4', 'Rfc1', 'Bc6', 'Rab1', 'b6', 'c4', 'Ne7', 'Be2', 'Rfd8', 'Qf4', 'Qe3', 'Bb7', 'Bc3', 'Qc6', 'Bf1', 'Ba6', 'Rb4', 'Rd7', 'Be2', 'Bb7', 'f3', 'Qc5', 'Qxc5', 'Rbb1', 'f6', 'Rd1', 'Rcc7', 'Rxd7', 'Rxd7', 'a4', 'Kf7', 'Kf2', 'Nf4', 'Bf1', 'e5', 'a5', 'Ra1', 'Rc7', 'Rxa5', 'Bc8', 'Bb4', 'Bf5', 'Bd6', 'Rb7', 'Rb5', 'Rb6', 'Bc5', 'Rc6', 'Bxa7', 'Be3', 'Bxc2', 'Bxf4', 'exf4', 'c5', 'Rc6', 'Bc4+', 'Ke7', 'Ke2', 'Rc7', 'Kd2', 'Ba4', 'Ra5', 'Kc3', 'g5', 'Ra2', 'h5', 'Re2+', 'Kf8', 'Re6', 'Kg7', 'Kd4', 'Rd7+', 'Rd6', 'Rc7', 'Bd5', 'Kxd5', 'Re7', 'c6', 'Re5+', 'Kc4', 'Re1', 'Kd3', '', '1-0', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>']\n",
            "['e4', 'e6', 'd4', 'd5', 'Nc3', 'Bb4', 'Bd3', 'dxe4', 'Bxe4', 'Nf6', 'Bg5', 'Nbd7', 'Bf3', 'c5', 'dxc5', 'Qd3', 'Nxc5', 'Qc4', 'Na4', 'Bd2', 'Nd5', 'Nge2', 'O-O', 'a3', 'Naxc3', 'Nxc3', 'Bxc3', 'bxc3', 'Qd4', 'Rac8', 'O-O', 'Qa4', 'Rfc1', 'Bc6', 'Rab1', 'b6', 'c4', 'Ne7', 'Be2', 'Rfd8', 'Qf4', 'Qe3', 'Bb7', 'Bc3', 'Qc6', 'Bf1', 'Ba6', 'Rb4', 'Rd7', 'Be2', 'Bb7', 'f3', 'Qc5', 'Qxc5', 'Rbb1', 'f6', 'Rd1', 'Rcc7', 'Rxd7', 'Rxd7', 'a4', 'Kf7', 'Kf2', 'Nf4', 'Bf1', 'e5', 'a5', 'Ra1', 'Rc7', 'Rxa5', 'Bc8', 'Bb4', 'Bf5', 'Bd6', 'Rb7', 'Rb5', 'Rb6', 'Bc5', 'Rc6', 'Bxa7', 'Be3', 'Bxc2', 'Bxf4', 'exf4', 'c5', 'Rc6', 'Bc4+', 'Ke7', 'Ke2', 'Rc7', 'Kd2', 'Ba4', 'Ra5', 'Kc3', 'g5', 'Ra2', 'h5', 'Re2+', 'Kf8', 'Re6', 'Kg7', 'Kd4', 'Rd7+', 'Rd6', 'Rc7', 'Bd5', 'Kxd5', 'Re7', 'c6', 'Re5+', 'Kc4', 'Re1', 'Kd3', '', '1-0', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>']\n",
            "---------------\n",
            "['Nf3', 'Nf6', 'c4', 'g6', 'g3', 'Bg7', 'Bg2', 'O-O', 'O-O', 'd6', 'd4', 'Nc6', 'Nc3', 'a6', 'h3', 'Be3', 'b5', 'cxb5', 'axb5', 'Nd2', 'Qe8', 'Rc1', 'Bd7', 'Ncb1', 'Rc8', 'Na3', 'Nd8', 'Nb3', 'dxe5', 'Qxe5', 'Bd4', 'Qe7', 'Nb1', 'b4', 'N1d2', 'Bb5', 'Nc4', 'Ne6', 'Ne3', 'c5', 'Bxf6', 'Rc2', 'c4', 'Qc1', 'cxb3', 'Rxc8', 'Nd4', 'Rxf8+', 'Bxf8', 'Qb1', 'Bxe2', 'Re1', 'bxa2', 'Qxa2', 'Bxf3', 'Bxf3', 'Qb3', 'Bc6', 'Nc2', 'Bh6', 'Nxb4', 'Bd2', 'Nxc6', 'Bxe1', 'Qe3', 'd5', 'Ne7+', 'Nxd5', 'Qf5', 'Qh6+', 'Ke8', 'Qe3+', 'Kf8', 'Qh6+', 'Ke8', 'Qe3+', 'Kf8', '', '1/2-1/2', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>']\n",
            "['Nf3', 'Nf6', 'c4', 'g6', 'g3', 'Bg7', 'Bg2', 'O-O', 'O-O', 'd6', 'd4', 'Nc6', 'Nc3', 'a6', 'h3', 'Be3', 'b5', 'cxb5', 'axb5', 'Nd2', 'Qe8', 'Rc1', 'Bd7', 'Ncb1', 'Rc8', 'Na3', 'Nd8', 'Nb3', 'dxe5', 'Qxe5', 'Bd4', 'Qe7', 'Nb1', 'b4', 'N1d2', 'Bb5', 'Nc4', 'Ne6', 'Ne3', 'c5', 'Bxf6', 'Rc2', 'c4', 'Qc1', 'cxb3', 'Rxc8', 'Nd4', 'Rxf8+', 'Bxf8', 'Qb1', 'Bxe2', 'Re1', 'bxa2', 'Qxa2', 'Bxf3', 'Bxf3', 'Qb3', 'Bc6', 'Nc2', 'Bh6', 'Nxb4', 'Bd2', 'Nxc6', 'Bxe1', 'Qe3', 'd5', 'Ne7+', 'Nxd5', 'Qf5', 'Qh6+', 'Ke8', 'Qe3+', 'Kf8', 'Qh6+', 'Ke8', 'Qe3+', 'Kf8', '', '1/2-1/2', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>']\n",
            "---------------\n",
            "['Nf3', 'c5', 'c4', 'Nf6', 'g3', 'b6', 'Bg2', 'Bb7', 'O-O', 'Nc6', 'Nc3', 'e6', 'd4', 'Nxd4', 'Nxd4', 'Kxg2', 'cxd4', 'Qxd4', 'Be7', 'b3', 'O-O', 'Bb2', 'Qc7', 'Rfd1', 'Rfd8', 'Qd3', 'Qb7+', 'Qf3', 'Kxf3', 'd6', 'Nb5', '', '1/2-1/2', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>']\n",
            "['Nf3', 'c5', 'c4', 'Nf6', 'g3', 'b6', 'Bg2', 'Bb7', 'O-O', 'Nc6', 'Nc3', 'e6', 'd4', 'Nxd4', 'Nxd4', 'Kxg2', 'cxd4', 'Qxd4', 'Be7', 'b3', 'O-O', 'Bb2', 'Qc7', 'Rfd1', 'Rfd8', 'Qd3', 'Qb7+', 'Qf3', 'Kxf3', 'd6', 'Nb5', '', '1/2-1/2', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>']\n",
            "---------------\n",
            "['d4', 'd5', 'c4', 'e6', 'Nc3', 'Be7', 'cxd5', 'exd5', 'Bf4', 'Nf6', 'e3', 'O-O', 'Bd3', 'c5', 'Nf3', 'O-O', 'cxd4', 'Nxd4', 'Bg4', 'Qa4', 'Nxd4', 'Qxd4', 'Qd7', 'h3', 'Be6', 'Rfd1', 'Rfc8', 'Be5', 'Rd2', 'Qd8', 'Bc2', 'Qa5', 'Qd3', 'Kf8', 'Rad1', 'Rd8', 'Qd4', 'Rac8', 'Bb3', 'Kg8', 'Qf4', 'Qg3', 'Nh5', 'Qf3', 'Nf6', 'Rd3', 'a6', 'Bxf6', 'Bxf6', 'Bxd5', 'Rcd8', 'e4', 'Bxc3', 'Bxe6', 'Qxf7+', 'Kh8', 'Rxd3', 'Bf6', 'Rxd8+', 'Qxd8', 'Qxb7', 'Qd1+', 'Kh2', 'Qd6+', 'g3', 'b3', 'Qd6', 'Kg2', 'Bd4', 'h4', 'Qf6', 'f4', 'Qd6', 'e5', 'Qc5', 'Qxa6', 'Qd5+', 'Kh2', 'Qc8+', 'Kh7', 'Qc2+', 'Kh8', 'Qg2', 'Qd3', 'h5', 'Bc5', 'e6', 'Kg8', 'a4', 'Qxb3', 'Qa8+', 'Qe8', 'Qc2+', 'Kh3', 'Qb1', 'Qf7+', 'Kh7', 'Qg6+', 'Qxg6', 'hxg6+', 'Kxg6', 'a5', 'h5', 'f5+', 'a6', 'Bc5', 'Kh4', 'g6', 'e7', 'Kxe7', 'fxg6', 'Kf6', '', '1/2-1/2', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>']\n",
            "['d4', 'd5', 'c4', 'e6', 'Nc3', 'Be7', 'cxd5', 'exd5', 'Bf4', 'Nf6', 'e3', 'O-O', 'Bd3', 'c5', 'Nf3', 'O-O', 'cxd4', 'Nxd4', 'Bg4', 'Qa4', 'Nxd4', 'Qxd4', 'Qd7', 'h3', 'Be6', 'Rfd1', 'Rfc8', 'Be5', 'Rd2', 'Qd8', 'Bc2', 'Qa5', 'Qd3', 'Kf8', 'Rad1', 'Rd8', 'Qd4', 'Rac8', 'Bb3', 'Kg8', 'Qf4', 'Qg3', 'Nh5', 'Qf3', 'Nf6', 'Rd3', 'a6', 'Bxf6', 'Bxf6', 'Bxd5', 'Rcd8', 'e4', 'Bxc3', 'Bxe6', 'Qxf7+', 'Kh8', 'Rxd3', 'Bf6', 'Rxd8+', 'Qxd8', 'Qxb7', 'Qd1+', 'Kh2', 'Qd6+', 'g3', 'b3', 'Qd6', 'Kg2', 'Bd4', 'h4', 'Qf6', 'f4', 'Qd6', 'e5', 'Qc5', 'Qxa6', 'Qd5+', 'Kh2', 'Qc8+', 'Kh7', 'Qc2+', 'Kh8', 'Qg2', 'Qd3', 'h5', 'Bc5', 'e6', 'Kg8', 'a4', 'Qxb3', 'Qa8+', 'Qe8', 'Qc2+', 'Kh3', 'Qb1', 'Qf7+', 'Kh7', 'Qg6+', 'Qxg6', 'hxg6+', 'Kxg6', 'a5', 'h5', 'f5+', 'a6', 'Bc5', 'Kh4', 'g6', 'e7', 'Kxe7', 'fxg6', 'Kf6', '', '1/2-1/2', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>']\n",
            "---------------\n",
            "['d4', 'g6', 'Nf3', 'Bg7', 'g3', 'Nf6', 'c4', 'O-O', 'Bg2', 'd5', 'cxd5', 'Nxd5', 'O-O', 'Nb6', 'Nc3', 'e3', 'e5', 'd5', 'Ne7', 'e4', 'Bg4', 'Bg5', 'h6', 'Bxe7', 'Qxe7', 'Qb3', 'c6', 'h3', 'Bxf3', 'Bxf3', 'Rfd1', 'cxd5', 'Nxd5', 'Nxd5', 'Rxd5', '', '1/2-1/2', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>']\n",
            "['d4', 'g6', 'Nf3', 'Bg7', 'g3', 'Nf6', 'c4', 'O-O', 'Bg2', 'd5', 'cxd5', 'Nxd5', 'O-O', 'Nb6', 'Nc3', 'e3', 'e5', 'd5', 'Ne7', 'e4', 'Bg4', 'Bg5', 'h6', 'Bxe7', 'Qxe7', 'Qb3', 'c6', 'h3', 'Bxf3', 'Bxf3', 'Rfd1', 'cxd5', 'Nxd5', 'Nxd5', 'Rxd5', '', '1/2-1/2', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>']\n",
            "---------------\n",
            "['e4', 'c5', 'Nf3', 'e6', 'd4', 'cxd4', 'Nxd4', 'Nc6', 'Nc3', 'a6', 'Be2', 'Qc7', 'f4', 'b5', 'Nxc6', 'Bf3', 'Bb7', 'Be3', 'Rc8', 'a3', 'Qc7', 'Qd4', 'Ne7', 'O-O-O', 'Nc6', 'Qb6', 'Qxb6', 'Bxb6', 'a4', 'b4', 'Nb1', 'Nb8', 'Nd2', 'Nd7', 'Ba5', 'Nc5', 'Kb1', 'Nxa4', 'Bxb4', 'Be7', 'Rhe1', 'e5', 'Bxf3', 'Nxf3', 'Rb8', 'Ba3', 'Nc3+', 'Kc1', 'Na2+', 'Kb1', 'Nc3+', 'Ka1', 'Nxd1', 'exd6', 'dxe7', 'Rfe8', 'Bxb2', 'Rxe7', 'c4', 'Reb7', 'Be5', 'Rc8', 'Rc1', 'f6', 'Bb2', 'Rbc7', 'Nd2', 'Re1', 'Kf7', 'Bc3', 'Rxc4', 'Nxc4', 'Rxc4', 'Bd2', 'Rd4', 'Re2', 'Kg6', 'g4', 'h5', '', '1/2-1/2', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>']\n",
            "['e4', 'c5', 'Nf3', 'e6', 'd4', 'cxd4', 'Nxd4', 'Nc6', 'Nc3', 'a6', 'Be2', 'Qc7', 'f4', 'b5', 'Nxc6', 'Bf3', 'Bb7', 'Be3', 'Rc8', 'a3', 'Qc7', 'Qd4', 'Ne7', 'O-O-O', 'Nc6', 'Qb6', 'Qxb6', 'Bxb6', 'a4', 'b4', 'Nb1', 'Nb8', 'Nd2', 'Nd7', 'Ba5', 'Nc5', 'Kb1', 'Nxa4', 'Bxb4', 'Be7', 'Rhe1', 'e5', 'Bxf3', 'Nxf3', 'Rb8', 'Ba3', 'Nc3+', 'Kc1', 'Na2+', 'Kb1', 'Nc3+', 'Ka1', 'Nxd1', 'exd6', 'dxe7', 'Rfe8', 'Bxb2', 'Rxe7', 'c4', 'Reb7', 'Be5', 'Rc8', 'Rc1', 'f6', 'Bb2', 'Rbc7', 'Nd2', 'Re1', 'Kf7', 'Bc3', 'Rxc4', 'Nxc4', 'Rxc4', 'Bd2', 'Rd4', 'Re2', 'Kg6', 'g4', 'h5', '', '1/2-1/2', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>']\n",
            "---------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0-c3LdQPlFzE"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}