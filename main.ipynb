{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chess 1.9.4\n",
      "Torch 2.0.0\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import chess \n",
    "import chess.pgn \n",
    "import chess.engine\n",
    "from stockfish import Stockfish\n",
    "\n",
    "import math \n",
    "from dataclasses import dataclass \n",
    "import numpy as np \n",
    "import os \n",
    "import time \n",
    "import asyncio\n",
    "\n",
    "print(\"Chess\", chess.__version__)\n",
    "print(\"Torch\", torch.__version__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "Only need to run once. Create a data folder before running. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Generate PGN and FEN files for Black and White Colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = \"data/Tal.pgn\"\n",
    "\n",
    "# # save the games based on if tal played white or black\n",
    "# games = {\n",
    "#     \"white\": [], \n",
    "#     \"black\": []\n",
    "# }\n",
    "\n",
    "# with open(file_path) as f: \n",
    "#     lines = f.readlines() \n",
    "#     tal_color = None \n",
    "#     on_pgn_line = False  \n",
    "#     pgn = \"\"\n",
    "#     for line in lines:\n",
    "#         if on_pgn_line:\n",
    "#             if line.startswith(\"[Event\"): \n",
    "#                 on_pgn_line = False \n",
    "#                 games[tal_color].append(pgn)\n",
    "#                 pgn = \"\"\n",
    "#             else: \n",
    "#                 pgn += line \n",
    "#         if line.startswith(\"[White \"): \n",
    "#             player_white = line.split('\"')[1]\n",
    "#             if player_white == \"Tal, Mihail\":\n",
    "#                 tal_color = \"white\"\n",
    "#         elif line.startswith(\"[Black \"):\n",
    "#             player_black = line.split('\"')[1]\n",
    "#             if player_black == \"Tal, Mihail\":\n",
    "#                 tal_color = \"black\"\n",
    "#         elif line.startswith(\"1.\"): \n",
    "#             on_pgn_line = True \n",
    "#             pgn += line \n",
    "#         else: \n",
    "#             # print(tal_color)\n",
    "#             continue \n",
    "\n",
    "\n",
    "# f.close()  \n",
    "\n",
    "# with open(\"data/tal_white_games.pgn\", \"w\") as f: \n",
    "#     for game in games[\"white\"]:\n",
    "#         f.write(game)\n",
    "\n",
    "# f.close()\n",
    "\n",
    "# with open(\"data/tal_black_games.pgn\", \"w\") as f:\n",
    "#     for game in games[\"black\"]:\n",
    "#         f.write(game)\n",
    "\n",
    "# f.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Generate UCI Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from itertools import product\n",
    "\n",
    "# files = [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\"]\n",
    "# ranks = [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\"]\n",
    "# promotion = [\"Q\", \"R\", \"B\", \"N\"]\n",
    "\n",
    "# # gets an upper bound for the UCI vocab\n",
    "\n",
    "# with open(\"data/UCIvocab.txt\", \"w\") as f:\n",
    "#     for file1, rank1, file2, rank2 in product(files, ranks, files, ranks):\n",
    "#         if file1 == file2 and rank1 == rank2:\n",
    "#             continue \n",
    "#         if rank2 == \"8\" and rank1 == \"7\" or rank2 == \"1\" and rank1 == \"2\":\n",
    "#             idx1 = files.index(file1)\n",
    "#             idx2 = files.index(file2)\n",
    "#             if abs(idx1 - idx2) > 1:\n",
    "#                 continue\n",
    "#             for promotion_piece in promotion:\n",
    "#                 f.write(f'{file1}{rank1}{file2}{rank2}{promotion_piece}\\n') \n",
    "#         f.write(f'{file1}{rank1}{file2}{rank2}\\n')\n",
    "\n",
    "# f.close() "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Generate PGN Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pieces = [\"K\", \"Q\", \"R\", \"B\", \"N\"] \n",
    "# pawns = [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\"]\n",
    "# files = [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\"]\n",
    "# ranks = [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\"]\n",
    "\n",
    "# moves = set()\n",
    "\n",
    "# for piece in pieces: \n",
    "#     for file in files: \n",
    "#         for rank in ranks: \n",
    "#             moves.add(f\"{piece}{file}{rank}\")\n",
    "#             moves.add(f\"{piece}{file}{rank}+\")\n",
    "#             moves.add(f\"{piece}{file}{rank}#\")\n",
    "#             moves.add(f\"{piece}x{file}{rank}\")\n",
    "#             moves.add(f\"{piece}x{file}{rank}+\")\n",
    "#             moves.add(f\"{piece}x{file}{rank}#\")\n",
    "#             # 2 pieces can move to the same square \n",
    "#             if piece == \"R\" or piece == \"N\" or piece == \"B\" or piece == \"Q\": \n",
    "#                 for rank1 in ranks: \n",
    "#                     moves.add(f\"{piece}{rank1}{file}{rank}\")\n",
    "#                     moves.add(f\"{piece}{rank1}{file}{rank}+\")\n",
    "#                     moves.add(f\"{piece}{rank1}{file}{rank}#\")\n",
    "#                     moves.add(f\"{piece}{rank1}x{file}{rank}\")\n",
    "#                     moves.add(f\"{piece}{rank1}x{file}{rank}+\")\n",
    "#                     moves.add(f\"{piece}{rank1}x{file}{rank}#\")\n",
    "#                 for file1 in files: \n",
    "#                     moves.add(f\"{piece}{file1}{file}{rank}\")\n",
    "#                     moves.add(f\"{piece}{file1}{file}{rank}+\")\n",
    "#                     moves.add(f\"{piece}{file1}{file}{rank}#\")\n",
    "#                     moves.add(f\"{piece}{file1}x{file}{rank}\")\n",
    "#                     moves.add(f\"{piece}{file1}x{file}{rank}+\")\n",
    "#                     moves.add(f\"{piece}{file1}x{file}{rank}#\")\n",
    "#                 # if there are 3 pieces due to promotion, need to specify file and rank \n",
    "#                 for rank1 in ranks: \n",
    "#                     for file1 in files: \n",
    "#                         moves.add(f\"{piece}{file1}{rank1}{file}{rank}\")\n",
    "#                         moves.add(f\"{piece}{file1}{rank1}{file}{rank}+\")\n",
    "#                         moves.add(f\"{piece}{file1}{rank1}{file}{rank}#\")\n",
    "#                         moves.add(f\"{piece}{file1}{rank1}x{file}{rank}\")\n",
    "#                         moves.add(f\"{piece}{file1}{rank1}x{file}{rank}+\")\n",
    "#                         moves.add(f\"{piece}{file1}{rank1}x{file}{rank}#\")\n",
    "\n",
    "# for (idx, pawn) in enumerate(pawns): \n",
    "#     for rank in ranks[1:6]: #2,3,4,5,6,7\n",
    "#         # pawn pushes \n",
    "#         if rank == \"7\": \n",
    "#             for piece in pieces[1:]: \n",
    "#                 moves.add(f\"{pawn}{str(int(rank)+1)}={piece}\")\n",
    "#                 moves.add(f\"{pawn}{str(int(rank)+1)}={piece}+\")\n",
    "#                 moves.add(f\"{pawn}{str(int(rank)+1)}={piece}#\")\n",
    "#         elif rank == \"2\":\n",
    "#             for piece in pieces[1:]: \n",
    "#                 moves.add(f\"{pawn}{str(int(rank)-1)}={piece}\")\n",
    "#                 moves.add(f\"{pawn}{str(int(rank)-1)}={piece}+\")\n",
    "#                 moves.add(f\"{pawn}{str(int(rank)-1)}={piece}#\")  \n",
    "#         else: \n",
    "#             moves.add(f\"{pawn}{str(int(rank)+1)}\")\n",
    "#             moves.add(f\"{pawn}{str(int(rank)+1)}+\")\n",
    "#             moves.add(f\"{pawn}{str(int(rank)+1)}#\")\n",
    "#             moves.add(f\"{pawn}{str(int(rank)-1)}\")\n",
    "#             moves.add(f\"{pawn}{str(int(rank)-1)}+\")\n",
    "#             moves.add(f\"{pawn}{str(int(rank)-1)}#\")\n",
    "#         # pawn captures \n",
    "#         if pawn == \"a\": \n",
    "#             if rank == \"7\": \n",
    "#                 for piece in pieces[1:]: \n",
    "#                     moves.add(f\"{pawn}xb{str(int(rank)+1)}={piece}\")\n",
    "#                     moves.add(f\"{pawn}xb{str(int(rank)+1)}={piece}+\")\n",
    "#                     moves.add(f\"{pawn}xb{str(int(rank)+1)}={piece}#\")\n",
    "#             elif rank == \"2\":\n",
    "#                 for piece in pieces[1:]:\n",
    "#                     moves.add(f\"{pawn}xb{str(int(rank)-1)}={piece}\")\n",
    "#                     moves.add(f\"{pawn}xb{str(int(rank)-1)}={piece}+\")\n",
    "#                     moves.add(f\"{pawn}xb{str(int(rank)-1)}={piece}#\")\n",
    "#             else: \n",
    "#                 moves.add(f\"{pawn}xb{str(int(rank)+1)}\")\n",
    "#                 moves.add(f\"{pawn}xb{str(int(rank)+1)}+\")\n",
    "#                 moves.add(f\"{pawn}xb{str(int(rank)+1)}#\")\n",
    "#                 moves.add(f\"{pawn}xb{str(int(rank)-1)}\")\n",
    "#                 moves.add(f\"{pawn}xb{str(int(rank)-1)}+\")\n",
    "#                 moves.add(f\"{pawn}xb{str(int(rank)-1)}#\")\n",
    "#         elif pawn == \"h\": \n",
    "#             if rank == \"7\":\n",
    "#                 for piece in pieces[1:]:\n",
    "#                     moves.add(f\"{pawn}xg{str(int(rank)+1)}={piece}\")\n",
    "#                     moves.add(f\"{pawn}xg{str(int(rank)+1)}={piece}+\")\n",
    "#                     moves.add(f\"{pawn}xg{str(int(rank)+1)}={piece}#\")\n",
    "#             elif rank == \"2\":\n",
    "#                 for piece in pieces[1:]:\n",
    "#                     moves.add(f\"{pawn}xg{str(int(rank)-1)}={piece}\")\n",
    "#                     moves.add(f\"{pawn}xg{str(int(rank)-1)}={piece}+\")\n",
    "#                     moves.add(f\"{pawn}xg{str(int(rank)-1)}={piece}#\")\n",
    "#             else: \n",
    "#                 moves.add(f\"{pawn}xg{str(int(rank)+1)}\")\n",
    "#                 moves.add(f\"{pawn}xg{str(int(rank)+1)}+\")\n",
    "#                 moves.add(f\"{pawn}xg{str(int(rank)+1)}#\")\n",
    "#                 moves.add(f\"{pawn}xg{str(int(rank)-1)}\")\n",
    "#                 moves.add(f\"{pawn}xg{str(int(rank)-1)}+\")\n",
    "#                 moves.add(f\"{pawn}xg{str(int(rank)-1)}#\")\n",
    "#         else: \n",
    "#             if rank == \"7\":\n",
    "#                 for piece in pieces[1:]: \n",
    "#                     moves.add(f\"{pawn}{str(int(rank)+1)}={piece}\")\n",
    "#                     moves.add(f\"{pawn}{str(int(rank)+1)}={piece}+\")\n",
    "#                     moves.add(f\"{pawn}{str(int(rank)+1)}={piece}#\")\n",
    "#                     moves.add(f\"{pawn}x{str(int(rank)+1)}={piece}\")\n",
    "#                     moves.add(f\"{pawn}x{str(int(rank)+1)}={piece}+\")\n",
    "#                     moves.add(f\"{pawn}x{str(int(rank)+1)}={piece}#\")\n",
    "#             elif rank == \"2\":\n",
    "#                 for piece in pieces[1:]: \n",
    "#                     moves.add(f\"{pawn}{str(int(rank)-1)}={piece}\")\n",
    "#                     moves.add(f\"{pawn}{str(int(rank)-1)}={piece}+\")\n",
    "#                     moves.add(f\"{pawn}{str(int(rank)-1)}={piece}#\")\n",
    "#                     moves.add(f\"{pawn}x{str(int(rank)-1)}={piece}\")\n",
    "#                     moves.add(f\"{pawn}x{str(int(rank)-1)}={piece}+\")\n",
    "#                     moves.add(f\"{pawn}x{str(int(rank)-1)}={piece}#\")\n",
    "#             else: \n",
    "#                 moves.add(f\"{pawn}x{pawns[idx-1]}{str(int(rank)+1)}\")\n",
    "#                 moves.add(f\"{pawn}x{pawns[idx-1]}{str(int(rank)+1)}+\")\n",
    "#                 moves.add(f\"{pawn}x{pawns[idx-1]}{str(int(rank)+1)}#\")\n",
    "#                 moves.add(f\"{pawn}x{pawns[idx+1]}{str(int(rank)+1)}\")\n",
    "#                 moves.add(f\"{pawn}x{pawns[idx+1]}{str(int(rank)+1)}+\")\n",
    "#                 moves.add(f\"{pawn}x{pawns[idx+1]}{str(int(rank)+1)}#\")\n",
    "#                 moves.add(f\"{pawn}x{pawns[idx-1]}{str(int(rank)-1)}\")\n",
    "#                 moves.add(f\"{pawn}x{pawns[idx-1]}{str(int(rank)-1)}+\")\n",
    "#                 moves.add(f\"{pawn}x{pawns[idx-1]}{str(int(rank)-1)}#\")\n",
    "#                 moves.add(f\"{pawn}x{pawns[idx+1]}{str(int(rank)-1)}\")\n",
    "#                 moves.add(f\"{pawn}x{pawns[idx+1]}{str(int(rank)-1)}+\")\n",
    "#                 moves.add(f\"{pawn}x{pawns[idx+1]}{str(int(rank)-1)}#\")\n",
    "\n",
    "# # add castling         \n",
    "# moves.add(\"O-O\")\n",
    "# moves.add(\"O-O+\")\n",
    "# moves.add(\"O-O#\")\n",
    "# moves.add(\"O-O-O\")\n",
    "# moves.add(\"O-O-O+\")\n",
    "# moves.add(\"O-O-O#\")\n",
    "\n",
    "# with open(\"data/PGNVocab.txt\", \"w\") as f: \n",
    "#     for move in moves: \n",
    "#         f.write(move + \"\\n\")\n",
    "\n",
    "# f.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_names = [\"tal_black_games\", \"tal_white_games\"]\n",
    "\n",
    "# for file in file_names:\n",
    "#     pgn = open(f\"data/{file}.pgn\", 'r')\n",
    "#     iter = 0 \n",
    "#     while iter < 1: \n",
    "#         iter += 1 \n",
    "#         game = chess.pgn.read_game(pgn)\n",
    "#         if game is None: \n",
    "#             break\n",
    "#         board = game.board()\n",
    "#         # print(type(board)) \n",
    "#         for move in game.mainline_moves():\n",
    "#             # print(type(move))\n",
    "#             # print(move)\n",
    "#             # print(board.san(move)) \n",
    "#             board.push(move) \n",
    "#             # print(move) #uci \n",
    "#             # print(board.fen()) #fen\n",
    "#             # print(board.san(move))\n",
    "#             # how to get pgn? \n",
    "\n",
    "#     pgn.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Generate PGNVocab Stripped of + and \\#  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# moves = set() \n",
    "\n",
    "# with open(\"data/PGNVocab.txt\", \"r\") as f: \n",
    "#     lines = f.readlines() \n",
    "#     for line in lines: \n",
    "#         moves.add(line.strip(\"+#\\n\"))\n",
    "\n",
    "# f.close() \n",
    "\n",
    "# with open(\"data/PGNVocabStrip.txt\", \"w\") as f: \n",
    "#     for move in moves: \n",
    "#         f.write(move + \"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chess Agent used to interface with Stockfish engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass \n",
    "class ChessAgentConfig: \n",
    "    depth: int = 20 # engine depth \n",
    "    mate_score: int = 10000 # engine mate score\n",
    "\n",
    "class ChessAgent: \n",
    "    def __init__(self, config):\n",
    "        self.engine = chess.engine.SimpleEngine.popen_uci(\"stockfish\")\n",
    "        self.config = config \n",
    "\n",
    "    def fen_and_pgn_from_uci(self, uci, fen): \n",
    "        board = chess.Board(fen)\n",
    "        move = chess.Move.from_uci(uci)\n",
    "        pgn = board.san(move)\n",
    "        board.push(move)\n",
    "        fen = board.fen()\n",
    "        \n",
    "        return fen, pgn \n",
    "\n",
    "    async def position_info(self, fen): \n",
    "        board = chess.Board(fen)\n",
    "        return self.engine.analyse(board, chess.engine.Limit(depth=20))\n",
    "\n",
    "    async def position_eval(self, fen): \n",
    "        info = await self.position_info(fen)\n",
    "        return info[\"score\"].white().score(mate_score=self.config.mate_score)\n",
    "\n",
    "    def quit(self): \n",
    "        self.engine.quit()\n",
    "\n",
    "asyncio.set_event_loop_policy(chess.engine.EventLoopPolicy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "chessConfig = ChessAgentConfig()\n",
    "\n",
    "chessAgent = ChessAgent(chessConfig) \n",
    "\n",
    "fen, pgn = chessAgent.fen_and_pgn_from_uci(\"e2e4\",\"rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq -\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r n b q k b n r\n",
      "p p p p . p p p\n",
      ". . . . p . . .\n",
      ". . . . . . . .\n",
      ". . . . . P P .\n",
      ". . . . . . . .\n",
      "P P P P P . . P\n",
      "R N B Q K B N R\n",
      "Black Eval:  -9999\n",
      "\n",
      "\n",
      "r n b q k b n r\n",
      "p p p p p . . p\n",
      ". . . . . . . .\n",
      ". . . . . p p .\n",
      ". . . . P . . .\n",
      ". . . . . . . .\n",
      "P P P P Q P P P\n",
      "R N B . K B N R\n",
      "White Eval:  9999\n"
     ]
    }
   ],
   "source": [
    "black_mate_in_1 = \"rnbqkbnr/pppp1ppp/4p3/8/5PP1/8/PPPPP2P/RNBQKBNR b KQkq - 0 2\"\n",
    "white_mate_in_1 = \"rnbqkbnr/ppppp2p/8/5pp1/4P3/8/PPPPQPPP/RNB1KBNR w KQkq - 0 3\"\n",
    "\n",
    "print(chess.Board(black_mate_in_1))\n",
    "eval_black = await chessAgent.position_eval(black_mate_in_1)\n",
    "print(\"Black Eval: \", eval_black)\n",
    "print(\"\\n\")\n",
    "print(chess.Board(white_mate_in_1))\n",
    "eval_white = await chessAgent.position_eval(white_mate_in_1)\n",
    "print(\"White Eval: \", eval_white)\n",
    "\n",
    "chessAgent.quit()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Custom Dataset Class to be used in Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChessDataset(Dataset): \n",
    "    def __init__(self, white, black): \n",
    "        super().__init__()\n",
    "\n",
    "        assert os.path.exists(f\"data/{white}.pgn\"), f\"data/{white} does not exist\" \n",
    "        assert os.path.exists(f\"data/{white}_fen.txt\"), f\"data/{white}_fen.txt does not exist\" \n",
    "\n",
    "        self.white_uci = self.load_uci_moves(white)\n",
    "        self.white_fen = self.load_fen_moves(white)\n",
    "        self.white_pgn = self.load_pgn_moves(white)\n",
    "\n",
    "        assert len(self.white_uci) == len(self.white_fen), f\"white_uci and white_fen are not the same length\"\n",
    "\n",
    "        assert os.path.exists(f\"data/{black}.pgn\"), f\"data/{black} does not exist\"\n",
    "        assert os.path.exists(f\"data/{black}_fen.txt\"), f\"data/{black}_fen.txt does not exist\"\n",
    "\n",
    "        self.black_uci = self.load_uci_moves(black)\n",
    "        self.black_fen = self.load_fen_moves(black)\n",
    "        self.black_pgn = self.load_pgn_moves(black)\n",
    "\n",
    "        assert len(self.black_uci) == len(self.black_fen), f\"black_uci and black_fen are not the same length\"\n",
    "\n",
    "        self.pgn_vocab = self.load_pgn_vocab() \n",
    "         \n",
    "        self.pgn_embedding = nn.Embedding(131072, 768)\n",
    "        self.uci_embedding = nn.Embedding(8192, 768)\n",
    "\n",
    "    def load_pgn_vocab(self): \n",
    "        vocab = open(\"data/PGNVocab.txt\", 'r')\n",
    "        vocab_dict = dict()\n",
    "        for (idx,line) in enumerate(vocab.readlines()): \n",
    "            vocab_dict[line.strip(\"\\n\")] = idx\n",
    "            \n",
    "        vocab.close() \n",
    "\n",
    "        return vocab_dict\n",
    "        \n",
    "    def load_fen_moves(self, file_name): \n",
    "        with open(f\"data/{file_name}_fen.txt\", 'r') as f: \n",
    "            lines = f.readlines() \n",
    "            games = []\n",
    "            for line in lines: \n",
    "                games.append(line.strip(\"\\n\").split(\";\")[:-1])\n",
    "        f.close() \n",
    "\n",
    "        return games\n",
    "\n",
    "    def load_uci_moves(self, file_name): \n",
    "        pgn = open(f\"data/{file_name}.pgn\", 'r')\n",
    "        games = [] \n",
    "\n",
    "        while True: \n",
    "            game = chess.pgn.read_game(pgn)\n",
    "            if game is None: \n",
    "                break\n",
    "            board = game.board() \n",
    "            moves = []\n",
    "            for move in game.mainline_moves(): \n",
    "                moves.append(str(move))\n",
    "            games.append(moves)\n",
    "        \n",
    "        return games \n",
    "\n",
    "    def load_pgn_moves(self, file_name): \n",
    "        pgn = open(f\"data/{file_name}.pgn\", \"r\")\n",
    "        games = [] \n",
    "\n",
    "        pgn_content = pgn.read() \n",
    "\n",
    "        games = pgn_content.split(\"\\n\\n\")[:-1]\n",
    "\n",
    "        game_move_list = []\n",
    "\n",
    "        for game in games: \n",
    "            moves = [move[max(0,move.find(\".\")+1):].strip(\"\\n\") for move in game.split(\" \")]\n",
    "            game_move_list.append(moves)\n",
    "\n",
    "        return game_move_list\n",
    "\n",
    "    def __len__(self): \n",
    "        return len(self.white_uci) + len(self.black_uci)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx < len(self.white_uci):\n",
    "            white_uci_game = self.white_uci[idx]\n",
    "            white_fen_game = self.white_fen[idx]\n",
    "            white_pgn_game = self.white_pgn[idx]\n",
    "            game = {\"UCI\": white_uci_game, \"PGN\": white_pgn_game, \"FEN\": white_fen_game, \"color\": \"white\"}\n",
    "        else:\n",
    "            idx -= len(self.white_uci)\n",
    "            black_uci_game = self.black_uci[idx]\n",
    "            black_fen_game = self.black_fen[idx]\n",
    "            black_pgn_game = self.black_pgn[idx]\n",
    "            game = {\"UCI\": black_uci_game, \"PGN\": black_pgn_game, \"FEN\": black_fen_game, \"color\": \"black\"}\n",
    "\n",
    "        return game\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Blocks for Transformer \n",
    "\n",
    "Sourced Heavily from https://github.com/karpathy/nanoGPT/blob/master/model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, ndim, bias):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))\n",
    "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n",
    "\n",
    "# masked attention \n",
    "class CausalSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias) # in features, out features. ok this makes sense because it is a n_embd size matrix. \n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.dropout = config.dropout\n",
    "\n",
    "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
    "        if not self.flash:\n",
    "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
    "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                        .view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # hs = hidden state \n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q, k ,v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs) # .view() returns a new tensor of the same data but with a different shape specified by the args \n",
    "        # reshape() is a more robust version of view(). has to do with the tensor being contiguous or not. view() is more efficient if the tensor is contiguous. \n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        if self.flash:\n",
    "            # efficient attention using Flash Attention CUDA kernels\n",
    "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout, is_causal=True)\n",
    "        else:\n",
    "            # manual implementation of attention\n",
    "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "            att = F.softmax(att, dim=-1)\n",
    "            att = self.attn_dropout(att)\n",
    "            y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y\n",
    "\n",
    "class CausalCrossAttention(CausalSelfAttention): \n",
    "\n",
    "    def __init__(self, config): \n",
    "        super().__init__(config)\n",
    "        self.c_attn = nn.Linear(config.n_embd, config.n_embd * 2, bias=config.bias)\n",
    "        \n",
    "    def forward(self, x1, x2): \n",
    "        # x1 provides the Q and K \n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        q, k = self.c_attn(x1).split(self.n_embd, dim=2)\n",
    "        v = self.c_attn(x2)\n",
    "\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "\n",
    "        if self.flash: \n",
    "            y = torch.nn.functional.scaled_dot_product_attention(q, k , v, attn_mask=None, dropout_p=self.dropout, is_causal=True)\n",
    "        else: \n",
    "            att = (q @ k.transpose(-2,-1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "            att = F.softmax(att, dim=-1)\n",
    "            att = self.attn_dropout(att)\n",
    "            y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side \n",
    "\n",
    "        # output projection \n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y \n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias) # classifier fully connected\n",
    "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias) # classifier projection \n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = new_gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    # uses pre layer norm \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "class DecoderBlock(nn.Module): \n",
    "    def __init__(self, config): \n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.attn_1 = CausalSelfAttention(config)\n",
    "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.attn_2 = CausalCrossAttention(config)\n",
    "        self.ln_3 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.mlp = MLP(config)\n",
    "    \n",
    "    def forward(self, x1, x2): \n",
    "        x2 = x2 + self.attn_1(self.ln_1(x2))\n",
    "        x2 = x2 + self.attn_2(self.ln_2(x2), x1)\n",
    "        x2 = x2 + self.mlp(self.ln_3(x2))\n",
    "        return x2\n",
    "\n",
    "@dataclass \n",
    "class ChessGPTConfig: \n",
    "    block_size: int = 512 # longest chess game was 269 moves or 538 half moves. However, that's the only one. We can discard that as an outlier. 512 covers basically every chess game. \n",
    "    vocab_size: int = 131072 # using the PGNVocab.txt, there are 125394 words in the vocabulary. Not every vocabulary will be found in real games. Most will not because most of the moves are very rare. Some are also impossible. \n",
    "    n_layer: int = 12 # number of encoder and decoder blocks in the stack \n",
    "    n_head: int = 12 # multihead attention blocks \n",
    "    n_embd: int = 768 # size of the embedding... originally chosen by GPT-2 ... may have to modify later \n",
    "    dropout: float = 0.0 # none for now \n",
    "    bias: bool = True "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chess Transformer Module \n",
    "\n",
    "Also sourced heavily from https://github.com/karpathy/nanoGPT/blob/master/model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChessGPT(nn.Module): \n",
    "\n",
    "    def __init__(self, config): \n",
    "        super().__init__() \n",
    "        self.config = config \n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd), \n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            drop = nn.Dropout(config.dropout), \n",
    "            enc = nn.ModuleList([EncoderBlock(config) for _ in range(config.n_layer)]),\n",
    "            dec = nn.ModuleList([DecoderBlock(config) for _ in range(config.n_layer)]), \n",
    "            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n",
    "        ))\n",
    "\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        self.transformer.wte.weight = self.lm_head.weight \n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "        for pn, p in self.named_parameters(): \n",
    "            if pn.endswith('c_proj.weight'): \n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
    "\n",
    "        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
    "\n",
    "    def get_num_params(self, non_embedding=True): \n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        if non_embedding: \n",
    "            n_params -= self.transformer.wpe.weight.numel()\n",
    "        return n_params \n",
    "\n",
    "    def _init_weights(self, module): \n",
    "        if isinstance(module, nn.Linear): \n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None: \n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            # initializes module.weight with normal distribution from N(mean, std^2)\n",
    "\n",
    "    def forward(self, idx, targets=None): \n",
    "        device = idx.device \n",
    "        b, t = idx.size() \n",
    "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0) # shape (1, t)\n",
    "        \n",
    "        tok_emb = self.transformer.wte(idx)\n",
    "        pos_emb = self.transformer.wpe(pos)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "        for block in self.transformer.h: \n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        if targets is not None: \n",
    "            logits = self.lm_head(x)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "        else: \n",
    "            logits = self.lm_head(x[:, [-1], :])\n",
    "            loss = None \n",
    "\n",
    "        return logits, loss \n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens = 1, temperature=1.0, top_k=None): \n",
    "        for _ in range(max_new_tokens): \n",
    "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            if top_k is not None: \n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        return idx "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ChessDataset(\"tal_white_games\", \"tal_black_games\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "chessAgentConfig = ChessAgentConfig()\n",
    "chessAgent = ChessAgent(chessAgentConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 292.05M\n"
     ]
    }
   ],
   "source": [
    "chessGPTConfig = ChessGPTConfig()\n",
    "chessGPT = ChessGPT(chessGPTConfig)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
