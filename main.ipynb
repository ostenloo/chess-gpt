{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chess 1.9.4\n",
      "Torch 2.0.0\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "\n",
    "import chess \n",
    "import chess.pgn \n",
    "import chess.engine\n",
    "from stockfish import Stockfish\n",
    "\n",
    "import math \n",
    "from dataclasses import dataclass \n",
    "import numpy as np \n",
    "import os \n",
    "import time \n",
    "import asyncio\n",
    "\n",
    "print(\"Chess\", chess.__version__)\n",
    "print(\"Torch\", torch.__version__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "Only need to run once. Create a data folder before running. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Generate PGN and FEN files for Black and White Colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = \"data/Tal.pgn\"\n",
    "\n",
    "# # save the games based on if tal played white or black\n",
    "# games = {\n",
    "#     \"white\": [], \n",
    "#     \"black\": []\n",
    "# }\n",
    "\n",
    "# with open(file_path) as f: \n",
    "#     lines = f.readlines() \n",
    "#     tal_color = None \n",
    "#     on_pgn_line = False  \n",
    "#     pgn = \"\"\n",
    "#     for line in lines:\n",
    "#         if on_pgn_line:\n",
    "#             if line.startswith(\"[Event\"): \n",
    "#                 on_pgn_line = False \n",
    "#                 games[tal_color].append(pgn)\n",
    "#                 pgn = \"\"\n",
    "#             else: \n",
    "#                 pgn += line \n",
    "#         if line.startswith(\"[White \"): \n",
    "#             player_white = line.split('\"')[1]\n",
    "#             if player_white == \"Tal, Mihail\":\n",
    "#                 tal_color = \"white\"\n",
    "#         elif line.startswith(\"[Black \"):\n",
    "#             player_black = line.split('\"')[1]\n",
    "#             if player_black == \"Tal, Mihail\":\n",
    "#                 tal_color = \"black\"\n",
    "#         elif line.startswith(\"1.\"): \n",
    "#             on_pgn_line = True \n",
    "#             pgn += line \n",
    "#         else: \n",
    "#             # print(tal_color)\n",
    "#             continue \n",
    "\n",
    "\n",
    "# f.close()  \n",
    "\n",
    "# with open(\"data/tal_white_games.pgn\", \"w\") as f: \n",
    "#     for game in games[\"white\"]:\n",
    "#         f.write(game)\n",
    "\n",
    "# f.close()\n",
    "\n",
    "# with open(\"data/tal_black_games.pgn\", \"w\") as f:\n",
    "#     for game in games[\"black\"]:\n",
    "#         f.write(game)\n",
    "\n",
    "# f.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Generate UCI Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from itertools import product\n",
    "\n",
    "# files = [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\"]\n",
    "# ranks = [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\"]\n",
    "# promotion = [\"Q\", \"R\", \"B\", \"N\"]\n",
    "\n",
    "# # gets an upper bound for the UCI vocab\n",
    "\n",
    "# with open(\"data/UCIvocab.txt\", \"w\") as f:\n",
    "#     for file1, rank1, file2, rank2 in product(files, ranks, files, ranks):\n",
    "#         if file1 == file2 and rank1 == rank2:\n",
    "#             continue \n",
    "#         if rank2 == \"8\" and rank1 == \"7\" or rank2 == \"1\" and rank1 == \"2\":\n",
    "#             idx1 = files.index(file1)\n",
    "#             idx2 = files.index(file2)\n",
    "#             if abs(idx1 - idx2) > 1:\n",
    "#                 continue\n",
    "#             for promotion_piece in promotion:\n",
    "#                 f.write(f'{file1}{rank1}{file2}{rank2}{promotion_piece}\\n') \n",
    "#         f.write(f'{file1}{rank1}{file2}{rank2}\\n')\n",
    "\n",
    "# f.close() "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Generate PGN Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Q', 'R', 'B', 'N']\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "# pieces = [\"K\", \"Q\", \"R\", \"B\", \"N\"] \n",
    "# print(pieces[1:])\n",
    "# ranks = [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\"]\n",
    "# for rank in ranks[1:7]:\n",
    "#     print(rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pieces = [\"K\", \"Q\", \"R\", \"B\", \"N\"] \n",
    "# pawns = [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\"]\n",
    "# files = [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\"]\n",
    "# ranks = [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\"]\n",
    "\n",
    "# moves = set()\n",
    "\n",
    "# for piece in pieces: \n",
    "#     for file in files: \n",
    "#         for rank in ranks: \n",
    "#             moves.add(f\"{piece}{file}{rank}\")\n",
    "#             moves.add(f\"{piece}{file}{rank}+\")\n",
    "#             moves.add(f\"{piece}{file}{rank}#\")\n",
    "#             moves.add(f\"{piece}x{file}{rank}\")\n",
    "#             moves.add(f\"{piece}x{file}{rank}+\")\n",
    "#             moves.add(f\"{piece}x{file}{rank}#\")\n",
    "#             # 2 pieces can move to the same square \n",
    "#             if piece == \"R\" or piece == \"N\" or piece == \"B\" or piece == \"Q\": \n",
    "#                 for rank1 in ranks: \n",
    "#                     moves.add(f\"{piece}{rank1}{file}{rank}\")\n",
    "#                     moves.add(f\"{piece}{rank1}{file}{rank}+\")\n",
    "#                     moves.add(f\"{piece}{rank1}{file}{rank}#\")\n",
    "#                     moves.add(f\"{piece}{rank1}x{file}{rank}\")\n",
    "#                     moves.add(f\"{piece}{rank1}x{file}{rank}+\")\n",
    "#                     moves.add(f\"{piece}{rank1}x{file}{rank}#\")\n",
    "#                 for file1 in files: \n",
    "#                     moves.add(f\"{piece}{file1}{file}{rank}\")\n",
    "#                     moves.add(f\"{piece}{file1}{file}{rank}+\")\n",
    "#                     moves.add(f\"{piece}{file1}{file}{rank}#\")\n",
    "#                     moves.add(f\"{piece}{file1}x{file}{rank}\")\n",
    "#                     moves.add(f\"{piece}{file1}x{file}{rank}+\")\n",
    "#                     moves.add(f\"{piece}{file1}x{file}{rank}#\")\n",
    "#                 # if there are 3 pieces due to promotion, need to specify file and rank \n",
    "#                 for rank1 in ranks: \n",
    "#                     for file1 in files: \n",
    "#                         moves.add(f\"{piece}{file1}{rank1}{file}{rank}\")\n",
    "#                         moves.add(f\"{piece}{file1}{rank1}{file}{rank}+\")\n",
    "#                         moves.add(f\"{piece}{file1}{rank1}{file}{rank}#\")\n",
    "#                         moves.add(f\"{piece}{file1}{rank1}x{file}{rank}\")\n",
    "#                         moves.add(f\"{piece}{file1}{rank1}x{file}{rank}+\")\n",
    "#                         moves.add(f\"{piece}{file1}{rank1}x{file}{rank}#\")\n",
    "\n",
    "# for (idx, pawn) in enumerate(pawns): \n",
    "#     for rank in ranks[1:7]: #2,3,4,5,6,7\n",
    "#         if pawn == \"a\": \n",
    "#             if rank == \"7\": \n",
    "#                 for piece in pieces[1:]: \n",
    "#                     moves.add(f\"{pawn}xb{str(int(rank)+1)}={piece}\")\n",
    "#                     moves.add(f\"{pawn}xb{str(int(rank)+1)}={piece}+\")\n",
    "#                     moves.add(f\"{pawn}xb{str(int(rank)+1)}={piece}#\")\n",
    "#                     moves.add(f\"{pawn}{str(int(rank)+1)}={piece}\")\n",
    "#                     moves.add(f\"{pawn}{str(int(rank)+1)}={piece}+\")\n",
    "#                     moves.add(f\"{pawn}{str(int(rank)+1)}={piece}#\")\n",
    "#             elif rank == \"2\":\n",
    "#                 for piece in pieces[1:]:\n",
    "#                     moves.add(f\"{pawn}xb{str(int(rank)-1)}={piece}\")\n",
    "#                     moves.add(f\"{pawn}xb{str(int(rank)-1)}={piece}+\")\n",
    "#                     moves.add(f\"{pawn}xb{str(int(rank)-1)}={piece}#\")\n",
    "#                     moves.add(f\"{pawn}{str(int(rank)-1)}={piece}\")\n",
    "#                     moves.add(f\"{pawn}{str(int(rank)-1)}={piece}+\")\n",
    "#                     moves.add(f\"{pawn}{str(int(rank)-1)}={piece}#\")\n",
    "#             else: \n",
    "#                 moves.add(f\"{pawn}xb{str(int(rank)+1)}\")\n",
    "#                 moves.add(f\"{pawn}xb{str(int(rank)+1)}+\")\n",
    "#                 moves.add(f\"{pawn}xb{str(int(rank)+1)}#\")\n",
    "#                 moves.add(f\"{pawn}xb{str(int(rank)-1)}\")\n",
    "#                 moves.add(f\"{pawn}xb{str(int(rank)-1)}+\")\n",
    "#                 moves.add(f\"{pawn}xb{str(int(rank)-1)}#\")\n",
    "#                 moves.add(f\"{pawn}{str(int(rank)+1)}\")\n",
    "#                 moves.add(f\"{pawn}{str(int(rank)+1)}+\")\n",
    "#                 moves.add(f\"{pawn}{str(int(rank)+1)}#\")\n",
    "#                 moves.add(f\"{pawn}{str(int(rank)-1)}\")\n",
    "#                 moves.add(f\"{pawn}{str(int(rank)-1)}+\")\n",
    "#                 moves.add(f\"{pawn}{str(int(rank)-1)}#\")\n",
    "#         elif pawn == \"h\": \n",
    "#             if rank == \"7\":\n",
    "#                 for piece in pieces[1:]:\n",
    "#                     moves.add(f\"{pawn}xg{str(int(rank)+1)}={piece}\")\n",
    "#                     moves.add(f\"{pawn}xg{str(int(rank)+1)}={piece}+\")\n",
    "#                     moves.add(f\"{pawn}xg{str(int(rank)+1)}={piece}#\")\n",
    "#                     moves.add(f\"{pawn}{str(int(rank)+1)}={piece}\")\n",
    "#                     moves.add(f\"{pawn}{str(int(rank)+1)}={piece}+\")\n",
    "#                     moves.add(f\"{pawn}{str(int(rank)+1)}={piece}#\")\n",
    "#             elif rank == \"2\":\n",
    "#                 for piece in pieces[1:]:\n",
    "#                     moves.add(f\"{pawn}xg{str(int(rank)-1)}={piece}\")\n",
    "#                     moves.add(f\"{pawn}xg{str(int(rank)-1)}={piece}+\")\n",
    "#                     moves.add(f\"{pawn}xg{str(int(rank)-1)}={piece}#\")\n",
    "#                     moves.add(f\"{pawn}{str(int(rank)-1)}={piece}\")\n",
    "#                     moves.add(f\"{pawn}{str(int(rank)-1)}={piece}+\")\n",
    "#                     moves.add(f\"{pawn}{str(int(rank)-1)}={piece}#\")\n",
    "#             else: \n",
    "#                 moves.add(f\"{pawn}xg{str(int(rank)+1)}\")\n",
    "#                 moves.add(f\"{pawn}xg{str(int(rank)+1)}+\")\n",
    "#                 moves.add(f\"{pawn}xg{str(int(rank)+1)}#\")\n",
    "#                 moves.add(f\"{pawn}xg{str(int(rank)-1)}\")\n",
    "#                 moves.add(f\"{pawn}xg{str(int(rank)-1)}+\")\n",
    "#                 moves.add(f\"{pawn}xg{str(int(rank)-1)}#\")\n",
    "#                 moves.add(f\"{pawn}{str(int(rank)+1)}\")\n",
    "#                 moves.add(f\"{pawn}{str(int(rank)+1)}+\")\n",
    "#                 moves.add(f\"{pawn}{str(int(rank)+1)}#\")\n",
    "#                 moves.add(f\"{pawn}{str(int(rank)-1)}\")\n",
    "#                 moves.add(f\"{pawn}{str(int(rank)-1)}+\")\n",
    "#                 moves.add(f\"{pawn}{str(int(rank)-1)}#\")\n",
    "#         else: \n",
    "#             if rank == \"7\":\n",
    "#                 for piece in pieces[1:]: \n",
    "#                     moves.add(f\"{pawn}{str(int(rank)+1)}={piece}\")\n",
    "#                     moves.add(f\"{pawn}{str(int(rank)+1)}={piece}+\")\n",
    "#                     moves.add(f\"{pawn}{str(int(rank)+1)}={piece}#\")\n",
    "#                     moves.add(f\"{pawn}x{pawns[idx-1]}{str(int(rank)+1)}={piece}\")\n",
    "#                     moves.add(f\"{pawn}x{pawns[idx-1]}{str(int(rank)+1)}={piece}+\")\n",
    "#                     moves.add(f\"{pawn}x{pawns[idx-1]}{str(int(rank)+1)}={piece}#\")\n",
    "#                     moves.add(f\"{pawn}x{pawns[idx+1]}{str(int(rank)+1)}={piece}\")\n",
    "#                     moves.add(f\"{pawn}x{pawns[idx+1]}{str(int(rank)+1)}={piece}+\")\n",
    "#                     moves.add(f\"{pawn}x{pawns[idx+1]}{str(int(rank)+1)}={piece}#\")\n",
    "#             elif rank == \"2\":\n",
    "#                 for piece in pieces[1:]: \n",
    "#                     moves.add(f\"{pawn}{str(int(rank)-1)}={piece}\")\n",
    "#                     moves.add(f\"{pawn}{str(int(rank)-1)}={piece}+\")\n",
    "#                     moves.add(f\"{pawn}{str(int(rank)-1)}={piece}#\")\n",
    "#                     moves.add(f\"{pawn}x{pawns[idx-1]}{str(int(rank)-1)}={piece}\")\n",
    "#                     moves.add(f\"{pawn}x{pawns[idx-1]}{str(int(rank)-1)}={piece}+\")\n",
    "#                     moves.add(f\"{pawn}x{pawns[idx-1]}{str(int(rank)-1)}={piece}#\")\n",
    "#                     moves.add(f\"{pawn}x{pawns[idx+1]}{str(int(rank)-1)}={piece}\")\n",
    "#                     moves.add(f\"{pawn}x{pawns[idx+1]}{str(int(rank)-1)}={piece}+\")\n",
    "#                     moves.add(f\"{pawn}x{pawns[idx+1]}{str(int(rank)-1)}={piece}#\")\n",
    "#             else: \n",
    "#                 moves.add(f\"{pawn}x{pawns[idx-1]}{str(int(rank)+1)}\")\n",
    "#                 moves.add(f\"{pawn}x{pawns[idx-1]}{str(int(rank)+1)}+\")\n",
    "#                 moves.add(f\"{pawn}x{pawns[idx-1]}{str(int(rank)+1)}#\")\n",
    "#                 moves.add(f\"{pawn}x{pawns[idx+1]}{str(int(rank)+1)}\")\n",
    "#                 moves.add(f\"{pawn}x{pawns[idx+1]}{str(int(rank)+1)}+\")\n",
    "#                 moves.add(f\"{pawn}x{pawns[idx+1]}{str(int(rank)+1)}#\")\n",
    "#                 moves.add(f\"{pawn}x{pawns[idx-1]}{str(int(rank)-1)}\")\n",
    "#                 moves.add(f\"{pawn}x{pawns[idx-1]}{str(int(rank)-1)}+\")\n",
    "#                 moves.add(f\"{pawn}x{pawns[idx-1]}{str(int(rank)-1)}#\")\n",
    "#                 moves.add(f\"{pawn}x{pawns[idx+1]}{str(int(rank)-1)}\")\n",
    "#                 moves.add(f\"{pawn}x{pawns[idx+1]}{str(int(rank)-1)}+\")\n",
    "#                 moves.add(f\"{pawn}x{pawns[idx+1]}{str(int(rank)-1)}#\")\n",
    "#                 moves.add(f\"{pawn}{str(int(rank)+1)}\")\n",
    "#                 moves.add(f\"{pawn}{str(int(rank)+1)}+\")\n",
    "#                 moves.add(f\"{pawn}{str(int(rank)+1)}#\")\n",
    "#                 moves.add(f\"{pawn}{str(int(rank)-1)}\")\n",
    "#                 moves.add(f\"{pawn}{str(int(rank)-1)}+\")\n",
    "#                 moves.add(f\"{pawn}{str(int(rank)-1)}#\")\n",
    "\n",
    "# # add castling         \n",
    "# moves.add(\"O-O\")\n",
    "# moves.add(\"O-O+\")\n",
    "# moves.add(\"O-O#\")\n",
    "# moves.add(\"O-O-O\")\n",
    "# moves.add(\"O-O-O+\")\n",
    "# moves.add(\"O-O-O#\")\n",
    "\n",
    "# with open(\"data/PGNVocab.txt\", \"w\") as f: \n",
    "#     f.write(\"<eos>\\n1-0\\n0-1\\n1/2-1/2\\n\\n*\\n\")\n",
    "#     for move in moves: \n",
    "#         f.write(move + \"\\n\")\n",
    "\n",
    "# f.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_names = [\"tal_black_games\", \"tal_white_games\"]\n",
    "\n",
    "# for file in file_names:\n",
    "#     pgn = open(f\"data/{file}.pgn\", 'r')\n",
    "#     iter = 0 \n",
    "#     while iter < 1: \n",
    "#         iter += 1 \n",
    "#         game = chess.pgn.read_game(pgn)\n",
    "#         if game is None: \n",
    "#             break\n",
    "#         board = game.board()\n",
    "#         # print(type(board)) \n",
    "#         for move in game.mainline_moves():\n",
    "#             # print(type(move))\n",
    "#             # print(move)\n",
    "#             # print(board.san(move)) \n",
    "#             board.push(move) \n",
    "#             # print(move) #uci \n",
    "#             # print(board.fen()) #fen\n",
    "#             # print(board.san(move))\n",
    "#             # how to get pgn? \n",
    "\n",
    "#     pgn.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Generate PGNVocab Stripped of + and \\#  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# moves = []\n",
    "# move_set = set()\n",
    "\n",
    "# with open(\"data/PGNVocab.txt\", \"r\") as f: \n",
    "#     lines = f.readlines() \n",
    "#     for line in lines:\n",
    "#         line = line.strip(\"+#\\n\")\n",
    "#         if line not in move_set:\n",
    "#             moves.append(line)\n",
    "#             move_set.add(line)\n",
    "\n",
    "# f.close() \n",
    "\n",
    "# with open(\"data/PGNVocabStrip.txt\", \"w\") as f: \n",
    "#     for move in moves: \n",
    "#         f.write(move + \"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chess Agent used to interface with Stockfish engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass \n",
    "class ChessAgentConfig: \n",
    "    depth: int = 20 # engine depth \n",
    "    mate_score: int = 10000 # engine mate score\n",
    "\n",
    "class ChessAgent: \n",
    "    def __init__(self, config):\n",
    "        self.engine = chess.engine.SimpleEngine.popen_uci(\"stockfish\")\n",
    "        self.config = config \n",
    "\n",
    "    def fen_and_pgn_from_uci(self, uci, fen): \n",
    "        board = chess.Board(fen)\n",
    "        move = chess.Move.from_uci(uci)\n",
    "        pgn = board.san(move)\n",
    "        board.push(move)\n",
    "        fen = board.fen()\n",
    "        \n",
    "        return fen, pgn \n",
    "\n",
    "    async def position_info(self, fen): \n",
    "        board = chess.Board(fen)\n",
    "        return self.engine.analyse(board, chess.engine.Limit(depth=20))\n",
    "\n",
    "    async def position_eval(self, fen): \n",
    "        info = await self.position_info(fen)\n",
    "        return info[\"score\"].white().score(mate_score=self.config.mate_score)\n",
    "\n",
    "    def quit(self): \n",
    "        self.engine.quit()\n",
    "\n",
    "asyncio.set_event_loop_policy(chess.engine.EventLoopPolicy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "chessConfig = ChessAgentConfig()\n",
    "\n",
    "chessAgent = ChessAgent(chessConfig) \n",
    "\n",
    "fen, pgn = chessAgent.fen_and_pgn_from_uci(\"e2e4\",\"rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq -\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r n b q k b n r\n",
      "p p p p . p p p\n",
      ". . . . p . . .\n",
      ". . . . . . . .\n",
      ". . . . . P P .\n",
      ". . . . . . . .\n",
      "P P P P P . . P\n",
      "R N B Q K B N R\n",
      "Black Eval:  -9999\n",
      "\n",
      "\n",
      "r n b q k b n r\n",
      "p p p p p . . p\n",
      ". . . . . . . .\n",
      ". . . . . p p .\n",
      ". . . . P . . .\n",
      ". . . . . . . .\n",
      "P P P P Q P P P\n",
      "R N B . K B N R\n",
      "White Eval:  9999\n"
     ]
    }
   ],
   "source": [
    "black_mate_in_1 = \"rnbqkbnr/pppp1ppp/4p3/8/5PP1/8/PPPPP2P/RNBQKBNR b KQkq - 0 2\"\n",
    "white_mate_in_1 = \"rnbqkbnr/ppppp2p/8/5pp1/4P3/8/PPPPQPPP/RNB1KBNR w KQkq - 0 3\"\n",
    "\n",
    "print(chess.Board(black_mate_in_1))\n",
    "eval_black = await chessAgent.position_eval(black_mate_in_1)\n",
    "print(\"Black Eval: \", eval_black)\n",
    "print(\"\\n\")\n",
    "print(chess.Board(white_mate_in_1))\n",
    "eval_white = await chessAgent.position_eval(white_mate_in_1)\n",
    "print(\"White Eval: \", eval_white)\n",
    "\n",
    "chessAgent.quit()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Custom Dataset Class to be used in Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChessDataset(Dataset): \n",
    "    def __init__(self, white, black): \n",
    "        super().__init__()\n",
    "\n",
    "        assert os.path.exists(f\"data/{white}.pgn\"), f\"data/{white} does not exist\" \n",
    "        assert os.path.exists(f\"data/{white}_fen.txt\"), f\"data/{white}_fen.txt does not exist\" \n",
    "\n",
    "        self.white_uci = self.load_uci_moves(white)\n",
    "        self.white_fen = self.load_fen_moves(white)\n",
    "        self.white_pgn = self.load_pgn_moves(white)\n",
    "\n",
    "        assert len(self.white_uci) == len(self.white_fen), f\"white_uci and white_fen are not the same length\"\n",
    "\n",
    "        assert os.path.exists(f\"data/{black}.pgn\"), f\"data/{black} does not exist\"\n",
    "        assert os.path.exists(f\"data/{black}_fen.txt\"), f\"data/{black}_fen.txt does not exist\"\n",
    "\n",
    "        self.black_uci = self.load_uci_moves(black)\n",
    "        self.black_fen = self.load_fen_moves(black)\n",
    "        self.black_pgn = self.load_pgn_moves(black)\n",
    "\n",
    "        assert len(self.black_uci) == len(self.black_fen), f\"black_uci and black_fen are not the same length\"\n",
    "\n",
    "        self.pgn_vocab = self.load_pgn_vocab() \n",
    "         \n",
    "        self.pgn_embedding = nn.Embedding(131072, 768)\n",
    "        self.uci_embedding = nn.Embedding(8192, 768)\n",
    "\n",
    "        self.pad_game_length()\n",
    "\n",
    "        self.to_numpy()\n",
    "\n",
    "    def load_pgn_vocab(self): \n",
    "        vocab = open(\"data/PGNVocab.txt\", 'r')\n",
    "        vocab_dict = dict()\n",
    "        for (idx,line) in enumerate(vocab.readlines()): \n",
    "            vocab_dict[line.strip(\"\\n\")] = idx\n",
    "            \n",
    "        vocab.close() \n",
    "\n",
    "        return vocab_dict\n",
    "        \n",
    "    def load_fen_moves(self, file_name): \n",
    "        with open(f\"data/{file_name}_fen.txt\", 'r') as f: \n",
    "            lines = f.readlines() \n",
    "            games = []\n",
    "            for line in lines: \n",
    "                games.append(line.strip(\"\\n\").split(\";\")[:-1])\n",
    "        f.close() \n",
    "\n",
    "        return games\n",
    "\n",
    "    def load_uci_moves(self, file_name): \n",
    "        pgn = open(f\"data/{file_name}.pgn\", 'r')\n",
    "        games = [] \n",
    "\n",
    "        while True: \n",
    "            game = chess.pgn.read_game(pgn)\n",
    "            if game is None: \n",
    "                break\n",
    "            board = game.board() \n",
    "            moves = []\n",
    "            for move in game.mainline_moves(): \n",
    "                moves.append(str(move))\n",
    "            games.append(moves)\n",
    "        \n",
    "        return games \n",
    "\n",
    "    def load_pgn_moves(self, file_name): \n",
    "        pgn = open(f\"data/{file_name}.pgn\", \"r\")\n",
    "        games = [] \n",
    "\n",
    "        pgn_content = pgn.read() \n",
    "\n",
    "        games = pgn_content.split(\"\\n\\n\")[:-1]\n",
    "\n",
    "        game_move_list = []\n",
    "\n",
    "        for game in games: \n",
    "            moves = [move[max(0,move.find(\".\")+1):].strip(\"\\n\") for move in game.split(\" \")]\n",
    "            game_move_list.append(moves)\n",
    "\n",
    "        return game_move_list\n",
    "\n",
    "    def __len__(self): \n",
    "        return len(self.white_uci) + len(self.black_uci)\n",
    "\n",
    "    # for pgn moves \n",
    "    def to_numpy(self): \n",
    "        stoi = { move:i for i, move in enumerate(self.pgn_vocab) }\n",
    "        itos = { i:move for i, move in enumerate(self.pgn_vocab) }\n",
    "\n",
    "        def encode(pgn): \n",
    "            return [stoi[move] for move in pgn]\n",
    "        def decode(l): \n",
    "            return [itos[i] for i in l]\n",
    "\n",
    "        # print(self.white_pgn[0:10])\n",
    "        \n",
    "        self.white_np = np.array([encode(pgn) for pgn in self.white_pgn])\n",
    "        self.black_np = np.array([encode(pgn) for pgn in self.black_pgn])\n",
    "\n",
    "    # required for batch loading \n",
    "    def pad_game_length(self): \n",
    "        max_len = 512 \n",
    "        lists = [self.white_uci, self.black_uci, self.white_fen, self.black_fen, self.white_pgn, self.black_pgn]\n",
    "        for game_list in lists:\n",
    "            for game in game_list:\n",
    "                game_len = len(game)\n",
    "                while game_len < max_len:\n",
    "                    game.append(\"<eos>\")\n",
    "                    game_len += 1\n",
    "\n",
    "    def game_item(self, idx): \n",
    "        if idx < len(self.white_uci):\n",
    "            game = {\"UCI\": self.white_uci[idx], \"PGN\": self.white_pgn[idx], \"FEN\": self.white_fen[idx], \"np\": self.white_np[idx], \"color\": \"white\"}\n",
    "        else:\n",
    "            game = {\"UCI\": self.black_uci[idx - len(self.white_uci)], \"PGN\": self.black_pgn[idx - len(self.white_uci)], \"FEN\": self.black_fen[idx - len(self.white_uci)], \"np\": self.black_np[idx - len(self.white_uci)],  \"color\": \"black\"}\n",
    "\n",
    "        return game\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.game_item(idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to do this for the dataloader to work \n",
    "def collate_fn(batch):\n",
    "    # Combine individual samples into a list of dictionaries (batch)\n",
    "    batch_list = []\n",
    "    for sample in batch:\n",
    "        batch_list.append(sample)\n",
    "\n",
    "    return batch_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader1 = DataLoader(dataset1, batch_size=64, shuffle=True, num_workers=0, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(dataloader1))\n",
    "print(len(batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "62\n"
     ]
    }
   ],
   "source": [
    "for batch in dataloader1: \n",
    "    print(len(batch))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Blocks for Transformer \n",
    "\n",
    "Sourced Heavily from https://github.com/karpathy/nanoGPT/blob/master/model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, ndim, bias):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))\n",
    "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n",
    "\n",
    "# masked attention \n",
    "class CausalSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias) # in features, out features. ok this makes sense because it is a n_embd size matrix. \n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.dropout = config.dropout\n",
    "\n",
    "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
    "        if not self.flash:\n",
    "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
    "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                        .view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # hs = hidden state \n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q, k ,v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs) # .view() returns a new tensor of the same data but with a different shape specified by the args \n",
    "        # reshape() is a more robust version of view(). has to do with the tensor being contiguous or not. view() is more efficient if the tensor is contiguous. \n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        if self.flash:\n",
    "            # efficient attention using Flash Attention CUDA kernels\n",
    "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout, is_causal=True)\n",
    "        else:\n",
    "            # manual implementation of attention\n",
    "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "            att = F.softmax(att, dim=-1)\n",
    "            att = self.attn_dropout(att)\n",
    "            y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y\n",
    "\n",
    "class CausalCrossAttention(CausalSelfAttention): \n",
    "\n",
    "    def __init__(self, config): \n",
    "        super().__init__(config)\n",
    "        self.c_attn = nn.Linear(config.n_embd, config.n_embd * 2, bias=config.bias)\n",
    "        \n",
    "    def forward(self, x1, x2): \n",
    "        # x1 provides the Q and K \n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        q, k = self.c_attn(x1).split(self.n_embd, dim=2)\n",
    "        v = self.c_attn(x2)\n",
    "\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "\n",
    "        if self.flash: \n",
    "            y = torch.nn.functional.scaled_dot_product_attention(q, k , v, attn_mask=None, dropout_p=self.dropout, is_causal=True)\n",
    "        else: \n",
    "            att = (q @ k.transpose(-2,-1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "            att = F.softmax(att, dim=-1)\n",
    "            att = self.attn_dropout(att)\n",
    "            y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side \n",
    "\n",
    "        # output projection \n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y \n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias) # classifier fully connected\n",
    "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias) # classifier projection \n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = new_gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    # uses pre layer norm \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "class DecoderBlock(nn.Module): \n",
    "    def __init__(self, config): \n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.attn_1 = CausalSelfAttention(config)\n",
    "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.attn_2 = CausalCrossAttention(config)\n",
    "        self.ln_3 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.mlp = MLP(config)\n",
    "    \n",
    "    def forward(self, x1, x2): \n",
    "        x2 = x2 + self.attn_1(self.ln_1(x2))\n",
    "        x2 = x2 + self.attn_2(self.ln_2(x2), x1)\n",
    "        x2 = x2 + self.mlp(self.ln_3(x2))\n",
    "        return x2\n",
    "\n",
    "@dataclass \n",
    "class ChessGPTConfig: \n",
    "    block_size: int = 512 # longest chess game was 269 moves or 538 half moves. However, that's the only one. We can discard that as an outlier. 512 covers basically every chess game. \n",
    "    vocab_size: int = 131072 # using the PGNVocab.txt, there are 125394 words in the vocabulary. Not every vocabulary will be found in real games. Most will not because most of the moves are very rare. Some are also impossible. \n",
    "    n_layer: int = 12 # number of encoder and decoder blocks in the stack \n",
    "    n_head: int = 12 # multihead attention blocks \n",
    "    n_embd: int = 768 # size of the embedding... originally chosen by GPT-2 ... may have to modify later \n",
    "    dropout: float = 0.0 # none for now \n",
    "    bias: bool = True "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chess Transformer Module \n",
    "\n",
    "Also sourced heavily from https://github.com/karpathy/nanoGPT/blob/master/model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChessGPT(nn.Module): \n",
    "\n",
    "    def __init__(self, config): \n",
    "        super().__init__() \n",
    "        self.config = config \n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd), \n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            drop = nn.Dropout(config.dropout), \n",
    "            enc = nn.ModuleList([EncoderBlock(config) for _ in range(config.n_layer)]),\n",
    "            dec = nn.ModuleList([DecoderBlock(config) for _ in range(config.n_layer)]), \n",
    "            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n",
    "        ))\n",
    "\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        self.transformer.wte.weight = self.lm_head.weight \n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "        for pn, p in self.named_parameters(): \n",
    "            if pn.endswith('c_proj.weight'): \n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
    "\n",
    "        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
    "\n",
    "    def get_num_params(self, non_embedding=True): \n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        if non_embedding: \n",
    "            n_params -= self.transformer.wpe.weight.numel()\n",
    "        return n_params \n",
    "\n",
    "    def _init_weights(self, module): \n",
    "        if isinstance(module, nn.Linear): \n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None: \n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            # initializes module.weight with normal distribution from N(mean, std^2)\n",
    "\n",
    "    def forward(self, idx, targets=None): \n",
    "        device = idx.device \n",
    "        b, t = idx.size() \n",
    "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0) # shape (1, t)\n",
    "        \n",
    "        tok_emb = self.transformer.wte(idx)\n",
    "        pos_emb = self.transformer.wpe(pos)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "        for block in self.transformer.h: \n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        if targets is not None: \n",
    "            logits = self.lm_head(x)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "        else: \n",
    "            logits = self.lm_head(x[:, [-1], :])\n",
    "            loss = None \n",
    "\n",
    "        return logits, loss \n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens = 1, temperature=1.0, top_k=None): \n",
    "        for _ in range(max_new_tokens): \n",
    "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            if top_k is not None: \n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        return idx \n",
    "\n",
    "    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n",
    "        \"\"\"\n",
    "        This long function is unfortunately doing something very simple and is being very defensive:\n",
    "        We are separating out all parameters of the model into two buckets: those that will experience\n",
    "        weight decay for regularization and those that won't (biases, and layernorm/embedding weights).\n",
    "        We are then returning the PyTorch optimizer object.\n",
    "        \"\"\"\n",
    "\n",
    "        # separate out all parameters to those that will and won't experience regularizing weight decay\n",
    "        decay = set()\n",
    "        no_decay = set()\n",
    "        whitelist_weight_modules = (torch.nn.Linear, )\n",
    "        blacklist_weight_modules = (torch.nn.LayerNorm, LayerNorm, torch.nn.Embedding)\n",
    "        for mn, m in self.named_modules():\n",
    "            for pn, p in m.named_parameters():\n",
    "                fpn = '%s.%s' % (mn, pn) if mn else pn # full param name\n",
    "                # random note: because named_modules and named_parameters are recursive\n",
    "                # we will see the same tensors p many many times. but doing it this way\n",
    "                # allows us to know which parent module any tensor p belongs to...\n",
    "                if pn.endswith('bias'):\n",
    "                    # all biases will not be decayed\n",
    "                    no_decay.add(fpn)\n",
    "                elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):\n",
    "                    # weights of whitelist modules will be weight decayed\n",
    "                    decay.add(fpn)\n",
    "                elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n",
    "                    # weights of blacklist modules will NOT be weight decayed\n",
    "                    no_decay.add(fpn)\n",
    "\n",
    "        # subtle: 'transformer.wte.weight' and 'lm_head.weight' are tied, so they\n",
    "        # will appear in the no_decay and decay sets respectively after the above.\n",
    "        # In addition, because named_parameters() doesn't return duplicates, it\n",
    "        # will only return the first occurence, key'd by 'transformer.wte.weight', below.\n",
    "        # so let's manually remove 'lm_head.weight' from decay set. This will include\n",
    "        # this tensor into optimization via transformer.wte.weight only, and not decayed.\n",
    "        decay.remove('lm_head.weight')\n",
    "\n",
    "        # validate that we considered every parameter\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        inter_params = decay & no_decay\n",
    "        union_params = decay | no_decay\n",
    "        assert len(inter_params) == 0, \"parameters %s made it into both decay/no_decay sets!\" % (str(inter_params), )\n",
    "        assert len(param_dict.keys() - union_params) == 0, \"parameters %s were not separated into either decay/no_decay set!\" \\\n",
    "                                                    % (str(param_dict.keys() - union_params), )\n",
    "\n",
    "        # create the pytorch optimizer object\n",
    "        optim_groups = [\n",
    "            {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": weight_decay},\n",
    "            {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},\n",
    "        ]\n",
    "        # new PyTorch nightly has a new 'fused' option for AdamW that is much faster\n",
    "        use_fused = (device_type == 'cuda') and ('fused' in inspect.signature(torch.optim.AdamW).parameters)\n",
    "        print(f\"using fused AdamW: {use_fused}\")\n",
    "        extra_args = dict(fused=True) if use_fused else dict()\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n",
    "\n",
    "        return optimizer\n",
    "\n",
    "    def estimate_mfu(self, fwdbwd_per_iter, dt):\n",
    "        \"\"\" estimate model flops utilization (MFU) in units of A100 bfloat16 peak FLOPS \"\"\"\n",
    "        # first estimate the number of flops we do per iteration.\n",
    "        # see PaLM paper Appendix B as ref: https://arxiv.org/abs/2204.02311\n",
    "        N = self.get_num_params()\n",
    "        cfg = self.config\n",
    "        L, H, Q, T = cfg.n_layer, cfg.n_head, cfg.n_embd//cfg.n_head, cfg.block_size\n",
    "        flops_per_token = 6*N + 12*L*H*Q*T\n",
    "        flops_per_fwdbwd = flops_per_token * T\n",
    "        flops_per_iter = flops_per_fwdbwd * fwdbwd_per_iter\n",
    "        # express our flops throughput as ratio of A100 bfloat16 peak flops\n",
    "        flops_achieved = flops_per_iter * (1.0/dt) # per second\n",
    "        flops_promised = 312e12 # A100 GPU bfloat16 peak flops is 312 TFLOPS\n",
    "        mfu = flops_achieved / flops_promised\n",
    "        return mfu"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# default config values designed to train a gpt2 (124M) on OpenWebText\n",
    "# I/O\n",
    "out_dir = 'out'\n",
    "eval_interval = 2000\n",
    "log_interval = 1\n",
    "eval_iters = 200\n",
    "eval_only = False # if True, script exits right after the first eval\n",
    "always_save_checkpoint = True # if True, always save a checkpoint after each eval\n",
    "init_from = 'scratch' # 'scratch' or 'resume' or 'gpt2*'\n",
    "# wandb logging\n",
    "wandb_log = False # disabled by default\n",
    "wandb_project = 'owt'\n",
    "wandb_run_name = 'gpt2' # 'run' + str(time.time())\n",
    "# data\n",
    "dataset = 'openwebtext'\n",
    "gradient_accumulation_steps = 5 # used to simulate larger batch sizes\n",
    "batch_size = 64 # if gradient_accumulation_steps > 1, this is the micro-batch size\n",
    "block_size = 512 \n",
    "# model\n",
    "n_layer = 12\n",
    "n_head = 12\n",
    "n_embd = 768\n",
    "dropout = 0.0 # for pretraining 0 is good, for finetuning try 0.1+\n",
    "bias = False # do we use bias inside LayerNorm and Linear layers?\n",
    "# adamw optimizer\n",
    "learning_rate = 6e-4 # max learning rate\n",
    "max_iters = 600000 # total number of training iterations\n",
    "weight_decay = 1e-1\n",
    "beta1 = 0.9\n",
    "beta2 = 0.95\n",
    "grad_clip = 1.0 # clip gradients at this value, or disable if == 0.0\n",
    "# learning rate decay settings\n",
    "decay_lr = True # whether to decay the learning rate\n",
    "warmup_iters = 2000 # how many steps to warm up for\n",
    "lr_decay_iters = 600000 # should be ~= max_iters per Chinchilla\n",
    "min_lr = 6e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla\n",
    "# DDP settings\n",
    "backend = 'nccl' # 'nccl', 'gloo', etc.\n",
    "# system\n",
    "device = 'cpu' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks\n",
    "dtype = 'bfloat16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
    "compile = False # use PyTorch 2.0 to compile the model to be faster\n",
    "device_type = 'cpu'\n",
    "master_process = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ChessDataset(\"tal_white_games\", \"tal_black_games\")\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2187\n",
      "243\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))\n",
    "print(len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    dataloader = train if split == 'train' else val \n",
    "    batch = next(iter(dataloader))\n",
    "    x = torch.stack([torch.from_numpy((game['np']).astype(np.int64)) for game in batch])\n",
    "    y = torch.stack([torch.cat((torch.from_numpy((game['np'][1:]).astype(np.int64)),torch.tensor([0]))) for game in batch])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = get_batch('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "chessAgentConfig = ChessAgentConfig()\n",
    "chessAgent = ChessAgent(chessAgentConfig)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Run Training Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# default config values designed to train a gpt2 (124M) on OpenWebText\n",
    "# I/O\n",
    "out_dir = 'out'\n",
    "eval_interval = 2000\n",
    "log_interval = 1\n",
    "eval_iters = 200\n",
    "eval_only = False # if True, script exits right after the first eval\n",
    "always_save_checkpoint = True # if True, always save a checkpoint after each eval\n",
    "init_from = 'scratch' # 'scratch' or 'resume' or 'gpt2*'\n",
    "# wandb logging\n",
    "wandb_log = False # disabled by default\n",
    "wandb_project = 'owt'\n",
    "wandb_run_name = 'gpt2' # 'run' + str(time.time())\n",
    "# data\n",
    "dataset = 'openwebtext'\n",
    "gradient_accumulation_steps = 5 # used to simulate larger batch sizes\n",
    "batch_size = 64 # if gradient_accumulation_steps > 1, this is the micro-batch size\n",
    "block_size = 512 \n",
    "# model\n",
    "n_layer = 12\n",
    "n_head = 12\n",
    "n_embd = 768\n",
    "dropout = 0.0 # for pretraining 0 is good, for finetuning try 0.1+\n",
    "bias = False # do we use bias inside LayerNorm and Linear layers?\n",
    "# adamw optimizer\n",
    "learning_rate = 6e-4 # max learning rate\n",
    "max_iters = 600000 # total number of training iterations\n",
    "weight_decay = 1e-1\n",
    "beta1 = 0.9\n",
    "beta2 = 0.95\n",
    "grad_clip = 1.0 # clip gradients at this value, or disable if == 0.0\n",
    "# learning rate decay settings\n",
    "decay_lr = True # whether to decay the learning rate\n",
    "warmup_iters = 2000 # how many steps to warm up for\n",
    "lr_decay_iters = 600000 # should be ~= max_iters per Chinchilla\n",
    "min_lr = 6e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla\n",
    "# DDP settings\n",
    "backend = 'nccl' # 'nccl', 'gloo', etc.\n",
    "# system\n",
    "device = 'cpu' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks\n",
    "dtype = 'bfloat16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
    "compile = False # use PyTorch 2.0 to compile the model to be faster\n",
    "device_type = 'cpu'\n",
    "master_process = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_num = 0\n",
    "best_val_loss = 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 292.05M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChessGPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(131072, 768)\n",
       "    (wpe): Embedding(512, 768)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (enc): ModuleList(\n",
       "      (0-11): 12 x EncoderBlock(\n",
       "        (ln_1): LayerNorm()\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (dec): ModuleList(\n",
       "      (0-11): 12 x DecoderBlock(\n",
       "        (ln_1): LayerNorm()\n",
       "        (attn_1): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm()\n",
       "        (attn_2): CausalCrossAttention(\n",
       "          (c_attn): Linear(in_features=768, out_features=1536, bias=True)\n",
       "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ln_3): LayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=131072, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chessGPTConfig = ChessGPTConfig()\n",
    "model = ChessGPT(chessGPTConfig)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using fused AdamW: False\n"
     ]
    }
   ],
   "source": [
    "optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            with ctx:\n",
    "                logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate decay scheduler (cosine with warmup)\n",
    "def get_lr(it):\n",
    "    # 1) linear warmup for warmup_iters steps\n",
    "    if it < warmup_iters:\n",
    "        return learning_rate * it / warmup_iters\n",
    "    # 2) if it > lr_decay_iters, return min learning rate\n",
    "    if it > lr_decay_iters:\n",
    "        return min_lr\n",
    "    # 3) in between, use cosine decay down to min learning rate\n",
    "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
    "    return min_lr + coeff * (learning_rate - min_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_batch = next(iter(train))\n",
    "t0 = time.time() \n",
    "local_iter_num = 0 \n",
    "raw_model = model  \n",
    "running_mfu = -1.0\n",
    "ctx = nullcontext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True: \n",
    "    lr = get_lr(iter_num)\n",
    "    for param_group in optimizer.param_groups: \n",
    "        param_group['lr'] = lr \n",
    "\n",
    "    if iter_num % eval_interval == 0 and master_process: \n",
    "        losses = estimate_loss() \n",
    "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "    if iter_num == 0 and eval_only:\n",
    "        break\n",
    "\n",
    "    # forward backward update, with optional gradient accumulation to simulate larger batch size\n",
    "    # and using the GradScaler if data type is float16\n",
    "    for micro_step in range(gradient_accumulation_steps):\n",
    "        with ctx:\n",
    "            logits, loss = model(X, Y)\n",
    "        # immediately async prefetch next batch while model is doing the forward pass on the GPU\n",
    "        game_batch = next(iter(train))\n",
    "        # backward pass, with gradient scaling if training in fp16\n",
    "        scaler.scale(loss).backward()\n",
    "    # clip the gradient\n",
    "    if grad_clip != 0.0:\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "    # step the optimizer and scaler if training in fp16\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    # flush the gradients as soon as we can, no need for this memory anymore\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    # timing and logging\n",
    "    t1 = time.time()\n",
    "    dt = t1 - t0\n",
    "    t0 = t1\n",
    "    if iter_num % log_interval == 0 and master_process:\n",
    "        lossf = loss.item() # loss as float. note: this is a CPU-GPU sync point\n",
    "        if local_iter_num >= 5: # let the training loop settle a bit\n",
    "            mfu = raw_model.estimate_mfu(batch_size * gradient_accumulation_steps, dt)\n",
    "            running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu\n",
    "        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\")\n",
    "    iter_num += 1\n",
    "    local_iter_num += 1\n",
    "\n",
    "    # termination conditions\n",
    "    if iter_num > max_iters:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
